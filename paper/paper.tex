\documentclass[envcountsame]{llncs}


\usepackage[table]{xcolor}
\usepackage[utf8x]{inputenc}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{amsmath}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage[noend]{algpseudocode}
\usepackage{stmaryrd}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{array}
\usepackage[colon,round,sort]{natbib}
\usepackage{textcomp}
\usepackage{pifont}
% \usepackage{bbding}
\usepackage{comment}
\usepackage{graphicx}
\usepackage[svgpath=figures/]{svg}
\usepackage[labelfont=bf]{caption}
\captionsetup[table]{skip=5pt}
\usepackage{tikz}
\usetikzlibrary{patterns}
\usetikzlibrary{calc}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{shapes.geometric}
% \usepackage{tikz}
% \usetikzlibrary{patterns}
% \usetikzlibrary{calc}
% \usetikzlibrary{decorations.pathreplacing}
% \usetikzlibrary{shapes,arrows}
% \usetikzlibrary{shapes.geometric}
% \usetikzlibrary{arrows}


\usepackage{ntheorem}
\newtheorem*{definition*}{Definition}


\usepackage{lineno}


\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{relsize}
\tikzset{fontscale/.style = {font=\relsize{#1}}}
\usepackage{marvosym}
\usepackage{paper}


\newcommand\theauthor{
  Manuel Schneckenreither\inst{1}
  % \orcidID{0000-0002-4812-4665}
  \and Stefan Haeussler\inst{1}
  % \orcidID{0000-0003-2589-1367}
  \and Juanjo Peiró\inst{2}
}
\newcommand\thetitle{Average Reward Adjusted Deep Reinforcement Learning: Near-Blackwell-Optimal
  Policies for Order Release}


% \usepackage{academicons}
% \usepackage{cite}
\usepackage[hidelinks,
plainpages=false,
pdftitle={\thetitle{}},
pdfauthor={Manuel Schneckenreither, Stefan Haeussler},
pdfsubject={\thetitle{}},
pdfkeywords={average reward adjusted reinforcement learning, production planning, order release, machine learning,
  operations research},
]{hyperref}
\usepackage{url}
\pagestyle{plain}


\newcommand{\SH}[1]{\begin{center} \textcolor{green!50!black}{#1} \end{center}}
\newcommand\MS[2][r]{\ifx t#1 \textcolor{blue}{[\textbf{MS:} #2]}
  \else \begin{center}\textcolor{blue}{\textbf{MS:} #2} \end{center} \fi}

% \newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}

\author{\theauthor}
\date{\today}
\title{\thetitle}


% \authorrunning{Schneckenreither and Haeussler}
\institute{
  Department of Information Systems, Production and Logistics Management, University of Innsbruck, Austria\\
  \email{manuel.schneckenreither@uibk.ac.at,stefan.haeussler@uibk.ac.at}
  \and
  Departament d’Estadística i Investigació Operativa, Universitat de València, Spain\\
  \email{juanjo.peiro@uv.es}
}


\begin{document}


\maketitle

\begin{abstract}
  % teilw. von IJPR paper uebernommen
  An essential task in manufacturing planning and control is to determine when to release orders to
  the shop floor. One key parameter is the lead time which is the planned time that elapses between
  the release of an order and its completion. Lead times are normally determined based on the actual
  duration orders previously took to traverse through the production system (flow times).
  Traditional order release models assume static lead times, although it has been shown that they
  should be set dynamically to react the dynamic operational characteristics of the system.
  Therefore, we present an order release model which sets lead times dynamically by using an
  reinforcement learning approach. Therefore, we provide an in-depth analysis of reinforcement
  learning to show that average reward reinforcement learning is a better approach for operations
  research applications as discounted reinforcement learning. Additionally we present an average
  reward reinforcement learning algorithm which infers near-Blackwell-optimal policies. We use a
  simulation model of a \MS[t]{todo}two-stage flow-shop to compare the algorithm
  to % and show that our approach outperforms
  well-known order release mechanisms. We show that in the current version our proposed model using
  reinforcement learning outperforms some, but not all other tested approaches.
  % EVTL: especially for scenarios with...
  \keywords{operations research, production planning, order release, machine learning, reinforcement
    learning}
\end{abstract}


\MS[t]{ideas: Similar Selçuk (2013) detect that a high update frequency has a negative effect and
  thus should be reduced. They conclude that the lead times ought to control the WIP level, instead
  of the due date reliability as changes are directly seen (Knollmann and Windt, 2013). For a review
  of analytical works in the context of the LTS see Bendul and Knollmann (2016).}


\section{Introduction}\label{sec:introduction}
An important goal in Manufacturing Planning and Control (MPC) systems is to achieve short and
predictable flow times, especially where high flexibility in meeting customer demand is required.
Besides achieving short flow times, one should also maintain high output and due-date performance
while keeping the work-in-process level low. One approach to address this problem is to collect all
incoming orders in an order-pool and periodically decide which orders to release to the shop floor.
Once orders are released, costs start to accumulate as planned orders materialise as actual jobs in
the production system. The main challenge is to find a good compromise of balancing the shop floor
and timely completion of jobs. Although the performance of such systems can be measured manifold the
most overarching objective is to maximise profits by adequately assigning holding and lateness
costs.

One of the key modeling parameters for order release mechanisms is the \textit{lead time}, which
refers to the planned time that elapses between the release of an order and its arrival in the
finished goods inventory. This planning parameter is often based on the observable time an order
needs to traverse the production system, which in contrast is denoted as the \textit{flow time}.
Flow times consist of processing, setup, control, transport, and waiting times, whereas the latter
is the governing factor \citep[][p.223]{zapfel1982produktionswirtschaft}. Waiting times are a result
from queuing (e.g.\@ jobs queue before and after processing), depend heavily on the amount of jobs in
the system (WIP) and thus are relatively difficult to estimate, which makes the
setting of favorable lead times so difficult (e.g., \citealt{Tatsiopoulos1983, Wiendahl1995}). Most
state-of-the-art order release mechanisms use static or fixed lead times to address the order
release problem, and thus neglect the nonlinear relationship between resource utilisation and flow
times, which is well known from practice and queuing theory (\citealt{Pahl2007}).

One way to address this nonlinear interaction effects is to set lead times dynamically. Intuitively
the order release problem is solved by perfectly matching the lead times to the flow times, but the
corresponding optimisation problem faces ``sampling issues'' meaning that the flow times depend on
the lead times. An extreme scenario of this problem is the so called ``lead time syndrome'', which
describes a vicious cycle where increasing flow times perpetually inflate the lead times which leads
to worse performance (see e.g., \citealt{Mather1978, knollmann2013control, Selcuk2006}.
%
Thus, setting lead times dynamically harbors optimisation potential \citep{hoyt1978dynamic}, but may
also substantially degrade the system performance.
%
\citet{schneckenreither2020order} have established
following categorisation of dynamic lead time management approaches:

\begin{itemize}
\item \textsf{Reactive lead time management} approaches set lead times by reacting on
  earlier flow times (e.g., \citealt{enns2004work, Selcuk2006}). Note that the forecast is always
  based on \textit{past} data as the most recent system changes cannot be reflected by flow times
  until the corresponding orders arrive in the FGI, which might take several
  periods.
\item
  \textsf{Proactive lead time management} may incorporate \textit{past} data in conjunction with
  the \textit{current} system state to set lead times (e.g., \citealt{Bertrand1983,
    ChungHuang2002}). Put differently, these methods aim to find a function that provides lead times
  based on the current state of the production system and possibly information from the past.

\item \textsf{Predictive lead time management} may not only incorporate \textit{past} data and the
  \textit{current} system state to set lead times, but also utilises the anticipated \textit{future}
  system state to detect arising issues of future periods and react accordingly (e.g.
  \citealt{PaterninaArboleda2001,schneckenreither2020order,Schneckenreither2019}). Thus, it extends
  proactive lead time management from a flow time forecasting or simple lead time setting technique
  to a lead time management approach that integrates the future behaviour of the system when setting
  lead times. This allows reasoning of the system dynamics, as for instance triggering the lead time
  system, and thus such an algorithm can react accordingly to find a more farsighted optimal lead
  time update.
\end{itemize}

We hypothesis that dynamic lead time management approaches need to aim for a predictive design in
order to be able to compete with state-of-the-art order release methods from literature. However,
there only exist three papers that propose a predictive lead time management algorithm in
literature.

The first approach by \citet{PaterninaArboleda2001} introduces a predictive order release model by
using reinforcement learning in a single product, four-station serial flow line and compare its
performance (WIP costs) with conventional order release policies (e.g., Kanban and CONWIP).
%
% Reinforcement learning is an optimisation technique that stems from dynamic programming and its goal
% is to find the best stationary policy for a given problem. This policy is usually provided by
% assessing current and future states (or state-action pairs) of an underlying Markov Decision Process
% (MDP).
% The advantage of reinforcement learning over dynamic programming is that (i) the problem
% space is explored by an agent and thus only expectantly interesting parts of the problem space need
% to be assessed and (ii) the knowledge (acquisition) of transition probabilities becomes unnecessary
% as the states are evaluated by consecutively observed states solely.
%
The algorithm of \citet{PaterninaArboleda2001} decides on whether or not to release an order after
each system change, the completion of an operation of any order at any stage or a new order arrival,
and assumes that any unsatisfied demand is lost. Thus, they use a continuous order release method
although in practice order release decisions often need to be made on a periodical basis, e.g.\@
daily (see \citealt{enns2004work,GeldersvanW1982}). They outperform existing control policies with their tabular
based reinforcement learning agent.
%
Then, \citet{Schneckenreither2019} use several different reinforcement learning algorithms to make
periodic order release decisions for a flow shop production system. The algorithm directly sets lead
times for each product type, which are then used to release the orders in the order pool. They show
that their approach outperforms static order release mechanisms by yielding lower costs, lateness
and standard deviation of lateness, but conclude that research using average reward reinforcement
learning methods harbor optimisation potential for the order release problem.
%
And finally, \citet{schneckenreither2020order} present a flow time estimation procedure to set lead
times dynamically using an artificial neural network, which is used to forecast flow times. By
implementing a rolling horizon order release simulation of the proceeding periods they lift their
approach to a predictive lead time management approach, which is able to detect backorders of future
periods and reacts by releasing orders earlier. Nonetheless, their method is unable to foresee the
triggering of the lead time syndrome and therefore they introduce an upper lead time bound to
prevent the negative effects of the lead time syndrome. Their model outperforms static and reactive
lead time management approaches, especially for scenarios with high utilisation and high variability
in processing times.


As reinforcement learning stems from dynamic programming the future system state is by design
considered as a main driver of decision making in the current period.
% Reinforcement learning is an optimisation technique that stems from dynamic programming and
Its goal is to find the best stationary policy for a given problem. % This policy is usually provided
% by assessing current and future states (or state-action pairs) of an underlying Markov Decision
% Process (MDP).
The advantage of reinforcement learning over dynamic programming is that (i) the problem
space is explored by an agent and thus only expectantly interesting parts of the problem space need
to be assessed and (ii) the knowledge (acquisition) of transition probabilities becomes unnecessary
as the states are evaluated by consecutively observed states solely.

Over the past decades reinforcement learning has been applied to various problems, for which
astonishing results have been reported. E.g.\@ only recently \citet{mnih2015human} have presented a
novel value-iteration reinforcement learning agent which exceeds human-level abilities in playing
many classic Atari 2600 games~\citep{bellemare2012investigating}. Further,
\citet{mnih2016asynchronous} present improved results with asynchronous actor-critic reinforcement
learning. Also games like Go~\citep{silver2016mastering} and Chess~\citep{silver2017mastering} have
been mastered with superhuman performance by \textit{tabula rasa} reinforcement learning agents.
Furthermore, the method has also been applied in the setting of manufacturing system, e.g.\@ to
improve the ramp-up process~\citep{doltsinis2012reinforcement}, in locally selecting appropriate
dispatching rules \citep{zhang1995reinforcement,wang2005application} or scheduling
\citep{zhang1995reinforcement, waschneck2018optimization}.
%
However, all these applications use discounted reinforcement learning and are either designed to
investigate a rather simple MDP, e.g.\@ by selecting heuristics instead of optimising the underlying
problem itself, or by mapping the actual objective in a reward function that is approximately \(0\)
on average over time.
%
This is due to the fact that state value is largely composed of a term defined by the policys'
average reward value \citep{MillerVeinott1969,Blackwell62} which would otherwise dilute the state
values and thus decrease the solution quality, as can for instance be observed in
\citet{SchneckenreitherHaeussler2019} and \citet{gijsbrechts2018can}.
%


Therefore, most applications that incorporate and directly reflect costs or profit in the reward
function use average reward reinforcement learning. \citet{aydin2000dynamic} use it in the setting
of scheduling, while in a series of papers Mahadevan et al.\@ investigated several problem domains
starting with simple MDPs
\citep{Mahadevan96_AverageRewardReinforcementLearningFoundationsAlgorithmsAndEmpiricalResults,Mahadevan96_OptimalityCriteriaInReinforcementLearning}.
After these foundational works for average reward reinforcement learning they introduced a
continuous time average reward reinforcement learning algorithm named SMART
\citep{Mahadevan97_SelfimprovingFactorySimulationUsingContinuoustimeAveragerewardReinforcementLearning}.
Applications of SMART reach from the optimisation of queuing systems
\citep{Mahadevan96_AnAveragerewardReinforcementLearningAlgorithmForComputingBiasoptimalPolicies,Mahadevan96_SensitiveDiscountOptimalityUnifyingDiscountedAndAverageRewardReinforcementLearning},
maintenance of an inventory system
\citep{Das99_SolvingSemiMarkovDecisionProblemsUsingAverageRewardReinforcementLearning} to optimising
transfer line in terms of maximizing demand, while keeping inventory levels of unfinished product as
low as possible \citep{Mahadevan98_OptimizingProductionManufacturingUsingReinforcementLearning}.
However, in practise usually decision have to be made on a daily basis
\citep{enns2004work,GeldersvanW1982}. Therefore, we refrain from this adaption and concentrate on
standard MDPs only. Furthermore, often continuous-time semi-MDP problems can be converted through
uniformisation into equivalent discrete time instances
\citep[see][]{Puterman94,bertsekas1995dynamic}.


Like discounted reinforcement learning also average reward reinforcement learning is based on an
oracle function, in our case the accumulated costs of a period, to assess the decisions taken by the
agent. By repeatedly choosing different actions the agent examines the problem space and rates
possible actions for any observed state. The advantage of average reward reinforcement learning over
the widely applied discounted reinforcement learning framework is that the underlying optimisation
technique is able to find better policies. This yields from the fact that in standard discounted
reinforcement learning method the states are assessed independently and by a single value. To be
more precise, average reward reinforcement learning splits up the evaluation of the average reward
per step, a bias value that specifies the amount of collected rewards to reach the optimal path when
starting in a suboptimal state and an error term which defines the number of steps to reach the
optimal path \citep{Howard64,Puterman94,
  Mahadevan96_AverageRewardReinforcementLearningFoundationsAlgorithmsAndEmpiricalResults}. % Thus,
% while in average reward reinforcement learning these terms are learned separately
In contrast to that in the standard discounted framework one single scalar value consisting of the
addition of these subterms is estimated, where however the average reward is scaled by
\(1/(1-\gamma)\). The discount factor \(\gamma\) is usually set very close to \(1\), e.g. \(0.99\),
which leads to the fact that this subterm dominates the other two.
%
Further, in commonly applied unichain (and thus ergodic) MDPs the average reward per step is equal
for all states. Thus, independently assessing it is not only computationally unwisely, but also
problematic when iteratively annealed as done in reinforcement learning. Therefore, we adapt an
algorithm that uses a scalar value for the estimation of the average reward over all states, and
estimates for every state-action pair that incorporate the bias and error term. The later values are
adjusted by the average reward.
% This, combined with the iterative evaluation and the independently assessing of the average reward
% for each state lets standard discounted reinforcement learning struggle to find good policies.
% Thus, this incautious combining of different kinds of state values as done in discounted
% reinforcement learning leads to the problems that (i) the average reward, which is equal for all
% states of usually investigated unichain MDPs, is dominating and thus diluting the bias values, and
% (ii) the state values are deteriorated by the error term that is only imposed due to the
% discounting technique.

Thus, as opposed to the commonly applied discounted reinforcement learning algorithm we use an
average reward adjusted reinforcement learning algorithm to adaptively release orders based on the
assessed state values of the production system. In contrast to the aforementioned works on average
reward reinforcement learning our algorithm incorporates the optimisation of not only the average
reward over time, but also the bias values, which is an important adaption in highly stochastic
systems. Only the work by
\citet{Mahadevan96_AnAveragerewardReinforcementLearningAlgorithmForComputingBiasoptimalPolicies}
integrates this second-level refinement optimisation. However, their algorithm requires the
selection of a reference state to prevent an infinite increase of state values. This results from
the lack of feedback in the iterative process of optimising the different refinement optimisation
levels. Furthermore, they asses the average reward independently for each state.

In summary, we propose a novel average reward adjusted reinforcement learning algorithm to assess
orders for their release to the shop floor. This is the first average reward adjusted reward
learning algorithm that operates using a artificial neural network as function approximation. To
ensure scalability we adapt a multi-agent approach, where each agent optimises the lead time
management of a single product type. The agents do so by assessesing the expected costs for the
possible releases imposed by adapting the lead time. According to the learned estimates each agent
sets a planned lead time for the corresponding product type and with that releases orders into the
production system. The highly optimised value-iteration algorithm uses improved n-step reinforcement
learning with small action-based replay memories and worker agents to infer order release policies
which outerperform \MS[t]{todo}.

%
% \MS[t]{TODO} We provide ample evidence of the viability of the approach and compare it to well-known
% order release mechanisms. Furthermore, we show that the advantage of average reward reinforcement
% learning for the problem structures encountered in the context of production and logistics by
% comparing the results to the discounted reinforcement learning framework. The preliminary results
% show that our approach performs better than the other methods. With regard to practical implications
% we are confident that decision support tools based on average reward reinforcement learning increase
% the decision quality of human planners.

\paragraph{Structure.} The rest of the paper is structured as follows. The next section introduced
average reward reinforcement learning and presents the used algorithm (see also
\citealt{schneckenreither2020average}). Section~\ref{sec:Experimental_Evaluation} describes the
simulation model we use to evaluate the approach. \MS[t]{todo}
% Then~\ref{sec:Preliminary} presents preliminary
% result data, whereas Section~\ref{sec:conclusion} concludes this working paper.


\section{Average Reward Adjusted Reinforcement Learning}

This section briefly reintroduces the most important concepts of average reward adjusted
reinforcement learning, elaborates on optimality criteria and provides insights of the underlying
algorithm. For a more extensive introduction to average reward reinforcement learning we refer to
\citet{schneckenreither2020average}.

Like \cite{MillerVeinott1969} we are considering problems that are observed in a sequence of points
in time labeled \(1,2,\ldots\) and can be modelled using a finite set of states \(\States\),
labelled \(1,2,\ldots,\size{\States}\), where the size \(\size{\States}\) is the number of elements
in \(\States\). At each point $t$ in time the system is in a state \(s_{t} \in \States\). Further,
by choosing an action $a_{t}$ of a finite set of possible actions \(A_{s}\) the system returns a
reward $r_{t} = r(s_{t}, a_{t})$ and transitions to another state \(s_{t+1} \in \States\) at time
\(t+1\) with conditional probability \(p(s_{t+1}, r_{t} \mid s_{t}, a_{t})\). That is we assume that
reaching state \(s_{t+1}\) from state \(s_{t}\) with reward \(r_{t}\) depends solely on the previous
state \(s_{t}\) and chosen action \(a_{t}\). In other words, we expect the system to possess the
Markov property \citep[p.63]{sutton1998introduction}. Reinforcement learning processes that possess
the Markov property are referred to as Markov decision processes (MDPs)
\citep[p.66]{sutton1998introduction}.

Thus, the action space is defined as \(F = \times_{s=1}^{\size{\States}} A_{s}\), where \(A_{s}\) is
a finite set of possible actions. A \emph{policy} is a sequence \(\pol = (f_{1},f_{2},\ldots)\) of
elements \(f_{t} \in F\). Using the policy \(\pol\) means that if the system is in state \(s\) at
time \(t\) the action \(f_{t}(s)\), i.e.% that is
the \(s\)-th component of \(f_{t}\), is chosen. A stationary policy \(\pol = (f,f,\ldots)\) does not
depend on time. In the sequel we are concerned with stationary policies only.


\subsection{Discounted Reinforcement Learning}
\label{subsec:Discounted_Reinforcement_Learning}


In the widely applied discounted framework the value of a state \(V_{\gamma}^{\pol_{\gamma}}(s)\) is
defined as the expected discounted sum of rewards under the stationary policy \(\pol_{\gamma}\) when
starting in state \(s\). Note that the policy \(\pol_{\gamma}\) depends on the selected discount
factor. That is
\begin{align*}
  V_{\gamma}^{\pol_{\gamma}}(s)=\lim_{N \to \infty} \E[\sum_{t=0}^{N-1} \gamma^{t} R_{t}^{\pol_{\gamma}}(s)]\tcom
\end{align*}

where \(0 \leqslant \gamma < 1\) is the discount factor and
\(R_{t}^{\pol}(s) = \E_{\pol}[ r(s_{t},a_{t}) \mid s_{t} = s, a_{t} = a]\) the reward received at
time \(t\) upon starting in state \(s\) by following policy \(\pol\)
\citep{Mahadevan96_AverageRewardReinforcementLearningFoundationsAlgorithmsAndEmpiricalResults}.
%
The aim in the discounted framework is to find an optimal policy \(\polopt_{\gamma}\), which when
followed, maximises the state value for all states \(s\) as compared to any other policy
\(\pol_{\gamma}\): \(V_{\gamma}^{\polopt_{\gamma}} - V_{\gamma}^{\pol_{\gamma}} \geqslant 0\). This
criteria is usually referred to as \(\gamma\)-optimality as the discount factor \(\gamma\) is fixed.
However, this also means that the actual value set for \(\gamma\) determines the best achievable
policy. For instance, as can be seen in Figure~\ref{fig:printer} setting \(\gamma <0.8027\) the
printer-loop is preferred over the mail-loop, although the mail-loop accumulates more
reward over time, i.e.\@ selecting the mail-loop is a sub-optimal choice. Observe that the average
reward received per step equals \(1\) for the printer-loop and \(2\) for the mail-loop. Thus, the
(Blackwell-)optimal policy is to choose the mail-loop. However, if
\(\gamma < 3^{-\frac{1}{5}} \approx 0.8027\) an agent using discounted reinforcement learning
chooses the printer loop as going the printer loop has a higher evaluation.
%
\begin{figure}[t!]
  \centering
  \input{figures/printer}
  \caption{\label{fig:printer} A MDP with a single action choice in
    state \(1\), i.e.\@ two different deterministic policies
    % , in which the agent can choose between doing the printer-loop or the mail-loop  . The
    % Observe that the average reward received per step equals \(1\) for the printer-loop and \(2\)
    % for the mail-loop. Thus, the (Blackwell-)optimal policy is to choose the mail-loop. However,
    % if \(\gamma < 3^{-\frac{1}{5}} \approx 0.8027\) an agent using discounted reinforcement
    % learning chooses the printer loop.
    % %
    \citep[][adapted]{Mahadevan96_OptimalityCriteriaInReinforcementLearning}.
  }
\end{figure}
%

Therefore, in the seek of a more general optimality criteria for determining the best stationary
policy \(\pol\) for a given problem, we introduce \(n\)-discount-optimality.


% \subsection{Optimality Criteria}
% \label{subsec:More_Refined_Optimality_Criteria}

% The aforementioned and informally introduced notions of gain- and bias-optimality are formalised and
% generalised to \(n\)-discount-optimality as follows.

\begin{definition}[\(n\)-Discount-Optimality]
  Due to \citet{Veinott69} for a MDP a policy \(\polopt\) is \emph{\(n\)-discount-optimal} for
  \(n=-1,0,1,\ldots\) for all states \(s \in \States\) with discount factor
  \(\gamma\) % and \(V_{\gamma}^{\pol}(s)\) being the value function as defined above
  %
  if and only if
  \begin{align*}
    \lim_{\gamma \to 1}(1-\gamma)^{-n}\ (V_{\gamma}^{\polopt}(s) - V_{\gamma}^{\pol}(s)) \geqslant 0 \tpkt
  \end{align*}
\end{definition}


Only if a policy is \(n\)-discount-optimal for all \(n < m\) it can be \(m\)-discount
optimal \citep{Puterman94,Veinott69}.
%
If a policy is \(-1\)-discount-optimal it is called gain-optimal, if it is \(0\)-discount-optimal it
is also called bias-optimal.
%
Furthermore, if a policy is \(\infty\)-discount-optimal then it is said to be Blackwell-optimal
\citep{Blackwell62}.
%
That is, for Blackwell-optimal policies \(\polopt\) there exists a discount factor \(\gammaopt < 1\)
such that \(V_{\gamma}^{\polopt}(s) \geqslant V_{\gamma}^{\pol}(s)\) for all
\(\gamma \geqslant \gammaopt\) and under all policies \(\pol\)
\citep{Mahadevan96_SensitiveDiscountOptimalityUnifyingDiscountedAndAverageRewardReinforcementLearning,Blackwell62}.
Informally that means there exists a discount factor \(\gamma <1\) which finds Blackwell-optimal
policies. However, (i) in more complex, i.e.\@ real world, MDPs this value can be arbitrary close to
\(1\) and (ii) the difference in state values may be very small, which (iii) due to the need of
state value function approximation likely causes errors when choosing among different actions in the
discounted framework. \citet{schneckenreither2020average} establishes a practically relevant
definition for near-Blackwell-optimality.

\begin{definition}
  If an algorithm infers for any MDP bias-optimal policies, and for a given MDP can in theory be
  configured to infer Blackwell-optimal policies, but in practise this ability is naturally limited
  due to the finite accuracy of floating-point representation of modern computer systems, it is said
  to be \emph{near-Blackwell-optimal}. An according to a near-Blackwell-optimal algorithm inferred
  Blackwell-optimal policy is called near-Blackwell-optimal.
\end{definition}

Note that standard discounted RL does not meet the requirements of near-Blackwell-optimality, as it
generally does not infer bias-optimal policies. Only by chance the resulting policy could be
bias-optimal.
%
% In the sequel we present the Laurent series expansion which not only links discounted reinforcement
% learning with average reward reinforcement learning, but also connects to \(n\)-discount-optimality.
%
% Furthermore, for the rest of the paper we occasionally write drop the \(V(s)\) and $V_{\gamma}$ instead of
% \(V^{\pol}(s)\) and \(V^{\pol}_{\gamma}\) when the policy is clear from the context.
%
The corresponding Bellman equation defined as
\(V_{\gamma}^{\pol}(s) = \E_{\pol}[r + \gamma V_{\gamma}^{\pol}(s')]\) \citep[see
e.g.][p.70]{sutton1998introduction} provides a way to assess the state values using two
consecutively observed states \(s, s'\) and the observed reward \(r\) returned by the system. When
this formula is used as an update rule it provides an algorithm that converges the state values by
iteratively adapting the state value estimates.
%
There are four major issues with discounted RL, which results in the fact that it is inapplicable
for many problems of operations research.


\begin{enumerate}
\item In general standard discounted RL can only infer suboptimal policies as the discount factor is
  strictly less than one, i.e.\@ \(\gamma < 1\).
\item It is very difficult to specify a desired balance between the short-term and long-term
  (average) rewards. This results from the fact that the long-term rewards are scaled exponentially,
  where the factor depends on the chosen \(\gamma\) value.
\item Episodic MDPs with an average reward per step that is non-zero cannot be solved correctly, as
  the average reward is ignored in the terminal states.
\item The average reward is independently assessed for each state, even though it is the dominating
  part for all state values and thus is shared between more than one or, for unichain MDPs, even all
  states. This usually leads to an exponentially increased number of learning steps required for
  policies to converge.
\end{enumerate}


% One idea behind the \(\gamma\)-parameter of the discounted framework is to be able to balance
% short-term rewards (low \(\gamma\)-values) and long-term rewards (high \(\gamma\)-values). However,
% what seems to be an advantage rather becomes a disadvantage for most applications. The issue is that
% the average reward value is non-linearly increased when \(\gamma\) approaches one. However, in
% almost all cases the aim is to perform well over time which in terms of reward means to first
% maximise for a policy with highest average reward before more selectively choosing actions. That is,
% we are searching for the policies with highest average reward before considering other criteria.
% %
% Therefore, in almost all RL studies the discount factor is set to a value very close to \(1\), for
% instance to \(0.99\)~\citep[e.g.][]{mnih2015human,mnih2016asynchronous,Lillicrap15}.
% %
% By doing so the above mentioned non-linear relationship leads to the fact that the state value
% consists almost solely of the up-scaled average reward term, whereas the bias values can be
% neglected. This leads to diluted state values and thus difficulties in distinguishing actions which
% impose policies that possess the same average reward.
% %
% In contrast to that, average reward RL separately assesses these values and according to these
% selectively chooses the best action, comparing one after the other.
% %
% To illustrate the idea reconsider Figure~\ref{fig:printer}. First the agent picks the policy
% according to the highest average reward. Thus, all actions are assessed by an average reward of
% \(2\) which is inferred from the Mail-loop. This however, makes choosing the Printer-loop
% unattractive; i.e. the policy converges to the optimal policy of choosing the Mail loop.

Especially due to the fourth issue standard discounted RL is not well applicable in the operations
research domain. This mainly results from the fact that a continuous assessing of the state values
using costs or profit is often required, which leads to intractable long learning phases with high
exploration rate even for small sized problems. But as high exploration rates produce errors in the
state value estimations \citep{MillerVeinott1969} finding a well working parameterisation is
difficult. Average reward reinforcement learning overcomes these issues by separately assessing the
subterms of the state values directly.


\subsection{Average Reward Reinforcement Learning}

Due to \cite{Howard64} the average reward \(\avgrew^{\pol}(s)\) of a policy \(\pol\) and a starting
state \(s\) is defined as
\begin{align*}
  \avgrew^{\pol}(s) = \lim_{N \to \infty} \frac{\E [\sum_{t=0}^{N-1}R_{t}^{\pol}(s)]}{N}\tpkt
\end{align*}


In the common case of unichain MDPs, in which only a single set of recurrent states exists, the
average reward \(\avgrew^{\pol}(s)\) is equal for all states \(s\)
\citep{Mahadevan96_AverageRewardReinforcementLearningFoundationsAlgorithmsAndEmpiricalResults,Puterman94}.
In the sequel we focus on unichain MDPs in this work and thus may simply refer to it as
\(\avgrew^{\pol}\). A policy \(\polopt\) that maximises the average reward
\(\avgrew^{\polopt}(s) - \avgrew^{\pol}(s) \geqslant 0\) in every state \(s\) as compared to any
other policy \(\pol\) is called gain-optimal.
%
% A policy that maximises the average reward in every state is called
% gain-optimal
% \citep{Mahadevan96_AnAveragerewardReinforcementLearningAlgorithmForComputingBiasoptimalPolicies}.
% Gain-optimality is the least selective criteria an average reward reinforcement
% learning aims for.
\citet{Mahadevan96_AnAveragerewardReinforcementLearningAlgorithmForComputingBiasoptimalPolicies}
shows that \(n\)-discount-optimality for \(n=-1\) describes gain-optimality.

Further, for an unichain aperiodic\footnote{In the periodic case the Cesaro limit of degree \(1\)
  is required to ensure stationary state transition probabilities and thus stationary bias
  values \citep{Puterman94}. Therefore to ease readability we concentrate on unichain
  and aperiodic MDPs.} MDP problem, such as ergodic MDPs are, the average adjusted sum of rewards or
bias value is defined as
\begin{align*}
  V^{\pol}(s) = \lim_{N \to \infty}{ \E [ \sum_{t=0}^{N-1}(R_{t}^{\pol}(s) - \avgrew^{\pol} )]}\tcom
\end{align*}


where again \(R_{t}^{\pol}(s)\) is the reward received at time \(t\), starting in state \(s\) and
following policy \(\pol\). Note that the bias values are bounded due to the subtraction of the
average reward. Thus the bias value can be seen as the rewards that additionally sum up in case the
process starts in state \(s\). A policy \(\polopt\) that is gain-optimal is also bias-optimal, if it
maximises the bias values \(V^{\polopt}(s) - V^{\pol}(s) \geqslant 0\) in every state \(s\) and
compared to every other policy \(\pol\). Bias-optimality is given by setting \(n=0\) in the
definition of \(n\)-discount-optimality
\citep{Mahadevan96_AnAveragerewardReinforcementLearningAlgorithmForComputingBiasoptimalPolicies}.


Especially for highly probabilistic systems bias-optimality is important. To clarify this consider
Figure~\ref{fig:three-states} which again consists of two possible deterministic policies with the
only choice in state \(1\). Both policies have the same average reward of \(1\). However, only
taking the A loop is bias-optimal, as its policy \(\pol_{A}\) leads to bias values
\(V^{\pol_{A}}(1)=0.5\), \(V^{\pol_{A}}(0)=-0.5\) and \(V^{\pol_{A}}(2)=1.5\), while policy
\(\pol_{B}\) which selects the B loop generates bias values \(V^{\pol_{B}}(1)=-0.5\),
\(V^{\pol_{B}}(0)=-1.5\) and \(V^{\pol_{B}}(2)=0.5\).
%
This yields from the fact that the actions with non-zero rewards are selected earlier in policy
\(\pol_{A}\). Consider starting in state 1. Under the policy \(\pol_{A}\) the reward sequence is
\((2,0,2,0,\ldots)\), while for the other policy \(\pol_{B}\) it is \((0,2,0,2,\ldots)\).


\begin{figure}[t!]
  \centering
  \begin{tikzpicture}[thin, scale=0.75]
    % Nodes
    \draw (-3,0) node(0) [circle,draw,minimum size=25] {\footnotesize \(0\)};
    \draw (0,0)  node(1) [circle,draw,minimum size=25] {\footnotesize \(1\)};
    \draw (3,0)  node(2) [circle,draw,minimum size=25] {\footnotesize \(2\)};

    % Edges
    \path[thin, ->, bend right, >=stealth] (1) edge[above] node {\footnotesize\(2\)} (0);
    \path[thin, ->, bend right, >=stealth] (2) edge[above] node {\footnotesize\(2\)} (1);
    \path[thin, ->, bend right, >=stealth] (0) edge[below] node {\footnotesize\(0\)} (1);
    \path[thin, ->, bend right, >=stealth] (1) edge[below] node {\footnotesize\(0\)} (2);

    \draw (-1.5,0) node[] { A };
    \draw (1.5,0) node[] { B };


  \end{tikzpicture}
  \caption{\label{fig:three-states} A MDP with two gain-optimal deterministic policies, \(\pol_{A}\)
    going left in 1 is also bias-optimal, while \(\pol_{B}\) is not \citep[Adapted
    from][]{Mahadevan96_AverageRewardReinforcementLearningFoundationsAlgorithmsAndEmpiricalResults}.}
\end{figure}


\subsection{The Laurent Series Expansion of Discounted State Values}
\label{subsec:The_Laurent_Series_Expansion_of_Discounted_State_Values}


\cite{MillerVeinott1969} established the link between discounted RL state values
\(V_{\gamma}^{\pol}(s)\) and average reward RL values \(\avgrew^{\pol}(s)\) and \(V^{\pol}(s)\)
using the Laurent series expansion as
\begin{align*}
  V_{\gamma}^{\pol}(s) = \frac{\avgrew^{\pol}(s)}{1-\gamma} + V^{\pol}(s) + e_{\gamma}^{\pol}(s) \tcom
\end{align*}
%
where the error term \(e_{\gamma}^{\pol}(s)\) exists of infinitely many subterms and
\citet{Puterman94} shows that \(\lim_{\gamma \to 1} e_{\gamma}^{\pol}(s) = 0\). Note how the first
term depending on the average reward \(\avgrew^{\pol}(s)\) converges to infinity as \(\gamma\)
increases.


However, an important insight is the connection between \(n\)-discount-optimality and the Laurent
series expansion of \(V_{\gamma}^{\pol}(s)\). Each addend corresponds to one step in
\(n\)-discount-optimality. That is for \((-1)\)-discount-optimality the addend describes (a scaled
version of) the term to maximise for gain-optimality,
% the average reward must be
% maximised.
%
for \(0\)-discount-optimality the term to maximise for bias-optimality, and
%
finally \(n\)-discount-optimality for \(n \geqslant 1\) requires to maximise the error term
\(e_{\gamma}^{\pol}(s)\). The later only exists as \(\gamma\) is strictly less than \(1\). Thus this
term incorporates the number of expected steps and their corresponding reward on the path to the
Blackwell-optimal policy. Put differently, it optimises the expected reward collected to reach the
Blackwell optimal policy according to the occurrence on the paths, where shorter paths and those
which collect the rewards sooner are preferred.

% Note how the Laurent series expansion dissipates these values in single terms and therefore defines
% a divide and conquer methodological approach to reinforcement learning. Thus in average reward
% reinforcement learning these terms are learned separately, while in the discounted framework a
% single scalar value for each state with a fixed \(\gamma\)-value is estimated.

These addends, where \(n=-1,0,\ldots\) denote the coefficients of the Laurent series expansion and
thus correspond to the \(n\) of the definition of \(n\)-discount-optimality, can be reformulated to
following constraint problem \citep[p.346]{MillerVeinott1969,Puterman94}.
%
\begin{align}
  \label{eq:constr1}\avgrew^{\pol}(s) - \E[\avgrew^{\pol}(s)] & = 0 &  & \text{for } n = -1 \\
  \label{eq:constr2}\avgrew^{\pol}(s) + V^{\pol}(s) - \E[V^{\pol}(s)] & = R^{\pol}(s) && \text{for } n = 0 \\
  \label{eq:constr3}W^{\pol}_{n-1}(s) + W^{\pol}_{n}(s) - \E[W^{\pol}_{n}(s)] & = 0 && \text{for } n \geqslant 1, \text{where } W_{0}^{\pol}(s) = V^{\pol}(s)
\end{align}
%
The error term \(e_{\gamma}^{\pol}(s)\) is given by
\(e_{\gamma}^{\pol}(s) = \sum_{n=1}^{\infty}W^{\pol}_{n}(s)\) and the expected reward
\(R^{\pol}(s) = \E_{\pol}[ r(s, a) ]\). % = \sum_{a \in \mathcal{A}(s)}p(a | s) r(s, a)\)
%
\citet[p.343ff]{Puterman94} shows that due to the given degree of freedom if \(n=-1,0,\ldots,M\)
constraints are satisfying the above conditions for all states \(s\), then only
\(\avgrew^{\pol}(s), V^{\pol}(s), W^{\pol}_{1}, \ldots, W^{\pol}_{M-1}\) are unique, whereas
\(W^{\pol}_{M}\) is offset by the vector \(u\) where for the transition probability matrix \(P\) the
vector \(u\) is characterised by \((I-P)u = 0\). Note that \(u\) is determined by the number of
closed irreducible classes of \(P\), that is for ergodic MDPs \(u\) is determined by a single
constant. Average reward learning is based on the above formulation.


A major problem occurring at average reward RL is that the bias values are not uniquely defined
without solving the first set of constraints defined by the error term addends
\citep[see][p.346]{Puterman94,Mahadevan96_SensitiveDiscountOptimalityUnifyingDiscountedAndAverageRewardReinforcementLearning}.
% We could overcome this issue by simply requiring \(\gamma\) to be strictly less than \(1\).
%
Our algorithm, based on the tabular version of \citet{schneckenreither2020average}, does not require
the exact solution for \(V^{\pol}(s)\), but a solution which is offset suffices. Clearly this
observation reduces the required iteration steps tremendously as finding the exact solution,
especially for large discount factors, is tedious. Therefore, we allow to set \(\gamma = 1\), which
induces \(\X_{\gamma}^{\pol}(s) = V^{\pol}(s) + u\), where \(u\) is for unichain MDPs a scalar value
independent of \(s\), i.e.\@ equivalent for all states of the MDP~\cite[p.346]{Puterman94}.
%
%% Nonetheless, we require \(\gamma < 1\) to stabilise the process.
%
If we are interested in correct bias values, i.e. \(\gamma\) is sufficiently close but strictly less
than \(1\), our approach is a tremendous advantage over average reward RL as it reduces the number
of iterative learning steps by requiring only a single constraint per state plus one for the scalar
average reward value. That is, for an MDP with \(N\) states only one more constraint (\(N+1)\) has
to be solved in \ARA{} as compared to (at least) \(2N+1\) nested constraints for average reward RL.
Therefore, it is cheap to compute \(\X_{\gamma}^{\pol}(s)\), while it is rather expensive to find
the correct values of \(V^{\pol}(s)\) directly, especially in an iterative manner as RL is.

\subsection{Average Reward Adjusted Reinforcement Learning}
\label{subsec:Average_Reward_Adjusted_Reinforcement_Learning}

Thus, average reward adjusted RL can be seen as a specialised version of average reward RL, where it
targets unichain MDPs only. In the sequel we briefly present the needed formalism of average reward
adjusted RL. For more comprehensive version we refer to \citet{schneckenreither2020average}.

\begin{definition}
  The \textit{average reward adjusted (discounted) state value} $\X_{\gamma}^{\pol}(s)$ of a state
  $s$ under policy $\pol$ and with discount factor $0 \leqslant \gamma \leqslant 1$ is given as
  \(\X_{\gamma}^{\pol}(s) \defsym V^{\pol}(s) + e_{\gamma}^{\pol}(s)\).
\end{definition}
%
With the Laurent Series expansion this reformulates to
\(\X_{\gamma}^{\pol}(s) = V_{\gamma}^{\pol}(s) - \frac{\avgrew^{\pol}}{1-\gamma}\). Therefore,
\(X_{\gamma}^{\pol}(s)\) describes the discounted state value adjusted by the average reward term.
The Bellman Equation is given by
\(\X_{\gamma}^{\pol}(s) = \E_{\pol}[r_{t} + \gamma \X_{\gamma}^{\pol}(s_{t+1}) - \avgrew^{\pol} \mid
s_{t} = s]\). The corresponding Bellman optimality equation then is given by
\(\X_{\gamma}^{\polopt}(s) = \max_{a} \E_{\polopt}[r_{t} + \gamma \X_{\gamma}^{\polopt}(s_{t+1}) -
\avgrew^{\pol} \mid s_{t} = s]\). By turning this formula into an update rule it builds the
foundation of our algorithm.

\begin{algorithm}[t!]
  \begin{algorithmic}[1]
    \State{}Initialise state \(s_{0}\) and network parameters \(\netTarget, \netWorker\) randomly, set
    an exploration rate \(0 \leqslant \plearn \leqslant \pexp \leqslant 1\), exponential smoothing learning rates
    \(0 < \alpha, \gamma < 1\), and discount factors
    \(0 < \gamma_{0} < \gamma_{1} \leqslant 1\), where \(\gamma_{1} = 1\) is
    usually a good choice.
    \While{the stopping criterion is not fulfilled}
    \State{}\begin{minipage}[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}
      With probability \(\pexp\) choose a random action and
      probability \(1-\pexp\) one that fulfills
      \(\max_{a}\lexeq(\X^{\pol}_{\gamma_{1}}(s_{t},a; \netWorker{}),\X^\pol_ {\gamma_{0}}(s_{t},a; \netWorker{}))\). Let
      \(\isRandAct{t}\) indicate if the action was chosen randomly.
    \end{minipage}
    \State{}\begin{minipage}[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}
    Carry out action \(a_{t}\), observe reward \(r_{t}\) and resulting state \(s_{t+1}\).
    Store the experience \((s_{t}, a_{t}, \isRandAct{t}, r_{t}, s_{t+1})\) in the experience replay
    memory \(M\).
    \end{minipage}
    \State{Sample random mini-batch of experiences from \(M\)}
    \State{Reset gradients: \(d\netWorker{} \gets 0\)}
    \For{each experience \(i\) with \((s_{t}, \isRandAct{t}, a_{t}, r_{t}, s_{t+1})\)}
    \If{a non-random action was chosen or \(\pexp > \plearn\)}
    \State{\(
      \avgrew^{\pol}  \gets (1- \alpha) \avgrew^{\pol} + \alpha [r_{t} +
      \max_{a}\X^\pol_{\gamma_1}(s_{t+1},a; \netWorker{}) - \X^\pol_{\gamma_1}(s_{t},a_{t};
                        \netWorker{})]\)}
    \EndIf
    \State{\(y_{i,\gamma_0} \gets r_{t} + \gamma_{0} \max_{a}
                                        \X^\pol_{\gamma_0}(s_{t+1},a; \netTarget{}) - \avgrew^{\pol}\)}
    \State{\(y_{i,\gamma_{1}} \gets r_{t} + \gamma_{1} \max_{a}
                                        \X^\pol_{\gamma_1}(s_{t+1},a; \netTarget{}) - \avgrew^{\pol}\)}
    \State{Sum gradients on \({(y_{i,\cdot} - \X^\pol_{\cdot}(s_{t+1},a;
        \netWorker{}))}^{2}\) wrt. parameters \(\netWorker{}\) in \(d\netWorker{}\)}
    \EndFor
    \State{}%
    \begin{minipage}[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}
      Perform update with sum of the gradients \(d\netWorker{}\) on \(\netWorker{}\)
    \end{minipage}
    \State{Every \(C\) steps exponential smoothly set target network:
      \(\netTarget \gets (1 - \gamma) \netTarget{} + \gamma \netWorker{}\)}
    \State{Set \(s \gets s'\), \(t \gets t+1\) and decay parameters}
    \EndWhile{}
  \end{algorithmic}
  \caption{\label{alg:near}Basic near-Blackwell-optimal deep RL for unichain MDPs}
\end{algorithm}


\paragraph{Average Reward Adjusted Deep RL Algorithm.} %
The model-free average reward adjusted RL algorithm is based on the tabular version of
\citet{schneckenreither2020average} and depicted in Algorithm~\ref{alg:near}. The algorithm operates
on a target network \(\netTarget{}\) and a worker network \(\netWorker{}\) as in
\citet{mnih2015human}. % The target network reduces fluctuations in the state value estimates and
% therefore reduces the
%
In the action selection process a \(\epsilon\)-sensitive lexicographic order
\(a=(a_{1},\ldots,a_{n}) \lex (b_{1},\ldots,b_{n})=b\) is used. It is defined as \(a \lex b\) if and
only if \(| a_{j} - b_{j} | \leqslant \epsilon\) for all \(j < i\) and
\(|a_{i} - b_{i}| > \epsilon\). Note that the resulting sets of actions may not be disjoint, but
taking the maximum as required in our algorithm is straight-forward and thus cheap to compute. The
first action selection criteria selects the actions which maximise the bias values, the second the
error term.
%
Equation~\ref{eq:constr2} of the average reward RL constraints is used to estimate the average
reward based on the \MS[t]{reformulate avg RL constraints using s' etc.?} currently inferred state
values of two consecutive states and the returned reward. % As the average
% reward is equal among all states estimating the average reward is based on many observations.
This infers a solid average reward prediction of the current policy in few steps only and thus
quickly leads to better policies, as ones with higher gain are preferred.
\citet{schneckenreither2020average} and
\citet{Tadepalli98_ModelbasedAverageRewardReinforcementLearning} find that updating the average
reward by Equation~\ref{eq:constr2} is superior as compared to exponentially smoothing the actual
observed rewards. This makes sense as the later rather evaluates past policies.

The average reward adjusted state value, which is approximated based on the Bellman optimality
equation, is estimated twice. Once to infer the bias values with a very high discount factor
\(\gamma_{1}\), e.g.\@ \(\gamma_{1} = 1.0\), and a second time with a discount factor
\(\gamma_{0} < \gamma_{1}\). The later is used to more selectively choose actions in case more than
one action leads to bias-optimal policies. The predetermined discount value \(\gamma_{0}\) is a
measure for farsightedness. Lower values prefer to partially collect reward in fewer steps and
higher values are used to prefer actions for which the full bias is collected earlier.
%
The corresponding update values are saved to calculate the gradients based on the quadratic errors.
Every \(C\) steps the target network smoothly adapts the parameters of the worker network.


\section{Experimental Evaluation}
\label{sec:Experimental_Evaluation}


This section introduces the simulation model and the parameterisation of the algorithm. To provide a
proof-of-concept for the proposed algorithm we adapt the flow shop presented in
\citet{schneckenreither2020order}. However, our algorithms operates on discrete lead times on a
product type basis and thus is not directly comparable to their order based lead time management
approach. Nonetheless, this section is largely based on the corresponding section of
\citet{schneckenreither2020order}.

\subsection{Simulation Model}
\label{subsec:Simulation_Model}

We adapt the simulation model of a make-to-order (MTO) flow shop with six different
products and six work-centers of \citet{schneckenreither2020order} and examine the system under
moderate and high utilisation in combination with moderate or high variability in demand and
processing times. This section provides the details of the simulation model and the experimental
setup.

% \subsection{Make-To-Order Flow Shop}
% \label{subsec:make-to-order_flow_shop}
% \subsection{Simulation model}
% \label{subsec:Simulation_model}


The simulation model is depicted in Figure~\ref{fig:ps}, where the customers initiate the process by
placing orders as indicated by the dashed line.
%
\begin{figure*}[t!]
  \centering
  \input{figures/productionsystem}                      % Figure production system
  \caption{Production System of the Simulation Model with routing, processing time distributions and
    demand interarrival time distributions.} \label{fig:ps}
\end{figure*}
%
%
The manufacturer produces six different product types, each of which is identified by a natural
number ${1,2,\ldots,6}$ and has a different routing through the production system. The routing setup
is provided by corresponding labels of the edges. For instance, product types $1-3$ are routed from
machine $M1$ to machine $M2$, while the other products are transferred to machine $M3$. The
production system consists of three production stages with diverging material flow and no return
visits.
%
The incoming orders are uniformly distributed among the product types.
%
A period lasts $960$ minutes ($=16$ hours) which represent two \(8\)-hour shifts. At the
beginning of each period orders can be released into the production system assuming material
availability for all orders at their release date. Upon release, the orders are
placed in the buffer at machine $M1$. Buffers and inventories are plotted as filled squares. Thus,
each workstation is equipped with a buffer which queues the orders until processing is started. All
buffers use a first-come-first-serve dispatching rule.
%
Furthermore, each workstation processes only one order at a time and early deliveries are
prohibited. Orders completed ahead of their due date remain in the finished goods inventory (\fgi{})
until they are due and sent to the customer.


\paragraph{Processing Times.} The machine processing time distributions are given under the
corresponding node labels of the machines. To simulate high or moderate variance the processing
times for all machines are  drawn from an exponential or an uniform distribution,
respectively. There is one bottleneck machine ($M5$) and therefore we refer to product
types $2$ and $5$, those routed through machine $M5$, as
bottleneck products whereas products $1,3,4,6$ are non-bottleneck products.

\paragraph{Demand.} The incoming orders are placed in the order pool with a due date slack of
\dds{} periods, that is each order arriving within period $t$ will be due at the end of period
$t+\dds{}$. The due date slack of \(\dds{}\) was chosen to (i) ensure sufficient time between the
first occurrence of orders in the order pool and the latest possible release of orders and (ii) to
have a clear cause and effect relationship between the order release decision and the cost
performance in the analysis.
%
To simulate high and moderate variability of the demand process the interarrival time between
consecutive incoming orders is either drawn from an exponential ($\Exp$) or an uniform ($\Unif$)
distribution, which are adjusted to yield the desired bottleneck utilisation level of either $70\%$
with $\Exp(135)$ and $\Unif(95,175)$, or $80\%$ with $\Exp(118)$ and $\Unif(78,158)$. These values
were chosen as they comprise the non-linear relationship between flow times and high utilisation
levels.

\paragraph{Order Release.}
%
Before the order release decision is made, all orders in the order pool are sorted by due date. This
portrays the implementation of a sequencing rule. According to this sequence, orders are considered
for release in the beginning of each period starting with the highest priority order.

In our model, orders are released by specifying lead times
$\lt_{1}, \lt_{2}, \ldots, \lt_{6} \geqslant 1$, where the index corresponds to the product type and
$\lt_{i} \in \mathbb{N}$.
%
%
By setting a lead time $\lt_{i}$ a \emph{planned release date} is computed for each
order $j$ of product type $i$ in the order pool given by
%
\begin{linenomath*}
  \begin{equation}
    \prd{}_j= \dd{}_j - \lt_{i}\ ,
    \label{eq:Orel}
  \end{equation}
\end{linenomath*}

where $\dd_{j}$ denotes the due date of the order.  For dynamic order releases the lead times
$\lt_{i}$ may vary from period to period, whereas in static order release methods the lead times are
predetermined and fixed (\citealt{RagatzMabert1988,KimBobrowski1995}).
%
For both approaches a job of product type $i$ with lead time $\lt{}_{i}$ in period $t$ and due date
$\dd_{j}$ is released at the end of period $t$ if and only if $t \geqslant \prd{}_{j}$.
%
Thus, in the dynamic setting  the planned
release date $\prd{}_{j}$ of order $j$ can be updated several times before it is actually released
to the production system.
%
However, once an order is released its release cannot be revoked.
%
% If there are several orders with the same due date the order release
% sequence is randomized.
%
Hence, if the set lead time corresponds with the actual flow time the product is finished in the
same period as it is shipped. If the lead times are set either too long or short,
the order has to wait in the \fgi{} until it is due or the order is late and backorder (BO) costs
occur.

\paragraph{Costs.}
%
The cost parameters are set by assuming an increase in value from raw material ($1$~Dollar per order
and period) to the final product ($4$~Dollar per order and period) and the backorder costs are set
very high ($16$~Dollar per order and period) due to the MTO environment. All costs are assessed
based on the production system state at the end of each period.


\subsection{Conventional Order Release Rule}

As external benchmark for comparison we use different parameterized backward infinite loading
(\BIL{}) techniques \citep{Ackerman1963}. \BIL{} uses Equation~\ref{eq:Orel} with a predetermined
and fixed lead times \(\lt_{i}\).
%
% \begin{equation*}
%   \rd_{j} = \dd_{j} - \lt_{j} \tcom
%   \label{eq:BIL}
% \end{equation*}
%
% where the release date \(RD_{j}\) of product type \(j\) is calculated by the difference of the due
% date \(DD_{j}\) and the lead time \(LT_{j}\).
%
% Note that we set lead times for each product group and not for every order as done by
% \cite{Ackerman1963}.


\subsection{Algorithm Setup}
\label{subsec:Algorithm_Setup}

\MS[t]{Algorithm adaptions}

\paragraph{Markov Decision Process.} The underlying MDP is unichain and looks as follows. Both,
state space $\mathcal{S}$ and action space $\mathcal{A}$ are discrete.
%
% Recall that we gproducts routed over the bottleneck machine \(M5\) are bottleneck products.
%
For the \textit{state space} we adapt the result of \citep{knollmann2013control}, which state that
the lead times ought to control the WIP level, instead of the due date reliability as changes are
directly seen. Thus, any state $s \in \mathcal{S}$ of the state space is composed of the following
information for both groups \(g\), i.e.\@ for bottleneck and non-bottleneck products:
\begin{itemize}

\item The currently set lead time $LT_{g} \in \{1,2,\ldots,\DDS\}$ (Recall:
  $\text{Due Date Period} - \text{Lead Time} = \text{Release Period}$). Note that we bound the
  maximum lead time with due date slack $\DDS$, which is $7$ in our setup.

\item Counters $OP_{g,d}\in \N$ for the number of orders in the order pool divided in time buckets
  with $d \in \{1,2,\ldots,\DDS \}$, which stands for the number of periods until the due date.

  % \item Flag for each machine whether it is idle or not.

\item Counters $Q_{g,i,d}\in \N$ for the number of orders in the machine and queue \(i \in
  \{M_{1}, M_{2}, \ldots , M_{6}\}\) divided in
  time buckets with $d \in \{1,2,\ldots,\DDS \}$, which stands for the number of periods until the
  due date.
  % \MS[t]{depend}If a product type is not routed over the corresponding machine, then no entry for this
  % product type is specified.

\item Counters $FGI_{g,d} \in \N$ of orders in the finished goods inventory divided in time
  buckets with $d \in \{1, 2,\ldots, \DDS \}$, which stands for the number of periods until the
  due date. Orders with a due date with more than 3 periods ago are listed in the counter
  $FGI_{-3}$.

\item Counters $S_{g,d} \in \N$ of shipped orders from the last period divided in time buckets
  with $d \in\{-3,-2,\ldots, \DDS \}$, which stands for the number of periods until the due date.
  Orders with a due date with more than 3 periods ago are listed in the counter $S_{-3}$.

\end{itemize}

The algorithm implicitly learns a function which maps the current state of the production
system to a lead time update and thus a release decision. In an optimal situation it does so by
multiply exploring every action for each state and assessing its economic viability.
%
Orders are released once the due date is within the interval $[t,t+\text{lead time}]$, whereas $t$
is the current period. Clearly, this yields bulk releases, meaning that either all or no orders with
the same due date and of the same product group are
released.

The \textit{action space} is composed of two decisions, the relative change of the lead times to the
currently set lead times $LT_{g} \in \{1,2,\ldots,\DDS\}$ for each product group $g$.
%
We restrict the lead time update for state $s_{t+1}$ according to the lead time $LT_{g}$ from state
$s_{t}$ by a maximum change of 1. Thus, if $LT_{g}$ is the current lead time of product group $g$
the agent can choose a lead time in $\{1,2,\ldots,\DDS\} \cap \{LT_{g}-1,LT_{g},LT_{g}+1\}$. % Put
% differently the algorithm can increase or decrease the lead time by 1 or leave it as it is, as long
% as it acts within the discrete action space given by the set $\{1,2,\ldots,\DDS\}$ for each product
% group.
A naive approach is to define one action for each combination of actions over all product groups.
Although applicable in our setup, as we aggregate the data into the groups of bottleneck and
non-bottleneck products and thus yields \(9\) actions, it simply does not scale.


\begin{figure*}[t!]
  \centering
  \input{figures/indep_agents}                      % Figure production system
  \caption{Illustration of independent agents as agent one increases the lead time}\label{fig:indep-agents}
\end{figure*}
%
Therefore, we invent and implement \textit{independent agents}, each operating on the same system
state, but choosing actions independently for the designated product group.
Figure~\ref{fig:indep-agents} illustrates the idea, where wlog. we use two agents and for simplicity
omit the reward and truncate the state to the lead times. Agent one chooses to increase the lead
time from \(3\) to \(4\). Thus, the future state, seen from agent one, depends on the probability of
agent two choosing to increase (\(\inc\)), decrease (\(\dec\)) or not change (\(\pers\)) the lead
time. Note that the Markov property the agent, that is
\(p(s_{t+1}, r_{t} \mid s_{t}, a_{t}) = p((4,\cdot), r_{t} \mid (3,3), \inc) \), is not touched by
this adaption. While for agent one the reward and future state only depends on the current state and
action taken, the corresponding probabilities for future state and reward change during the learning
process as the other agent starts to behave differently.
%
\MS[t]{independent agents, true?}This allows us to investigate the algorithms behaviour for even
larger solution spaces. In particular, this adaption enables us to present the algorithm the above
defined solution space not only on the basis of product groups \(g\), but also for each product type
\(p\). That means, the state space, as well as the action space operates on a product type basis.
Note that without independent agents this is infeasible due to \(3^{6} = 729\) possible actions.
Removing the dependency between the lead times of different product types reduces the possibilities,
and thus the number of ANN output layer nodes, to \(18\). Therefore, this is a vital adaption that
ensures scalability.


        %         We allow the agent a higher increase
        %         of the lead time, compared to the decrease, to give it the chance to react quickly to high
        %         number of
        %         incoming demand.


\paragraph*{Reward.}
In each period the agents choose actions, which when executed generate a reward while traversing to
the next period by simulating the production system. The rewards are the accumulated costs at the
end of the period consisting of the backorders, the WIP orders and the number of orders in the
FGI\@. The objective is to minimise the returned reward. Therefore, we adapt the algorithm by
swapping all maximisation functions with minimisations.

\paragraph*{Optimised Algorithm.}
Our implementation includes several optimisations, that help the algorithm in better exploring the
solution space and stabilise the learning process. These include the above discussed independent
agents that operate on the same state, \(n\)-step learning \citep{mnih2016asynchronous},
overestimating the average reward \(\avgrew^{\pol}\) and ensuring to constantly reduce the average
reward as searching for policies with higher average costs is unnecessary. Algorithm~\ref{alg:full}
depicts a detailed view on the algorithm including these extensions and with the minimisation
objective. The idea is the same as in Algorithm~\ref{alg:near}, except that the action selection
procedure is a loop over all independent agents and the implementation of the n-step logic.

% N-Step
N-step RL collects a set of consecutive experiences, in our case retrieved from the replay memory,
and connects the state values starting from the latest observed experience, such that the newly
computed state value of the corresponding experience propagates to the experience of the previous
period. This allows a faster convergence, as values bootstrap over multiple periods in contrast to
just one step.
% Overestimating
The idea behind overestimating the average reward is to constantly seek for better policies than the
currently known. We overestimate the reward by subtracting \(10\%\) of the computed average reward
\(\avgrew^{\pol}\) and an exponentially smoothed average reward of the actually returned reward
values of greedy actions. Thus, if the computed and actually observed average reward coincide, the
overestimation equals \(0\).
% Rho Minimum
Preventing an increase of the average reward with \(\avgrew_{\max}^{\pol}\) reduces the search space
and provides a methodology for constantly improving policy.

\MS[t]{(shared rho)?, (gradient clipping)?}

Furthermore, we sometimes use multiple parallel workers (parallel RL) that operate on the same
environment to collect a broader range of experiences. The experiences are stored according to the
first taken action in the n-step series. We uniformly sample over the actions to ensure each action
is constantly trained. This reduces catastrophic forgetting.

The algorithm initiation tactics is to first fill the replay memory buffer by skipping any learning
(steps \(10-25\)) until the period, that corresponds to the replay memory size, is reached. Then
for \(500\) steps the average reward is set to the last computed exponentially smoothed value to
prevent divergence due to the untrained network weight initialisation values.


% \MS[t]{Extensions: overestimate rho, rho minimum, Multiple Agents (shared state,  shared rho?),
%   init-phase (0.5 for expSmthRewRate, always adapt rho, same rho for all agents): exp smooth reward
%   as rho, replay memory actions, n-step (describe replay memory), gradient clipping }
% \MS[t]{Side info: Replay memory also stores (disallowed) filtered action indices for optimisation purposes and on
% episodic tasks needs to store a bool indicating an episode end. Uses Int8 for state features values.  }


\begin{algorithm}[t!]
  \begin{algorithmic}[1]
    \State{}Initialise state \(s_{0}\) and network parameters \(\netTarget, \netWorker\) randomly,
    set an exploration rate \(0 \leqslant \plearn \leqslant \pexp \leqslant 1\), exponential
    smoothing learning rates \(0 < \alpha_{\max}, \alpha, \gamma < 1\), a sufficiently large default
    value for \(\avgrew_{\max}^{\pol}\) and discount factors \(0 < \gamma_{0} < \gamma_{1} \leqslant
    1\)
    \While{the stopping criterion is not fulfilled}
    \State{\(\isRandAct{t} \gets \mathsf{False}\)}
    \ForEach{independent agent \(\agent \in \agent_{1},\agent_{2},\ldots,\agent_{n_{\mathsf{ag}}}\)}
    \State{}%
    \begin{minipage}[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth-\leftmargin+2pt}
      With probability \(\frac{\pexp}{n_{\mathsf{ag}}}\) choose a random action and
      probability \(1-\frac{\pexp}{n_{\mathsf{ag}}}\) one that fulfills
      \(\min_{a \in \mathcal{A}(\agent)}\lexeq(\X^{\pol}_{\gamma_{1}}(s_{t},a; \netWorker{}),\X^\pol_ {\gamma_{0}}(s_{t},a;
      \netWorker{}))\). Set \(\isRandAct{t} \gets \mathsf{True}\) if the action was chosen randomly
    \end{minipage}
    \EndFor
    \State{}%
    \begin{minipage}[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}
    Carry out combined action \(a_{t}\), observe reward \(r_{t}\) and resulting state \(s_{t+1}\).
    Store the experience \((s_{t}, a_{t}, \isRandAct{t}, r_{t}, s_{t+1})\) in the experience replay
    memory \(M\).
    \end{minipage}
    \If{\(t \mod{\nstep} \equiv 0\)}
    \State{Reset gradient: \(d\netWorker{} \gets 0\)}
    \State{}%
    \begin{minipage}[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth-\leftmargin+2pt}
      Sample random mini-batch of \(\nstep\) consecutive experiences
      from \(M\) numbered by \(1 \leqslant i \leqslant \nstep\), each
      with \((s_{i}, \isRandAct{i}, a_{i}, r_{i}, s_{i+1})\).
    \end{minipage}
    \ForEach{mini-batch}
    \ForEach{experience, starting with the latest seen experience \(\nstep\)}
    \If{experience \(\nstep\) or (\(\isRandAct{i}\) and  \(\pexp \leqslant \plearn\))}
      \State{\(\target_{\gamma0} \gets \min_{a} \X^\pol_{\gamma_0}(s_{i+1},a; \netTarget{})\)}
      \State{\(\target_{\gamma1} \gets \min_{a} \X^\pol_{\gamma_1}(s_{i+1},a; \netTarget{})\)}
    \EndIf
    \If{\(\isRandAct{i}\) or \(\pexp > \plearn\)}
    \State{\(\avgrew^{\pol} \gets (1- \alpha) \avgrew^{\pol} + \alpha [r_{i} +
      \min_{a}\X^\pol_{\gamma_1}(s_{i+1},a; \netWorker{}) - \X^\pol_{\gamma_1}(s_{i},a_{i};
      \netWorker{})]\)}
    \State{\(\avgrew^{\pol} \gets \min(\avgrew^{\pol}, \avgrew_{\max}^{\pol})\)}
    \State{\(\avgrew_{\max}^{\pol} \gets (1- \alpha_{\max}) \avgrew_{\max}^{\pol} +
    \alpha_{\max} 1.025 \avgrew^{\pol}\)}
    \EndIf
    \State{Compute overestimate of \(\avgrew^{\pol}\) and store result in \(\avgrew_{\Overestim}^{\pol}\)}
    \State{\(\target_{\gamma_0}  \gets r_{i} + \gamma_{0} \target_{\gamma_{0}} - \avgrew_{\Overestim}^{\pol}  \)}
    \State{\(\target_{\gamma_{1}}  \gets r_{i} + \gamma_{1} \target_{\gamma_{1}} - \avgrew_{\Overestim}^{\pol}\)}
    \State{\(d\netWorker{} \gets d\netWorker{} + \partial{(\target_{\gamma_{0}} - \X^\pol_{\gamma_{0}}(s_{i+1},a; \netWorker{}))}^{2} / \partial\netWorker{}\)}
    \State{\(d\netWorker{} \gets d\netWorker{} +
      \partial{(\target_{\gamma_{1}} - \X^\pol_{\gamma_{1}}(s_{i+1},a; \netWorker{}))}^{2} / \partial\netWorker{}\)}
    \EndFor{}
    \EndFor{}
    \State{Perform update of \(\netWorker{}\) using \(d\netWorker{}\)}
    \EndIf{}
    \State{Every \(C\) steps exponential smoothly set target network:
    \(\netTarget \gets (1 - \gamma) \netTarget{} + \gamma \netWorker{}\)}
    \State{Set \(s \gets s'\), \(t \gets t+1\) and decay parameters}
    \EndWhile{}
  \end{algorithmic}
  \caption{\label{alg:full}Optimised near-Blackwell-optimal deep RL for unichain MDPs}
\end{algorithm}


\paragraph*{Neural Network Setup.}
%
Due to the vast possibilities the neural network architecture was manually optimised, starting with
less stochastic systems and aiming for a coffin shape that can be used in all setups. During this
process we constantly adapted and reevaluated the best network on various production system
configurations. The resulting architecture depends on the number of features \(\anninp\) and the
number of output nodes \(\annout\), which itself depends on the number of actions
\(\acts = \size{\States}\), the independent agent configuration, and the number of functions that
are approximated. For the later, in case of the proposed algorithm this is \(2\),
\(\X^\pol_{\gamma_{0}}\) and \(\X^\pol_{\gamma_{1}}\), while deep Q-Learning only approximates the
function \(Q_{\gamma}^\pol\).
%
The resulting feedforward deep ANN architecture is given as %
\(\anninp %
\rightarrow 3 \anninp % \rightarrow \relu
\rightarrow \anninp % \rightarrow \relu
\rightarrow 2 \annout % \rightarrow \relu
\rightarrow \annout %
% \rightarrow   \leakyTanh%
\), where each arrow symbolises a fully-connected layer with a rectified linear unit (\(\relu\)) as
activation function. As output activation function we use the hyperbolic tangent function
(\(\Tanh\)), but similar to leaky \(\relu\), leak values greater than \(0.98\) (less than \(-0.98\))
with \(5\%\). The idea is that we do not want to restrict the approximations to values inside this
range, but values outside of it are less important and thus can be less precise.

The values \(\anninp\) and \(\annout\) depend on the feature extraction method. In case of an
aggregation to product groups \(g\) we have \(\anninp=180\), and due to \(6\) possible actions for
the two independent agents, \(\annout=12\).
% SpecFullyConnected 180 540 :=> SpecRelu (540,1,1) :=> SpecFullyConnected 540 180 :=> SpecRelu
% (180,1,1) :=> SpecFullyConnected 180 24 :=> SpecRelu (24,1,1) :=> SpecFullyConnected 24 12 :=>
% SpecReshape (12,1,1) (6,2,1) :=> SpecLeakyTanh 0.98 (6,2,1) :=> SpecNNil2D 6x2
In the case of the full representation using 6 independent agents the setup is \(\anninp=540\) and
\(\annout=36\).
% SpecFullyConnected 540 1620 :=> SpecRelu (1620,1,1) :=> SpecFullyConnected 1620 540 :=> SpecRelu
% (540,1,1) :=> SpecFullyConnected 540 72 :=> SpecRelu (72,1,1) :=> SpecFullyConnected 72 36 :=>
% SpecReshape (36,1,1) (18,2,1) :=> SpecLeakyTanh 0.98 (18,2,1) :=> SpecNNil2D 18x2
For the Q-Learning algorithm, which only approximates one function, the output nodes
\(\annout\) are halved for both setups.

The ANN is trained using the Adam optimiser \citep{kingma2014adam} with learning rate \(0.0001\),
and the default beta-values \(\beta_{1}=0.9\), and \(\beta_{2}=0.999\). As neural network
initialisation method we use the commonly known Xavier initialisation
\citep{glorot2010understanding}, i.e.\@ the initial value for each weight \(i\) is sampled from a
uniform distribution \(W_{l,i} \sim \Unif(-1/\sqrt{n_l},1/\sqrt{n_l})\), where \(n_{l}\) is the
number of nodes in layer \(l\).


\section{Computational Experiments}
\label{sec:Preliminary}

This section provides preliminary results for the described setup and gives an overview of the
current performance of the algorithm compared to the conventional static lead time setting
algorithms.

Currently we evaluate two different kinds of reward reporting as they have been proposed by
\cite{Schneckenreither2019}. They find that it is beneficial to overcome the time offset imposed by
the production system. That is, although orders are immediately released to the production system
and thus generate a response in terms of a reward to the agent, the full impact of the chosen action
takes several periods to materialise. For instance, by deciding not to release an order, which later
becomes a backorder, it is beneficial to reward the actions actually responsible for the backorder
instead of the last action (only). Thus they propose to keep track of the orders currently in the
order pool until all fully traversed through the production system and reward the actions according
to all orders in the order pool.

\begin{table}[t]
  \centering
  \begin{tabular}{|l||c|c|c|c|c|c|c|c|c|c|}
    \hline
    Parameter & $\alpha$ & $\beta$ & $\delta$ & $\gamma$ & $\epsilon$ & $p_{exp}$ & $\xi$ \\
    \hline
    Value & 0.5 & 0.15 & 0.15 & 0.15 & 0.5 & 1.0 & 0.15 \\
    \hline
  \end{tabular}
  \caption{\label{tbl:params}Parameter Setup.}
\end{table}

The parameter setup is given in Table~\ref{tbl:params}. The parameters are exponentially decayed
with rate \(0.05\) and \(350k\) steps\footnote{The decay is defined as \(0.05 ^{(t/350k)}\), where
  \(t\) is the current period}. We implemented the deep Q-network algorithm proposed by
\cite{mnih2015human}, which is parameterised by learning rate \(\delta\) and a discount factor of
\(0.99\).


% \definecolor{lightgray}{rgb}{0.83, 0.83, 0.83}
% \begin{table}[tb]
%   \begin{center}
%     \begin{tabular}{c|ccc|c|c||c}
%       \hline
%       70-\(\Exp\)-\(\Unif\) & WIPC & FGIC & BOC & FGIC+BOC & SUM & SFTT & SL(\%) \\
%       \hline                &
%        ESSLTA{}             &      &      &     &          &     &      &        \\
%        ANN{}                &      &      &     &          &     &      &        \\
%        ESSLTB{}             &      &      &     &          &     &      &        \\
%        BILD{}               &      &      &     &          &     &      &        \\
%       \hline                &
% %
%       \hline                &
%       70-\(\Unif\)-\(\Exp\) & WIPC & FGIC & BOC & FGIC+BOC & SUM & SFTT & SL(\%) \\
%       \hline                &
%        ESSLTA{}             &      &      &     &          &     &      &        \\
%        ANN{}                &      &      &     &          &     &      &        \\
%        ESSLTB{}             &      &      &     &          &     &      &        \\
%        BILD{}               &      &      &     &          &     &      &        \\
%       \hline                &
% %
%       \hline                &
%       70-\(\Exp\)-\(\Exp\)  & WIPC & FGIC & BOC & FGIC+BOC & SUM & SFTT & SL(\%) \\
%       \hline                &
%        ANN{}                &      &      &     &          &     &      &        \\
%        ESSLTA{}             &      &      &     &          &     &      &        \\
%        ESSLTB{}             &      &      &     &          &     &      &        \\
%        BILD{}               &      &      &     &          &     &      &        \\
%     \end{tabular}
%   \end{center}
%   \caption{\label{tbl:70} Results for a bottleneck utilisation of 70\%. }
% \end{table}

\begin{table}[tb]
  \begin{center}
    \begin{tabular}{c|rrr|c|c||c|c|c|c|c}
      \hline
      80-\(\Exp\)-\(\Unif\)   & WIPC   & FGIC    & BOC      & FGIC+BOC & SUM     & SFTT  & SFTT+FGIT & SL(\%) & Cost.p.P & \(\avgrew^{\pol}\) \\
      \hline
      \rl{0.995}              & 55.22 & 137.00 & 52.09   &          & 244.32 & 1.51 & 2.60    & 0.915  & 34.902   & 32.791             \\
      \rl{1.000}              & 67.27 & 102.18 & 175.69  &          & 345.13 & 1.52 & 2.63    & 0.908  & 49.305   & 30.777             \\
      \BILOne{}               & 45.90 & 0.00   & 734.43  &          & 780.33 & 1.32 & 1.81    & 0.352  &          &                    \\
      \BILTwo{}               & 45.73 & 80.31  & 1425.22 &          & 268.57 & 1.32 & 2.158    & 0.876  &          &                    \\
      \BILThree{}             & 45.86 & 279.72 & 30.82   &          & 356.40 & 1.32 & 3.034    & 0.975  &          &                    \\
      \hline
%
%       \hline
%       80-\(\Unif\)-\(\Exp\) & WIPC   & FGIC    & BOC      & FGIC+BOC & SUM     & SFTT  & SFTP+FGI & SL(\%) & Cost.p.P & \(\avgrew^{\pol}\) \\
%       \hline                &
%        ESSLTA{}             &        &         &          &          &         &       &          &        &          &                    \\
%        ANN{}                &        &         &          &          &         &       &          &        &          &                    \\
%        ESSLTB{}             &        &         &          &          &         &       &          &        &          &                    \\
%        BILD{}               &        &         &          &          &         &       &          &        &          &                    \\
%       \hline
% %
%       \hline
%       80-\(\Exp\)-\(\Exp\)  & WIPC   & FGIC    & BOC      & FGIC+BOC & SUM     & SFTT  & SFTP+FGI & SL(\%) & Cost.p.P & \(\avgrew^{\pol}\) \\
%       \hline                &
%        ANN{}                &        &         &          &          &         &       &          &        &          &                    \\
%        ESSLTA{}             &        &         &          &          &         &       &          &        &          &                    \\
%        ESSLTB{}             &        &         &          &          &         &       &          &        &          &                    \\
%        BILD{}               &        &         &          &          &         &       &          &        &          &                    \\
    \end{tabular}
  \end{center}
  \caption{\label{tbl:80} Results for a bottleneck utilisation of 80\%. }
\end{table}


% \definecolor{lightgray}{rgb}{0.83, 0.83, 0.83}
% \begin{table}[tb]
%   \begin{center}
%     \begin{tabular}{c|ccc|c|c||c}
%       \hline
%       80-\(\Exp\)-\(\Unif\)   & WIPC      & FGIC      & BOC       & FGIC+BOC  & SUM   & SL(\%) \\
%       \hline
%        ESSLTA{}     &           &           &           &           &       &                  \\
%        ANN{}        &           &           &           &       &       &                      \\
%        ESSLTB{}     &           &           &           &       &       &                      \\
%        BILD{}       &           &           &           &       &       &                      \\
%       \hline
% %
%       \hline
%       80-\(\Unif\)-\(\Exp\)   & WIPC      & FGIC      & BOC       & FGIC+BOC  & SUM    & SL(\%) \\
%       \hline
%        ESSLTA{}     &           &           &           &           &       &      \\
%        ANN{}        &           &           &           &           &       &      \\
%        ESSLTB{}     &           &           &           &           &       &      \\
%        BILD{}       &           &           &           &           &       &      \\
%       \hline
% %
%       \hline
%       80-\(\Exp\)-\(\Exp\) & WIPC     & FGIC      & BOC       & FGIC+BOC  & SUM   & SL(\%) \\
%       \hline
%        ANN{}     &          &           &           &           &       &      \\
%        ESSLTA{}  &          &           &           &           &       &      \\
%        ESSLTB{}  &          &           &           &           &       &      \\
%        BILD{}    &          &           &           &           &       &      \\
%     \end{tabular}
%   \end{center}
%   \caption{\label{tbl:80} Results for a bottleneck utilisation of 70\%. }
% \end{table}


% Table~\ref{tbl:res} shows the results, where we evaluate the average reward and discounted
% reinforcement learning algorithm by providing either direct reward feedback, that is the reward
% accumulated in the next period, without keeping track of any orders, as \(\ActShipped\) and
% \(\ActShippedDQN\) respectively. Further both algorithms are evaluated with keeping track of the
% order pool orders, such that each action is responsible for all orders in the order pool. These are
% labelled \(\ActOrdPool\) and \(\ActOrdPoolDQN\). For comparisons reasons we provide
% \(\BILOne-\BILSix\) and immediate release (interval release).

% The results show that at the current time we are facing issues with scaling the algorithm to the
% order release problem. Likely this is due to parameterisation issues, which we are investigating
% currently. However, it can be seen that i) the technique of keeping track of the order pool orders
% leads to better policies and ii) that despite the parameterisation issues average reward
% reinforcement learning finds better solutions than its discounted counterpart. However, the least
% costs are generated by \(\BILThree\), which shows that there is room for improvement for the
% reinforcement learning policies.

% Interesting is also to see, that all reinforcement learning variants are releasing rather late,
% leading to low finished goods inventory costs (FGIC), but high backorder costs (BOC). Furthermore,
% the agents based on the order-pool orders, namely \(\ActOrdPool\) and \(\ActOrdPoolDQN\), are able
% achieve lower work-in-process costs (WIPC) and also lower tardiness, than the other the agents
% rewarded directly on the shipped orders.


% \begin{table}[t!]
%   \centering
%   \def\arraystretch{1.2}          %  1 is the default, change whatever you need
%   \setlength\tabcolsep{4pt}
%   \begin{tabular}{|l||c|c|c|c|c|c|c|c|c|}
%     % BEGIN RECEIVE ORGTBL res2
%     \hline
%     Algorithm/KPI & SUM & BOC & FGIC & WIPC & % \%SL &
%                                                        TARD & \(\sigma\)TARD & SFTT\\
%     \hline
%     % Evaluation 1 & \multicolumn{11}{c|}{} \\
%     \ActShipped    & 1777.74 & 1322.04 & 94.49 & 361.20       & 2.24 & 1.52 & 3.44\\
%     \ActOrdPool    & 1593.03 & 1318.65 &14.01 &260.36         & 2.11 & 1.42 & 2.68\\
%     \ActShippedDQN & 1940.62 & 1570.40 &6.30 &363.92          & 2.35 & 1.59 & 3.95 \\
%     \ActOrdPoolDQN & 1618.88 & 1306.20 & 40.68& 272.00        & 2.08 & 1.38 & 2.58\\
%     \BILOne        & 1736.98 & 1510.42  &  0.00  & 226.56     & 2.04 & 1.26 & 2.16 \\
%     \BILTwo        & 1081.79 & 769.86 & 86.89   & 225.03      & 1.87 & 1.18 & 2.15\\
%     \BILThree      &  \textbf{922.36} & 362.59 &  334.47  & 225.30     & 1.74 & 1.07 & 2.15\\
%     \BILFour       & 1062.68 & 155.72  & 681.03   & 225.93    & 1.63 & 0.99 & 2.15\\
%     \BILFive       & 1380.04 & 65.45  & 1088.94    & 225.64   & 1.67 & 1.02 & 2.15\\
%     \BILSix        & 1777.77 & 25.04 &  1529.77  & 222.96     & 1.61 & 0.85 & 2.13\\
%     \imre          & 2209.97 & 12.08 &1974.72 & 223.16        & 1.58 & 0.63 & 2.14 \\
%     \hline
%     % END RECEIVE ORGTBL res2
%   \end{tabular}
%         %         \egroup

%   \caption{\label{tbl:res}Evaluation results where the monetary units are in \(k\)-values.}

%         %         \end{figure}
% \end{table}


\section{Conclusion}
\label{sec:conclusion}

% This paper introduces deep theoretical insights in reinfocement learning and provides first evidence
% that average reward reinfocement learning is better applicable to operations research problems than
% the discounted framework. The paper describes an application of an order release model based on
% reinforcement learning. The performance is tested on a multi-product, two-stage hypothetical flow
% shop and is measured by cost, delivery and lead time related measures. We show that in the current
% version the machine learning approach is not able to outperform all other tested order release
% approaches. In the future we expect to be able to yield better results by investigating the
% parameterisation and resolving current issues when scaling average reinforcement learning to complex
% scenarios.

\MS[t]{todo}

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
