\documentclass[envcountsame]{llncs}


\usepackage[table]{xcolor}
\usepackage[utf8x]{inputenc}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{amsmath}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage[noend]{algpseudocode}
\usepackage{stmaryrd}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{array}
\usepackage[colon,round,sort]{natbib}
\usepackage{textcomp}
\usepackage{pifont}
% \usepackage{bbding}
\usepackage{comment}
\usepackage{graphicx}
\usepackage[svgpath=figures/]{svg}
\usepackage[labelfont=bf]{caption}
\captionsetup[table]{skip=5pt}
\usepackage{tikz}
\usetikzlibrary{patterns}
\usetikzlibrary{calc}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{shapes.geometric}
% \usepackage{tikz}
% \usetikzlibrary{patterns}
% \usetikzlibrary{calc}
% \usetikzlibrary{decorations.pathreplacing}
% \usetikzlibrary{shapes,arrows}
% \usetikzlibrary{shapes.geometric}
% \usetikzlibrary{arrows}


\usepackage{ntheorem}
\newtheorem*{definition*}{Definition}


\usepackage{lineno}


\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{relsize}
\tikzset{fontscale/.style = {font=\relsize{#1}}}
\usepackage{marvosym}
\usepackage{paper}


\newcommand\theauthor{
  Manuel Schneckenreither\inst{1}
  % \orcidID{0000-0002-4812-4665}
  \and Stefan Haeussler\inst{1}
  % \orcidID{0000-0003-2589-1367}
  \and Juanjo Peiró\inst{2}
}
\newcommand\thetitle{Average Reward Adjusted Deep Reinforcement Learning: Near-Blackwell-Optimal
  Policies applied to the Order Release Problem}


% \usepackage{academicons}
% \usepackage{cite}
\usepackage[hidelinks,
plainpages=false,
pdftitle={\thetitle{}},
pdfauthor={Manuel Schneckenreither, Stefan Haeussler},
pdfsubject={\thetitle{}},
pdfkeywords={average reward adjusted reinforcement learning, production planning, order release, machine learning,
  operations research},
]{hyperref}
\usepackage{url}
\pagestyle{plain}


\newcommand{\SH}[1]{\begin{center} \textcolor{green!50!black}{#1} \end{center}}
\newcommand\MS[2][r]{\ifx t#1 \textcolor{blue}{[\textbf{MS:} #2]}
  \else \begin{center}\textcolor{blue}{\textbf{MS:} #2} \end{center} \fi}

% \newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}

\author{\theauthor}
\date{\today}
\title{\thetitle}


% \authorrunning{Schneckenreither and Haeussler}
\institute{
  Department of Information Systems, Production and Logistics Management, University of Innsbruck, Austria\\
  \email{manuel.schneckenreither@uibk.ac.at,stefan.haeussler@uibk.ac.at}
  \and
  Departament d’Estadística i Investigació Operativa, Universitat de València, Spain\\
  \email{juanjo.peiro@uv.es}
}


\begin{document}


\maketitle

\begin{abstract}
  % teilw. von IJPR paper uebernommen
  An essential task in manufacturing planning and control is to determine when to release orders to
  the shop floor. One key parameter is the lead time which is the planned time that elapses between
  the release of an order and its completion. Lead times are usually determined based on the
  measured duration orders take to traverse the production system, i.e.\@ the flow times.
  Traditional order release models assume static lead times, although it has been shown that they
  should be set dynamically to be able to react to the dynamic operational characteristics of
  production system. Therefore, we present an order release model which sets lead times dynamically
  by using an reinforcement learning approach. This algorithm is designed especially for
  applications with high process variability and periodic feedback to the agent. Therefore, this paper aims
  at providing a proof-of-concept for the proposed algorithm. Therefore, we compare our method on a
  three-stage flow-shop simulation model not only to static order release techniques, but also to
  the commonly used deep Q-Learning algorithm. The results show that both reinforcement learning
  agents have room for improvement for queuing systems with low variability, that our proposed
  method usually outperforms deep Q-Learning and finally for systems with high utilisation that the
  proposed reinforcement learning algorithm performs very well, outperforming all other approaches.
  %
  % We use a  to compare the algorithm
  % to % and show that our approach outperforms
  % well-known order release mechanisms. We show that in the current version our proposed model using
  % reinforcement learning outperforms some, but not all other tested approaches.
  % EVTL: especially for scenarios with...
  \keywords{operations research, production planning, order release, machine learning, reinforcement
    learning}
\end{abstract}


% \MS[t]{ideas: Similar Selçuk (2013) detect that a high update frequency has a negative effect and
%   thus should be reduced. They conclude that the lead times ought to control the WIP level, instead
%   of the due date reliability as changes are directly seen (Knollmann and Windt, 2013). For a review
%   of analytical works in the context of the LTS see Bendul and Knollmann (2016).}


\section{Introduction}\label{sec:introduction}
An important goal in Manufacturing Planning and Control (MPC) systems is to achieve short and
predictable flow times, especially where high flexibility in meeting customer demand is required.
Besides achieving short flow times, one should also maintain high output and due-date performance
while keeping the work-in-process level low. One approach to address this problem is to collect all
incoming orders in an order-pool and periodically decide which orders to release to the shop floor.
Once orders are released, costs start to accumulate as planned orders materialise as actual jobs in
the production system. The main challenge is to find a good compromise of balancing the shop floor
and timely completion of jobs. Although the performance of such systems can be measured manifold the
most overarching objective is to maximise profits by adequately assigning holding and lateness
costs.

One of the key modeling parameters for order release mechanisms is the \textit{lead time}, which
refers to the planned time that elapses between the release of an order and its arrival in the
finished goods inventory. This planning parameter is often based on the observable time an order
needs to traverse the production system, which in contrast is denoted as the \textit{flow time}.
Flow times consist of processing, setup, control, transport, and waiting times, whereas the latter
is the governing factor \citep[][p.223]{zapfel1982produktionswirtschaft}. Waiting times are a result
from queuing (e.g.\@ jobs queue before and after processing), depend heavily on the amount of jobs in
the system (WIP) and thus are relatively difficult to estimate, which makes the
setting of favorable lead times so difficult (e.g., \citealt{Tatsiopoulos1983, Wiendahl1995}). Most
state-of-the-art order release mechanisms use static or fixed lead times to address the order
release problem, and thus neglect the nonlinear relationship between resource utilisation and flow
times, which is well known from practice and queuing theory (\citealt{Pahl2007}).

One way to address this nonlinear interaction effects is to set lead times dynamically. Intuitively
the order release problem is solved by perfectly matching the lead times to the flow times, but the
corresponding optimisation problem faces ``sampling issues'' meaning that the flow times depend on
the lead times. An extreme scenario of this problem is the so called ``lead time syndrome'', which
describes a vicious cycle where increasing flow times perpetually inflate the lead times which leads
to worse performance (see e.g., \citealt{Mather1978, knollmann2013control, Selcuk2006}.
%
Thus, setting lead times dynamically harbors optimisation potential \citep{hoyt1978dynamic}, but may
also substantially degrade the system performance.
%
\citet{schneckenreither2020order} have established
following categorisation of dynamic lead time management approaches:

\begin{itemize}
\item \textsf{Reactive lead time management} approaches set lead times by reacting on
  earlier flow times (e.g., \citealt{enns2004work, Selcuk2006}). Note that the forecast is always
  based on \textit{past} data as the most recent system changes cannot be reflected by flow times
  until the corresponding orders arrive in the FGI, which might take several
  periods.
\item
  \textsf{Proactive lead time management} may incorporate \textit{past} data in conjunction with
  the \textit{current} system state to set lead times (e.g., \citealt{Bertrand1983,
    ChungHuang2002}). Put differently, these methods aim to find a function that provides lead times
  based on the current state of the production system and possibly information from the past.

\item \textsf{Predictive lead time management} may not only incorporate \textit{past} data and the
  \textit{current} system state to set lead times, but also utilises the anticipated \textit{future}
  system state to detect arising issues of future periods and react accordingly (e.g.
  \citealt{PaterninaArboleda2001,schneckenreither2020order,Schneckenreither2019}). Thus, it extends
  proactive lead time management from a flow time forecasting or simple lead time setting technique
  to a lead time management approach that integrates the future behaviour of the system when setting
  lead times. This allows reasoning of the system dynamics, as for instance triggering the lead time
  system, and thus such an algorithm can react accordingly to find a more farsighted optimal lead
  time update.
\end{itemize}

We hypothesis that dynamic lead time management approaches need to aim for a predictive design in
order to be able to compete with state-of-the-art order release methods from literature. However,
there only exist three papers that propose a predictive lead time management algorithm in
literature.

The first approach by \citet{PaterninaArboleda2001} introduces a predictive order release model by
using reinforcement learning in a single product, four-station serial flow line and compare its
performance (WIP costs) with conventional order release policies (e.g., Kanban and CONWIP).
%
% Reinforcement learning is an optimisation technique that stems from dynamic programming and its goal
% is to find the best stationary policy for a given problem. This policy is usually provided by
% assessing current and future states (or state-action pairs) of an underlying Markov Decision Process
% (MDP).
% The advantage of reinforcement learning over dynamic programming is that (i) the problem
% space is explored by an agent and thus only expectantly interesting parts of the problem space need
% to be assessed and (ii) the knowledge (acquisition) of transition probabilities becomes unnecessary
% as the states are evaluated by consecutively observed states solely.
%
The algorithm of \citet{PaterninaArboleda2001} decides on whether or not to release an order after
each system change, the completion of an operation of any order at any stage or a new order arrival,
and assumes that any unsatisfied demand is lost. Thus, they use a continuous order release method
although in practice order release decisions often need to be made on a periodical basis, e.g.\@
daily (see \citealt{enns2004work,GeldersvanW1982}). They outperform existing control policies with their tabular
based reinforcement learning agent.
%
Then, \citet{Schneckenreither2019} use several different reinforcement learning algorithms to make
periodic order release decisions for a flow shop production system. The algorithm directly sets lead
times for each product type, which are then used to release the orders in the order pool. They show
that their approach outperforms static order release mechanisms by yielding lower costs, lateness
and standard deviation of lateness, but conclude that research using average reward reinforcement
learning methods harbor optimisation potential for the order release problem.
%
And finally, \citet{schneckenreither2020order} present a flow time estimation procedure to set lead
times dynamically using an artificial neural network, which is used to forecast flow times. By
implementing a rolling horizon order release simulation of the proceeding periods they lift their
approach to a predictive lead time management approach, which is able to detect backorders of future
periods and reacts by releasing orders earlier. Nonetheless, their method is unable to foresee the
triggering of the lead time syndrome and therefore they introduce an upper lead time bound to
prevent the negative effects of the lead time syndrome. Their model outperforms static and reactive
lead time management approaches, especially for scenarios with high utilisation.


As reinforcement learning stems from dynamic programming the future system state is by design
considered as a main driver of decision making in the current period.
% Reinforcement learning is an optimisation technique that stems from dynamic programming and
Its goal is to find the best stationary policy for a given problem. % This policy is usually provided
% by assessing current and future states (or state-action pairs) of an underlying Markov Decision
% Process (MDP).
The advantage of reinforcement learning over dynamic programming is that (i) the problem
space is explored by an agent and thus only expectantly interesting parts of the problem space need
to be assessed and (ii) the knowledge (acquisition) of transition probabilities becomes unnecessary
as the states are evaluated by consecutively observed states solely.

Over the past decades reinforcement learning has been applied to various problems, for which
astonishing results have been reported. E.g.\@ only recently \citet{mnih2015human} have presented a
novel value-iteration reinforcement learning agent which exceeds human-level abilities in playing
many classic Atari 2600 games~\citep{bellemare2012investigating}. Further,
\citet{mnih2016asynchronous} present improved results with asynchronous actor-critic reinforcement
learning. Also games like Go~\citep{silver2016mastering} and Chess~\citep{silver2017mastering} have
been mastered with superhuman performance by \textit{tabula rasa} reinforcement learning agents.
Furthermore, the method has also been applied in the setting of manufacturing system, e.g.\@ to
improve the ramp-up process~\citep{doltsinis2012reinforcement}, in locally selecting appropriate
dispatching rules \citep{zhang1995reinforcement,wang2005application} or scheduling
\citep{zhang1995reinforcement, waschneck2018optimization}.
%
However, all these applications use discounted reinforcement learning and are either designed to
investigate a rather simple MDP, e.g.\@ by selecting heuristics instead of optimising the underlying
problem itself, or by mapping the actual objective in a reward function that is approximately \(0\)
on average over time.
%
This is due to the fact that state value is largely composed of a term defined by the policys'
average reward value \citep{MillerVeinott1969,Blackwell62} which would otherwise dilute the state
values and thus decrease the solution quality, as can for instance be observed in
\citet{SchneckenreitherHaeussler2019} and \citet{gijsbrechts2018can}.
%


Therefore, most applications that incorporate and directly reflect costs or profit in the reward
function use average reward reinforcement learning. \citet{aydin2000dynamic} use it in the setting
of scheduling, while in a series of papers Mahadevan et al.\@ investigated several problem domains
starting with simple MDPs
\citep{Mahadevan96_AverageRewardReinforcementLearningFoundationsAlgorithmsAndEmpiricalResults,Mahadevan96_OptimalityCriteriaInReinforcementLearning}.
After these foundational works for average reward reinforcement learning they introduced a
continuous time average reward reinforcement learning algorithm named SMART
\citep{Mahadevan97_SelfimprovingFactorySimulationUsingContinuoustimeAveragerewardReinforcementLearning}.
Applications of SMART reach from the optimisation of queuing systems
\citep{Mahadevan96_AnAveragerewardReinforcementLearningAlgorithmForComputingBiasoptimalPolicies,Mahadevan96_SensitiveDiscountOptimalityUnifyingDiscountedAndAverageRewardReinforcementLearning},
maintenance of an inventory system
\citep{Das99_SolvingSemiMarkovDecisionProblemsUsingAverageRewardReinforcementLearning} to optimising
transfer line in terms of maximizing demand, while keeping inventory levels of unfinished product as
low as possible \citep{Mahadevan98_OptimizingProductionManufacturingUsingReinforcementLearning}.
However, in practise usually decision have to be made on a daily basis
\citep{enns2004work,GeldersvanW1982}. Therefore, we refrain from this adaption and concentrate on
standard MDPs only. Furthermore, often continuous-time semi-MDP problems can be converted through
uniformisation into equivalent discrete time instances
\citep[see][]{Puterman94,bertsekas1995dynamic}.


Like discounted reinforcement learning also average reward reinforcement learning is based on an
oracle function, in our case the accumulated costs of a period, to assess the decisions taken by the
agent. By repeatedly choosing different actions the agent examines the problem space and rates
possible actions for any observed state. The advantage of average reward reinforcement learning over
the widely applied discounted reinforcement learning framework is that the underlying optimisation
technique is able to find better policies. This yields from the fact that in standard discounted
reinforcement learning method the states are assessed independently and by a single value. To be
more precise, average reward reinforcement learning splits up the evaluation of the average reward
per step, a bias value that specifies the amount of collected rewards to reach the optimal path when
starting in a suboptimal state and an error term which defines the number of steps to reach the
optimal path \citep{Howard64,Puterman94,
  Mahadevan96_AverageRewardReinforcementLearningFoundationsAlgorithmsAndEmpiricalResults}. % Thus,
% while in average reward reinforcement learning these terms are learned separately
In contrast to that in the standard discounted framework one single scalar value consisting of the
addition of these subterms is estimated, where however the average reward is scaled by
\(1/(1-\gamma)\). The discount factor \(\gamma\) is usually set very close to \(1\), e.g. \(0.99\),
which leads to the fact that this subterm dominates the other two.
%
Further, in commonly applied unichain (and thus ergodic) MDPs the average reward per step is equal
for all states. Thus, independently assessing it is not only computationally unwisely, but also
problematic when iteratively annealed as done in reinforcement learning. Therefore, we adapt an
algorithm that uses a scalar value for the estimation of the average reward over all states, and
estimates for every state-action pair that incorporate the bias and error term. The later values are
adjusted by the average reward.
% This, combined with the iterative evaluation and the independently assessing of the average reward
% for each state lets standard discounted reinforcement learning struggle to find good policies.
% Thus, this incautious combining of different kinds of state values as done in discounted
% reinforcement learning leads to the problems that (i) the average reward, which is equal for all
% states of usually investigated unichain MDPs, is dominating and thus diluting the bias values, and
% (ii) the state values are deteriorated by the error term that is only imposed due to the
% discounting technique.

Thus, as opposed to the commonly applied discounted reinforcement learning algorithm we use an
average reward adjusted reinforcement learning algorithm to adaptively release orders based on the
assessed state values of the production system. In contrast to the aforementioned works on average
reward reinforcement learning our algorithm incorporates the optimisation of not only the average
reward over time, but also the bias values, which is an important adaption in highly stochastic
systems. Only the work by
\citet{Mahadevan96_AnAveragerewardReinforcementLearningAlgorithmForComputingBiasoptimalPolicies}
integrates this second-level refinement optimisation. However, their algorithm requires the
selection of a reference state to prevent an infinite increase of state values. This results from
the lack of feedback in the iterative process of optimising the different refinement optimisation
levels. Furthermore, they asses the average reward independently for each state.

In summary, we propose a novel average reward adjusted reinforcement learning algorithm to assess
orders for their release to the shop floor. This is the first average reward adjusted reward
learning algorithm that operates using a artificial neural network as function approximation. To
ensure scalability we adapt a multi-agent approach, where each agent optimises the lead time
management of a single product type. The agents do so by assessing the expected costs for the
possible releases imposed by adapting the lead time. According to the learned estimates each agent
sets a planned lead time for the corresponding product type and with that releases orders into the
production system. The highly optimised value-iteration algorithm uses improved n-step reinforcement
learning with small action-based replay memories and worker agents to infer order release policies
which, especially in case of high utilisation, outperform not only the static order release
mechanisms, but also the commonly applied deep Q-Learning algorithm~\citep[cf.][]{mnih2015human}.

%
% \MS[t]{TODO} We provide ample evidence of the viability of the approach and compare it to well-known
% order release mechanisms. Furthermore, we show that the advantage of average reward reinforcement
% learning for the problem structures encountered in the context of production and logistics by
% comparing the results to the discounted reinforcement learning framework. The preliminary results
% show that our approach performs better than the other methods. With regard to practical implications
% we are confident that decision support tools based on average reward reinforcement learning increase
% the decision quality of human planners.

\paragraph{Structure.} The rest of the paper is structured as follows. The next section introduces
average reward reinforcement learning and presents the used algorithm. For a more comprehensive
discussion of the theoretical foundations we refer to \citet{schneckenreither2020average}.
Section~\ref{sec:Experimental_Evaluation} describes the simulation model we use to evaluate the
approach. The results of the computation experiments are presented in
Section~\ref{sec:Computational}, and Section~\ref{sec:conclusion} concludes the paper.

% \MS[t]{todo}
% Then~\ref{sec:Preliminary} presents preliminary
% result data, whereas Section~\ref{sec:conclusion} concludes this working paper.


\section{Average Reward Adjusted Reinforcement Learning}

This section briefly reintroduces the most important concepts of average reward adjusted
reinforcement learning, elaborates on optimality criteria and provides insights of the underlying
algorithm. For a more extensive introduction to average reward reinforcement learning we refer to
\citet{schneckenreither2020average}.

Like \cite{MillerVeinott1969} we are considering problems that are observed in a sequence of points
in time labeled \(1,2,\ldots\) and can be modelled using a finite set of states \(\States\),
labelled \(1,2,\ldots,\size{\States}\), where the size \(\size{\States}\) is the number of elements
in \(\States\). At each point $t$ in time the system is in a state \(s_{t} \in \States\). Further,
by choosing an action $a_{t}$ of a finite set of possible actions \(A_{s}\) the system returns a
reward $r_{t} = r(s_{t}, a_{t})$ and transitions to another state \(s_{t+1} \in \States\) at time
\(t+1\) with conditional probability \(p(s_{t+1}, r_{t} \mid s_{t}, a_{t})\). That is we assume that
reaching state \(s_{t+1}\) from state \(s_{t}\) with reward \(r_{t}\) depends solely on the previous
state \(s_{t}\) and chosen action \(a_{t}\). In other words, we expect the system to possess the
Markov property \citep[p.63]{sutton1998introduction}. Reinforcement learning processes that possess
the Markov property are referred to as Markov decision processes (MDPs)
\citep[p.66]{sutton1998introduction}.

Thus, the action space is defined as \(F = \times_{s=1}^{\size{\States}} A_{s}\), where \(A_{s}\) is
a finite set of possible actions. A \emph{policy} is a sequence \(\pol = (f_{1},f_{2},\ldots)\) of
elements \(f_{t} \in F\). Using the policy \(\pol\) means that if the system is in state \(s\) at
time \(t\) the action \(f_{t}(s)\), i.e.% that is
the \(s\)-th component of \(f_{t}\), is chosen. A stationary policy \(\pol = (f,f,\ldots)\) does not
depend on time. In the sequel we are concerned with stationary policies only.


\subsection{Discounted Reinforcement Learning}
\label{subsec:Discounted_Reinforcement_Learning}


In the widely applied discounted framework the value of a state \(V_{\gamma}^{\pol_{\gamma}}(s)\) is
defined as the expected discounted sum of rewards under the stationary policy \(\pol_{\gamma}\) when
starting in state \(s\). Note that the policy \(\pol_{\gamma}\) depends on the selected discount
factor. That is
\begin{align*}
  V_{\gamma}^{\pol_{\gamma}}(s)=\lim_{N \to \infty} \E[\sum_{t=0}^{N-1} \gamma^{t} R_{t}^{\pol_{\gamma}}(s)]\tcom
\end{align*}

where \(0 \leqslant \gamma < 1\) is the discount factor and
\(R_{t}^{\pol}(s) = \E_{\pol}[ r(s_{t},a_{t}) \mid s_{t} = s, a_{t} = a]\) the reward received at
time \(t\) upon starting in state \(s\) by following policy \(\pol\)
\citep{Mahadevan96_AverageRewardReinforcementLearningFoundationsAlgorithmsAndEmpiricalResults}.
%
The aim in the discounted framework is to find an optimal policy \(\polopt_{\gamma}\), which when
followed, maximises the state value for all states \(s\) as compared to any other policy
\(\pol_{\gamma}\): \(V_{\gamma}^{\polopt_{\gamma}} - V_{\gamma}^{\pol_{\gamma}} \geqslant 0\). This
criteria is usually referred to as \(\gamma\)-optimality as the discount factor \(\gamma\) is fixed.
However, this also means that the actual value set for \(\gamma\) determines the best achievable
policy. For instance, as can be seen in Figure~\ref{fig:printer} setting \(\gamma <0.8027\) the
printer-loop is preferred over the mail-loop, although the mail-loop accumulates more
reward over time, i.e.\@ selecting the mail-loop is a sub-optimal choice. Observe that the average
reward received per step equals \(1\) for the printer-loop and \(2\) for the mail-loop. Thus, the
(Blackwell-)optimal policy is to choose the mail-loop. However, if
\(\gamma < 3^{-\frac{1}{5}} \approx 0.8027\) an agent using discounted reinforcement learning
chooses the printer loop as going the printer loop has a higher evaluation.
%
\begin{figure}[t!]
  \centering
  \input{figures/printer}
  \caption{\label{fig:printer} A MDP with a single action choice in
    state \(1\), i.e.\@ two different deterministic policies
    % , in which the agent can choose between doing the printer-loop or the mail-loop  . The
    % Observe that the average reward received per step equals \(1\) for the printer-loop and \(2\)
    % for the mail-loop. Thus, the (Blackwell-)optimal policy is to choose the mail-loop. However,
    % if \(\gamma < 3^{-\frac{1}{5}} \approx 0.8027\) an agent using discounted reinforcement
    % learning chooses the printer loop.
    % %
    \citep[][adapted]{Mahadevan96_OptimalityCriteriaInReinforcementLearning}.
  }
\end{figure}
%

Therefore, in the seek of a more general optimality criteria for determining the best stationary
policy \(\pol\) for a given problem, we introduce \(n\)-discount-optimality.


% \subsection{Optimality Criteria}
% \label{subsec:More_Refined_Optimality_Criteria}

% The aforementioned and informally introduced notions of gain- and bias-optimality are formalised and
% generalised to \(n\)-discount-optimality as follows.

\begin{definition}[\(n\)-Discount-Optimality]
  Due to \citet{Veinott69} for a MDP a policy \(\polopt\) is \emph{\(n\)-discount-optimal} for
  \(n=-1,0,1,\ldots\) for all states \(s \in \States\) with discount factor
  \(\gamma\) % and \(V_{\gamma}^{\pol}(s)\) being the value function as defined above
  %
  if and only if
  \begin{align*}
    \lim_{\gamma \to 1}(1-\gamma)^{-n}\ (V_{\gamma}^{\polopt}(s) - V_{\gamma}^{\pol}(s)) \geqslant 0 \tpkt
  \end{align*}
\end{definition}


Only if a policy is \(n\)-discount-optimal for all \(n < m\) it can be \(m\)-discount
optimal \citep{Puterman94,Veinott69}.
%
If a policy is \(-1\)-discount-optimal it is called gain-optimal, if it is \(0\)-discount-optimal it
is also called bias-optimal.
%
Furthermore, if a policy is \(\infty\)-discount-optimal then it is said to be Blackwell-optimal
\citep{Blackwell62}.
%
That is, for Blackwell-optimal policies \(\polopt\) there exists a discount factor \(\gammaopt < 1\)
such that \(V_{\gamma}^{\polopt}(s) \geqslant V_{\gamma}^{\pol}(s)\) for all
\(\gamma \geqslant \gammaopt\) and under all policies \(\pol\)
\citep{Mahadevan96_SensitiveDiscountOptimalityUnifyingDiscountedAndAverageRewardReinforcementLearning,Blackwell62}.
Informally that means there exists a discount factor \(\gamma <1\) which finds Blackwell-optimal
policies. However, (i) in more complex, i.e.\@ real world, MDPs this value can be arbitrary close to
\(1\) and (ii) the difference in state values may be very small, which (iii) due to the need of
state value function approximation likely causes errors when choosing among different actions in the
discounted framework. \citet{schneckenreither2020average} establishes a practically relevant
definition for near-Blackwell-optimality.

\begin{definition}
  If an algorithm infers for any MDP bias-optimal policies, and for a given MDP can in theory be
  configured to infer Blackwell-optimal policies, but in practise this ability is naturally limited
  due to the finite accuracy of floating-point representation of modern computer systems, it is said
  to be \emph{near-Blackwell-optimal}. An according to a near-Blackwell-optimal algorithm inferred
  Blackwell-optimal policy is called near-Blackwell-optimal.
\end{definition}

Note that standard discounted RL does not meet the requirements of near-Blackwell-optimality, as it
generally does not infer bias-optimal policies. Only by chance the resulting policy could be
bias-optimal.
%
% In the sequel we present the Laurent series expansion which not only links discounted reinforcement
% learning with average reward reinforcement learning, but also connects to \(n\)-discount-optimality.
%
% Furthermore, for the rest of the paper we occasionally write drop the \(V(s)\) and $V_{\gamma}$ instead of
% \(V^{\pol}(s)\) and \(V^{\pol}_{\gamma}\) when the policy is clear from the context.
%
The corresponding Bellman equation defined as
\(V_{\gamma}^{\pol}(s) = \E_{\pol}[r + \gamma V_{\gamma}^{\pol}(s')]\) \citep[see
e.g.][p.70]{sutton1998introduction} provides a way to assess the state values using two
consecutively observed states \(s, s'\) and the observed reward \(r\) returned by the system. When
this formula is used as an update rule it provides an algorithm that converges the state values by
iteratively adapting the state value estimates.
%
There are four major issues with discounted RL, which results in the fact that it is inapplicable
for many problems of operations research.


\begin{enumerate}
\item In general standard discounted RL can only infer suboptimal policies as the discount factor is
  strictly less than one, i.e.\@ \(\gamma < 1\).
\item It is very difficult to specify a desired balance between the short-term and long-term
  (average) rewards. This results from the fact that the long-term rewards are scaled exponentially,
  where the factor depends on the chosen \(\gamma\) value.
\item Episodic MDPs with an average reward per step that is non-zero cannot be solved correctly, as
  the average reward is ignored in the terminal states.
\item The average reward is independently assessed for each state, even though it is the dominating
  part for all state values and thus is shared between more than one or, for unichain MDPs, even all
  states. This usually leads to an exponentially increased number of learning steps required for
  policies to converge.
\end{enumerate}


% One idea behind the \(\gamma\)-parameter of the discounted framework is to be able to balance
% short-term rewards (low \(\gamma\)-values) and long-term rewards (high \(\gamma\)-values). However,
% what seems to be an advantage rather becomes a disadvantage for most applications. The issue is that
% the average reward value is non-linearly increased when \(\gamma\) approaches one. However, in
% almost all cases the aim is to perform well over time which in terms of reward means to first
% maximise for a policy with highest average reward before more selectively choosing actions. That is,
% we are searching for the policies with highest average reward before considering other criteria.
% %
% Therefore, in almost all RL studies the discount factor is set to a value very close to \(1\), for
% instance to \(0.99\)~\citep[e.g.][]{mnih2015human,mnih2016asynchronous,Lillicrap15}.
% %
% By doing so the above mentioned non-linear relationship leads to the fact that the state value
% consists almost solely of the up-scaled average reward term, whereas the bias values can be
% neglected. This leads to diluted state values and thus difficulties in distinguishing actions which
% impose policies that possess the same average reward.
% %
% In contrast to that, average reward RL separately assesses these values and according to these
% selectively chooses the best action, comparing one after the other.
% %
% To illustrate the idea reconsider Figure~\ref{fig:printer}. First the agent picks the policy
% according to the highest average reward. Thus, all actions are assessed by an average reward of
% \(2\) which is inferred from the Mail-loop. This however, makes choosing the Printer-loop
% unattractive; i.e. the policy converges to the optimal policy of choosing the Mail loop.

Especially due to the fourth issue standard discounted RL is not well applicable in the operations
research domain. This mainly results from the fact that a continuous assessing of the state values
using costs or profit is often required, which leads to intractable long learning phases with high
exploration rate even for small sized problems. But as high exploration rates produce errors in the
state value estimations \citep{MillerVeinott1969} finding a well working parameterisation is
difficult. Average reward reinforcement learning overcomes these issues by separately assessing the
subterms of the state values directly.


\subsection{Average Reward Reinforcement Learning}

Due to \cite{Howard64} the average reward \(\avgrew^{\pol}(s)\) of a policy \(\pol\) and a starting
state \(s\) is defined as
\begin{align*}
  \avgrew^{\pol}(s) = \lim_{N \to \infty} \frac{\E [\sum_{t=0}^{N-1}R_{t}^{\pol}(s)]}{N}\tpkt
\end{align*}


In the common case of unichain MDPs, in which only a single set of recurrent states exists, the
average reward \(\avgrew^{\pol}(s)\) is equal for all states \(s\)
\citep{Mahadevan96_AverageRewardReinforcementLearningFoundationsAlgorithmsAndEmpiricalResults,Puterman94}.
In the sequel we focus on unichain MDPs in this work and thus may simply refer to it as
\(\avgrew^{\pol}\). A policy \(\polopt\) that maximises the average reward
\(\avgrew^{\polopt}(s) - \avgrew^{\pol}(s) \geqslant 0\) in every state \(s\) as compared to any
other policy \(\pol\) is called gain-optimal.
%
% A policy that maximises the average reward in every state is called
% gain-optimal
% \citep{Mahadevan96_AnAveragerewardReinforcementLearningAlgorithmForComputingBiasoptimalPolicies}.
% Gain-optimality is the least selective criteria an average reward reinforcement
% learning aims for.
\citet{Mahadevan96_AnAveragerewardReinforcementLearningAlgorithmForComputingBiasoptimalPolicies}
shows that \(n\)-discount-optimality for \(n=-1\) describes gain-optimality.

Further, for an unichain aperiodic\footnote{In the periodic case the Cesaro limit of degree \(1\)
  is required to ensure stationary state transition probabilities and thus stationary bias
  values \citep{Puterman94}. Therefore to ease readability we concentrate on unichain
  and aperiodic MDPs.} MDP problem, such as ergodic MDPs are, the average adjusted sum of rewards or
bias value is defined as
\begin{align*}
  V^{\pol}(s) = \lim_{N \to \infty}{ \E [ \sum_{t=0}^{N-1}(R_{t}^{\pol}(s) - \avgrew^{\pol} )]}\tcom
\end{align*}


where again \(R_{t}^{\pol}(s)\) is the reward received at time \(t\), starting in state \(s\) and
following policy \(\pol\). Note that the bias values are bounded due to the subtraction of the
average reward. Thus the bias value can be seen as the rewards that additionally sum up in case the
process starts in state \(s\). A policy \(\polopt\) that is gain-optimal is also bias-optimal, if it
maximises the bias values \(V^{\polopt}(s) - V^{\pol}(s) \geqslant 0\) in every state \(s\) and
compared to every other policy \(\pol\). Bias-optimality is given by setting \(n=0\) in the
definition of \(n\)-discount-optimality
\citep{Mahadevan96_AnAveragerewardReinforcementLearningAlgorithmForComputingBiasoptimalPolicies}.


Especially for highly probabilistic systems bias-optimality is important. To clarify this consider
Figure~\ref{fig:three-states} which again consists of two possible deterministic policies with the
only choice in state \(1\). Both policies have the same average reward of \(1\). However, only
taking the A loop is bias-optimal, as its policy \(\pol_{A}\) leads to bias values
\(V^{\pol_{A}}(1)=0.5\), \(V^{\pol_{A}}(0)=-0.5\) and \(V^{\pol_{A}}(2)=1.5\), while policy
\(\pol_{B}\) which selects the B loop generates bias values \(V^{\pol_{B}}(1)=-0.5\),
\(V^{\pol_{B}}(0)=-1.5\) and \(V^{\pol_{B}}(2)=0.5\).
%
This yields from the fact that the actions with non-zero rewards are selected earlier in policy
\(\pol_{A}\). Consider starting in state 1. Under the policy \(\pol_{A}\) the reward sequence is
\((2,0,2,0,\ldots)\), while for the other policy \(\pol_{B}\) it is \((0,2,0,2,\ldots)\).


\begin{figure}[t!]
  \centering
  \begin{tikzpicture}[thin, scale=0.75]
    % Nodes
    \draw (-3,0) node(0) [circle,draw,minimum size=25] {\footnotesize \(0\)};
    \draw (0,0)  node(1) [circle,draw,minimum size=25] {\footnotesize \(1\)};
    \draw (3,0)  node(2) [circle,draw,minimum size=25] {\footnotesize \(2\)};

    % Edges
    \path[thin, ->, bend right, >=stealth] (1) edge[above] node {\footnotesize\(2\)} (0);
    \path[thin, ->, bend right, >=stealth] (2) edge[above] node {\footnotesize\(2\)} (1);
    \path[thin, ->, bend right, >=stealth] (0) edge[below] node {\footnotesize\(0\)} (1);
    \path[thin, ->, bend right, >=stealth] (1) edge[below] node {\footnotesize\(0\)} (2);

    \draw (-1.5,0) node[] { A };
    \draw (1.5,0) node[] { B };


  \end{tikzpicture}
  \caption{\label{fig:three-states} A MDP with two gain-optimal deterministic policies, \(\pol_{A}\)
    going left in 1 is also bias-optimal, while \(\pol_{B}\) is not \citep[Adapted
    from][]{Mahadevan96_AverageRewardReinforcementLearningFoundationsAlgorithmsAndEmpiricalResults}.}
\end{figure}


\subsection{The Laurent Series Expansion of Discounted State Values}
\label{subsec:The_Laurent_Series_Expansion_of_Discounted_State_Values}


\cite{MillerVeinott1969} established the link between discounted RL state values
\(V_{\gamma}^{\pol}(s)\) and average reward RL values \(\avgrew^{\pol}(s)\) and \(V^{\pol}(s)\)
using the Laurent series expansion as
\begin{align*}
  V_{\gamma}^{\pol}(s) = \frac{\avgrew^{\pol}(s)}{1-\gamma} + V^{\pol}(s) + e_{\gamma}^{\pol}(s) \tcom
\end{align*}
%
where the error term \(e_{\gamma}^{\pol}(s)\) exists of infinitely many subterms and
\citet{Puterman94} shows that \(\lim_{\gamma \to 1} e_{\gamma}^{\pol}(s) = 0\). Note how the first
term depending on the average reward \(\avgrew^{\pol}(s)\) converges to infinity as \(\gamma\)
increases.


However, an important insight is the connection between \(n\)-discount-optimality and the Laurent
series expansion of \(V_{\gamma}^{\pol}(s)\). Each addend corresponds to one step in
\(n\)-discount-optimality. That is for \((-1)\)-discount-optimality the addend describes (a scaled
version of) the term to maximise for gain-optimality,
% the average reward must be
% maximised.
%
for \(0\)-discount-optimality the term to maximise for bias-optimality, and
%
finally \(n\)-discount-optimality for \(n \geqslant 1\) requires to maximise the error term
\(e_{\gamma}^{\pol}(s)\). The later only exists as \(\gamma\) is strictly less than \(1\). Thus this
term incorporates the number of expected steps and their corresponding reward on the path to the
Blackwell-optimal policy. Put differently, it optimises the expected reward collected to reach the
Blackwell optimal policy according to the occurrence on the paths, where shorter paths and those
which collect the rewards sooner are preferred.

% Note how the Laurent series expansion dissipates these values in single terms and therefore defines
% a divide and conquer methodological approach to reinforcement learning. Thus in average reward
% reinforcement learning these terms are learned separately, while in the discounted framework a
% single scalar value for each state with a fixed \(\gamma\)-value is estimated.

These addends, where \(n=-1,0,\ldots\) denote the coefficients of the Laurent series expansion and
thus correspond to the \(n\) of the definition of \(n\)-discount-optimality, can be reformulated to
following constraint problem \citep[p.346]{MillerVeinott1969,Puterman94}.
%
\begin{align}
  \label{eq:constr1}\avgrew^{\pol}(s) - \E[\avgrew^{\pol}(s)] & = 0 &  & \text{for } n = -1 \\
  \label{eq:constr2}\avgrew^{\pol}(s) + V^{\pol}(s) - \E[V^{\pol}(s)] & = R^{\pol}(s) && \text{for } n = 0 \\
  \label{eq:constr3}W^{\pol}_{n-1}(s) + W^{\pol}_{n}(s) - \E[W^{\pol}_{n}(s)] & = 0 && \text{for } n \geqslant 1, \text{where } W_{0}^{\pol}(s) = V^{\pol}(s)
\end{align}
%
The error term \(e_{\gamma}^{\pol}(s)\) is given by
\(e_{\gamma}^{\pol}(s) = \sum_{n=1}^{\infty}W^{\pol}_{n}(s)\) and the expected reward
\(R^{\pol}(s) = \E_{\pol}[ r(s, a) ]\). % = \sum_{a \in \mathcal{A}(s)}p(a | s) r(s, a)\)
%
\citet[p.343ff]{Puterman94} shows that due to the given degree of freedom if \(n=-1,0,\ldots,M\)
constraints are satisfying the above conditions for all states \(s\), then only
\(\avgrew^{\pol}(s), V^{\pol}(s), W^{\pol}_{1}, \ldots, W^{\pol}_{M-1}\) are unique, whereas
\(W^{\pol}_{M}\) is offset by the vector \(u\) where for the transition probability matrix \(P\) the
vector \(u\) is characterised by \((I-P)u = 0\). Note that \(u\) is determined by the number of
closed irreducible classes of \(P\), that is for ergodic MDPs \(u\) is determined by a single
constant. Average reward learning is based on the above formulation.


A major problem occurring at average reward RL is that the bias values are not uniquely defined
without solving the first set of constraints defined by the error term addends
\citep[see][p.346]{Puterman94,Mahadevan96_SensitiveDiscountOptimalityUnifyingDiscountedAndAverageRewardReinforcementLearning}.
% We could overcome this issue by simply requiring \(\gamma\) to be strictly less than \(1\).
%
Our average reward adjusted learning algorithm (\ARA{}), based on the tabular version of
\citet{schneckenreither2020average}, does not require the exact solution for \(V^{\pol}(s)\), but a
solution which is offset suffices. Clearly this observation reduces the required iteration steps
tremendously as finding the exact solution, especially for large discount factors, is tedious.
Therefore, we allow to set \(\gamma = 1\), which induces
\(\X_{\gamma}^{\pol}(s) = V^{\pol}(s) + u\), where \(u\) is for unichain MDPs a scalar value
independent of \(s\), i.e.\@ equivalent for all states of the MDP~\cite[p.346]{Puterman94}.
%
%% Nonetheless, we require \(\gamma < 1\) to stabilise the process.
%
If we are interested in correct bias values, i.e. \(\gamma\) is sufficiently close but strictly less
than \(1\), our approach is a tremendous advantage over average reward RL as it reduces the number
of iterative learning steps by requiring only a single constraint per state plus one for the scalar
average reward value. That is, for an MDP with \(N\) states only one more constraint (\(N+1)\) has
to be solved in \ARA{} as compared to (at least) \(2N+1\) nested constraints for average reward RL.
Therefore, it is cheap to compute \(\X_{\gamma}^{\pol}(s)\), while it is rather expensive to find
the correct values of \(V^{\pol}(s)\) directly, especially in an iterative manner as RL is.

\subsection{Average Reward Adjusted Reinforcement Learning}
\label{subsec:Average_Reward_Adjusted_Reinforcement_Learning}

Thus, average reward adjusted RL can be seen as a specialised version of average reward RL, where it
targets unichain MDPs only. In the sequel we briefly present the needed formalism of average reward
adjusted RL. For more comprehensive version we refer to \citet{schneckenreither2020average}.

\begin{definition}
  The \textit{average reward adjusted (discounted) state value} $\X_{\gamma}^{\pol}(s)$ of a state
  $s$ under policy $\pol$ and with discount factor $0 \leqslant \gamma \leqslant 1$ is given as
  \(\X_{\gamma}^{\pol}(s) \defsym V^{\pol}(s) + e_{\gamma}^{\pol}(s)\).
\end{definition}
%
With the Laurent Series expansion this reformulates to
\(\X_{\gamma}^{\pol}(s) = V_{\gamma}^{\pol}(s) - \frac{\avgrew^{\pol}}{1-\gamma}\). Therefore,
\(X_{\gamma}^{\pol}(s)\) describes the discounted state value adjusted by the average reward term.
The Bellman Equation is given by
\(\X_{\gamma}^{\pol}(s) = \E_{\pol}[r_{t} + \gamma \X_{\gamma}^{\pol}(s_{t+1}) - \avgrew^{\pol} \mid
s_{t} = s]\). The corresponding Bellman optimality equation then is given by
\(\X_{\gamma}^{\polopt}(s) = \max_{a} \E_{\polopt}[r_{t} + \gamma \X_{\gamma}^{\polopt}(s_{t+1}) -
\avgrew^{\pol} \mid s_{t} = s]\). By turning this formula into an update rule it builds the
foundation of our algorithm.

\begin{algorithm}[t!]
  \begin{algorithmic}[1]
    \State{}Initialise state \(s_{0}\) and network parameters \(\netTarget, \netWorker\) randomly, set
    an exploration rate \(0 \leqslant \plearn < \pexp \leqslant 1\), exponential smoothing learning rates
    \(0 < \alpha, \gamma < 1\), and discount factors
    \(0 < \gamma_{0} < \gamma_{1} \leqslant 1\), where \(\gamma_{1} = 1\) is
    usually a good choice.
    \While{the stopping criterion is not fulfilled}
    \State{}\begin{minipage}[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}
      With probability \(\pexp\) choose a random action and
      probability \(1-\pexp\) one that fulfills
      \(\max_{a}\lexeq(\X^{\pol}_{\gamma_{1}}(s_{t},a; \netWorker{}),\X^\pol_ {\gamma_{0}}(s_{t},a; \netWorker{}))\). Let
      \(\isRandAct{t}\) indicate if the action was chosen randomly.
    \end{minipage}
    \State{}\begin{minipage}[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}
    Carry out action \(a_{t}\), observe reward \(r_{t}\) and resulting state \(s_{t+1}\).
    Store the experience \((s_{t}, a_{t}, \isRandAct{t}, r_{t}, s_{t+1})\) in the experience replay
    memory \(M\).
    \end{minipage}
    \State{Sample random mini-batch of \(m\) experiences from \(M\)}
    \State{Reset gradients: \(d\netWorker{} \gets 0\)}
    \For{each experience \(i\) with \((s_{t}, \isRandAct{t}, a_{t}, r_{t}, s_{t+1})\)}
    \If{a non-random action was chosen or \(\pexp > \plearn\)}
    \State{\(
      \avgrew^{\pol}  \gets (1- \alpha) \avgrew^{\pol} + \alpha [r_{t} +
      \max_{a}\X^\pol_{\gamma_1}(s_{t+1},a; \netWorker{}) - \X^\pol_{\gamma_1}(s_{t},a_{t};
                        \netWorker{})]\)}
    \EndIf
    \State{\(y_{i,\gamma_0} \gets r_{t} + \gamma_{0} \max_{a}
                                        \X^\pol_{\gamma_0}(s_{t+1},a; \netTarget{}) - \avgrew^{\pol}\)}
    \State{\(y_{i,\gamma_{1}} \gets r_{t} + \gamma_{1} \max_{a}
                                        \X^\pol_{\gamma_1}(s_{t+1},a; \netTarget{}) - \avgrew^{\pol}\)}
    \State{Sum gradients on \({(y_{i,\cdot} - \X^\pol_{\cdot}(s_{t+1},a;
        \netWorker{}))}^{2}\) wrt. parameters \(\netWorker{}\) in \(d\netWorker{}\)}
    \EndFor
    \State{}%
    \begin{minipage}[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}
      Perform update with average of the gradients \(d\netWorker{}/m\)  on \(\netWorker{}\)
    \end{minipage}
    \State{Every \(C\) steps exponential smoothly set target network:
      \(\netTarget \gets (1 - \gamma) \netTarget{} + \gamma \netWorker{}\)}
    \State{Set \(s \gets s'\), \(t \gets t+1\) and decay parameters}
    \EndWhile{}
  \end{algorithmic}
  \caption{\label{alg:near}Basic near-Blackwell-optimal deep RL for unichain MDPs}
\end{algorithm}


\paragraph{Average Reward Adjusted Deep RL Algorithm.} %
The model-free average reward adjusted RL algorithm is based on the tabular version of
\citet{schneckenreither2020average} and depicted in Algorithm~\ref{alg:near}. The algorithm operates
on a target network \(\netTarget{}\) and a worker network \(\netWorker{}\) as in
\citet{mnih2015human}. % The target network reduces fluctuations in the state value estimates and
% therefore reduces the
%
In the action selection process a \(\epsilon\)-sensitive lexicographic order
\(a=(a_{1},\ldots,a_{n}) \lex (b_{1},\ldots,b_{n})=b\) is used. It is defined as \(a \lex b\) if and
only if \(| a_{j} - b_{j} | \leqslant \epsilon\) for all \(j < i\) and
\(|a_{i} - b_{i}| > \epsilon\). Note that the resulting sets of actions may not be disjoint, but
taking the maximum as required in our algorithm is straight-forward and thus cheap to compute. The
first action selection criteria selects the actions which maximise the bias values, the second the
error term.
%
Equation~\ref{eq:constr2} of the average reward RL constraints is used to estimate the average
reward based on the currently inferred state values of two consecutive states and the returned
reward. % As the average
% reward is equal among all states estimating the average reward is based on many observations.
This infers a solid average reward prediction of the current policy in few steps only and thus
quickly leads to better policies, as ones with higher gain are preferred.
\citet{schneckenreither2020average} and
\citet{Tadepalli98_ModelbasedAverageRewardReinforcementLearning} find that updating the average
reward by Equation~\ref{eq:constr2} is superior as compared to exponentially smoothing the actual
observed rewards. This makes sense as the later rather evaluates past policies.

The average reward adjusted state value, which is approximated based on the Bellman optimality
equation, is estimated twice. Once to infer the bias values with a very high discount factor
\(\gamma_{1}\), e.g.\@ \(\gamma_{1} = 1.0\), and a second time with a discount factor
\(\gamma_{0} < \gamma_{1}\). The later is used to more selectively choose actions in case more than
one action leads to bias-optimal policies. The predetermined discount value \(\gamma_{0}\) is a
measure for farsightedness. Lower values prefer to partially collect reward in fewer steps and
higher values are used to prefer actions for which the full bias is collected earlier.
%
The corresponding update values are saved to calculate the gradients based on the quadratic errors.
Every \(C\) steps the target network smoothly adapts the parameters of the worker network.


\section{Experimental Evaluation}
\label{sec:Experimental_Evaluation}


This section introduces the simulation model and the parameterisation of the algorithm. To provide a
proof-of-concept for the proposed algorithm we adapt the flow shop presented in
\citet{schneckenreither2020order}. However, our algorithms operates on discrete lead times on a
product type basis and thus is not directly comparable to their order based lead time management
approach. Nonetheless, this section is largely based on the corresponding section of
\citet{schneckenreither2020order}.

\subsection{Simulation Model}
\label{subsec:Simulation_Model}

We adapt the simulation model of a make-to-order (MTO) flow shop with six different
products and six work-centers of \citet{schneckenreither2020order} and examine the system under
moderate and high utilisation in combination with moderate or high variability in demand and
processing times. This section provides the details of the simulation model and the experimental
setup.

% \subsection{Make-To-Order Flow Shop}
% \label{subsec:make-to-order_flow_shop}
% \subsection{Simulation model}
% \label{subsec:Simulation_model}


The simulation model is depicted in Figure~\ref{fig:ps}, where the customers initiate the process by
placing orders as indicated by the dashed line.
%
\begin{figure*}[t!]
  \centering
  \input{figures/productionsystem}                      % Figure production system
  \caption{Production System of the Simulation Model with routing, processing time distributions and
    demand interarrival time distributions.} \label{fig:ps}
\end{figure*}
%
%
The manufacturer produces six different product types, each of which is identified by a natural
number ${1,2,\ldots,6}$ and has a different routing through the production system. The routing setup
is provided by corresponding labels of the edges. For instance, product types $1-3$ are routed from
machine $M1$ to machine $M2$, while the other products are transferred to machine $M3$. The
production system consists of three production stages with diverging material flow and no return
visits.
%
The incoming orders are uniformly distributed among the product types.
%
A period lasts $960$ minutes ($=16$ hours) which represent two \(8\)-hour shifts. At the
beginning of each period orders can be released into the production system assuming material
availability for all orders at their release date. Upon release, the orders are
placed in the buffer at machine $M1$. Buffers and inventories are plotted as filled squares. Thus,
each workstation is equipped with a buffer which queues the orders until processing is started. All
buffers use a first-come-first-serve dispatching rule.
%
Furthermore, each workstation processes only one order at a time and early deliveries are
prohibited. Orders completed ahead of their due date remain in the finished goods inventory (\fgi{})
until they are due and sent to the customer.


\paragraph{Processing Times.} The machine processing time distributions are specified under the
corresponding node labels of the machines. To simulate high % or moderate
variance the processing times for all machines are drawn from an exponential % or an uniform
distribution% , respectively
. There is one bottleneck machine ($M5$) and therefore we refer to product types $2$ and $5$, those
routed through machine $M5$, as bottleneck products whereas products $1,3,4,6$ are non-bottleneck
products.

\paragraph{Demand.} The incoming orders are placed in the order pool with a due date slack of
\dds{} periods, that is each order arriving within period $t$ will be due at the end of period
$t+\dds{}$. The due date slack of \(\dds{}\) was chosen to (i) ensure sufficient time between the
first occurrence of orders in the order pool and the latest possible release of orders and (ii) to
have a clear cause and effect relationship between the order release decision and the cost
performance in the analysis.
%
To simulate medium and high variability of the demand process the interarrival time between
consecutive incoming orders is either drawn from an exponential ($\Exp$) or an uniform ($\Unif$)
distribution, which are adjusted to yield the desired bottleneck utilisation level of either $70\%$
with $\Exp(135)$ and $\Unif(95,175)$, $80\%$ with $\Exp(118)$ and $\Unif(78,158)$, or $90\%$ with
$\Exp(105)$ and $\Unif(65,145)$. These values were chosen as they comprise the non-linear
relationship between flow times and high utilisation levels.

\paragraph{Order Release.}
%
Before the order release decision is made, all orders in the order pool are sorted by due date. This
portrays the implementation of a sequencing rule. According to this sequence, orders are considered
for release in the beginning of each period starting with the highest priority order.

In our model, orders are released by specifying lead times
$\lt_{1}, \lt_{2}, \ldots, \lt_{6} \geqslant 1$, where the index corresponds to the product type and
$\lt_{i} \in \mathbb{N}$.
%
%
By setting a lead time $\lt_{i}$ a \emph{planned release date} is computed for each
order $j$ of product type $i$ in the order pool given by
%
\begin{linenomath*}
  \begin{equation}
    \prd{}_j= \dd{}_j - \lt_{i}\ ,
    \label{eq:Orel}
  \end{equation}
\end{linenomath*}

where $\dd_{j}$ denotes the due date of the order.  For dynamic order releases the lead times
$\lt_{i}$ may vary from period to period, whereas in static order release methods the lead times are
predetermined and fixed (\citealt{RagatzMabert1988,KimBobrowski1995}).
%
For both approaches a job of product type $i$ with lead time $\lt{}_{i}$ in period $t$ and due date
$\dd_{j}$ is released at the end of period $t$ if and only if $t \geqslant \prd{}_{j}$.
%
Thus, in the dynamic setting  the planned
release date $\prd{}_{j}$ of order $j$ can be updated several times before it is actually released
to the production system.
%
However, once an order is released its release cannot be revoked.
%
% If there are several orders with the same due date the order release
% sequence is randomized.
%
Hence, if the set lead time corresponds with the actual flow time the product is finished in the
same period as it is shipped. If the lead times are set either too long or short,
the order has to wait in the \fgi{} until it is due or the order is late and backorder (BO) costs
occur.

\paragraph{Costs.}
%
The cost parameters are set by assuming an increase in value from raw material ($1$~Dollar per order
and period) to the final product ($4$~Dollar per order and period) and the backorder costs are set
very high ($16$~Dollar per order and period) due to the MTO environment. All costs are assessed
based on the production system state at the end of each period.


\subsection{Conventional Order Release Rule}

As external benchmark for comparison we use different parameterized backward infinite loading
(\BIL{}) techniques \citep{Ackerman1963}. \BIL{} uses Equation~\ref{eq:Orel} with a predetermined
and fixed lead times \(\lt_{i}\).
%
% \begin{equation*}
%   \rd_{j} = \dd_{j} - \lt_{j} \tcom
%   \label{eq:BIL}
% \end{equation*}
%
% where the release date \(RD_{j}\) of product type \(j\) is calculated by the difference of the due
% date \(DD_{j}\) and the lead time \(LT_{j}\).
%
% Note that we set lead times for each product group and not for every order as done by
% \cite{Ackerman1963}.


\subsection{Algorithm Setup}
\label{subsec:Algorithm_Setup}

\paragraph{Markov Decision Process.} The underlying MDP is unichain and looks as follows. Both,
state space $\mathcal{S}$ and action space $\mathcal{A}$ are discrete.
%
% Recall that we products routed over the bottleneck machine \(M5\) are bottleneck products.
%
For the \textit{state space} we adapt the result of \citet{knollmann2013control}, which state that
the lead times ought to control the WIP level, instead of the due date reliability as changes are
directly seen. Thus, any state $s \in \mathcal{S}$ of the state space is composed of the following
information for each product type \(\pt\):
\begin{itemize}

\item The currently set lead time $LT_{g} \in \{1,2,\ldots,\DDS\}$ (Recall:
  $\text{Due Date Period} - \text{Lead Time} = \text{Release Period}$). Note that we bound the
  maximum lead time with due date slack $\DDS$, which is $7$ in our setup.

\item Counters $OP_{g,d}\in \N$ for the number of orders in the order pool divided in time buckets
  with $d \in \{1,2,\ldots,\DDS \}$, which stands for the number of periods until the due date.

  % \item Flag for each machine whether it is idle or not.

\item Counters $Q_{g,i,d}\in \N$ for the number of orders in the machine and queue \(i \in
  \{M_{1}, M_{2}, \ldots , M_{6}\}\) divided in
  time buckets with $d \in \{1,2,\ldots,\DDS \}$, which stands for the number of periods until the
  due date.
  % \MS[t]{depend}If a product type is not routed over the corresponding machine, then no entry for this
  % product type is specified.

\item Counters $FGI_{g,d} \in \N$ of orders in the finished goods inventory divided in time
  buckets with $d \in \{1, 2,\ldots, \DDS \}$, which stands for the number of periods until the
  due date. Orders with a due date with more than 3 periods ago are listed in the counter
  $FGI_{-3}$.

\item Counters $S_{g,d} \in \N$ of shipped orders from the last period divided in time buckets
  with $d \in\{-3,-2,\ldots, \DDS \}$, which stands for the number of periods until the due date.
  Orders with a due date with more than 3 periods ago are listed in the counter $S_{-3}$.

\end{itemize}

The algorithm implicitly learns a function which maps the current state of the production
system to a lead time update and thus a release decision. In an optimal situation it does so by
multiply exploring every action for each state and assessing its economic viability.
%
Orders are released once the due date is within the interval $[t,t+\text{lead time}]$, whereas $t$
is the current period. Clearly, this yields bulk releases, meaning that either all or no orders with
the same due date and of the same product type are released.

The \textit{action space} is composed of two decisions, the relative change of the lead times to the
currently set lead times $LT_{g} \in \{1,2,\ldots,\DDS\}$ for each product type $\pt$.
%
We restrict the lead time update for state $s_{t+1}$ according to the lead time $LT_{g}$ from state
$s_{t}$ by a maximum change of 1. Thus, if $LT_{g}$ is the current lead time of product type $\pt$
the agent can choose a lead time in $\{1,2,\ldots,\DDS\} \cap \{LT_{g}-1,LT_{g},LT_{g}+1\}$. % Put
% differently the algorithm can increase or decrease the lead time by 1 or leave it as it is, as long
% as it acts within the discrete action space given by the set $\{1,2,\ldots,\DDS\}$ for each product
% type.
A naive approaches is to define one action for each combination of actions over all product type.
This however, due to the exponential growth, results in only being able to solve very small
production systems with only few product types. A common solution in production planning is to
aggregate the products into product groups, cf.\@ hierarchical production
planning~\citep{schneeweibeta1995hierarchical}.
%
Although applicable in our setup, if we group the products into bottleneck and non-bottleneck
products as done by \citet{schneckenreither2020order}, also this approach simply does not scale.


\begin{figure*}[t!]
  \centering
  \input{figures/indep_agents}                      % Figure production system
  \caption{Illustration of independent agents as agent one increases the lead time}\label{fig:indep-agents}
\end{figure*}
%
Therefore, in the seek of a broader solution we establish the idea of \textit{independent agents}.
Each independent agent operates on the same system state, but chooses only a part of the actual
executed action. In our case each independent agent is responsible for the lead time update of a
specific product type. To the best of the author's knowledge this is the first work to reduce the
action space in this way. Figure~\ref{fig:indep-agents} illustrates the idea, where wlog.\@ we use
two agents and for simplicity omit the reward and truncate the state to the lead times. Assume that
the first independent agent chooses to increase the lead time from \(3\) to \(4\). Thus, the future
state, seen from this agent, depends on the probability of the other independent agent choosing to
increase (\(\inc\)), decrease (\(\dec\)) or not change (\(\pers\)) the lead time. The Markov
property, that is
\(p(s_{t+1}, r_{t} \mid s_{t}, a_{t}) = p((4,\cdot), r_{t} \mid (3,3), (\inc, \cdot)) \), is not
touched by this adaption. While for the first agent the reward and future state only depends on the
current state and action taken, the corresponding probabilities for future state and reward change
during the learning process as the the agent starts to behave differently.
%
This allows us to investigate the algorithms behaviour for even larger solution spaces. In
particular, this adaption enables us to present the algorithm the above defined state space not only
on the basis of product groups, but directly for each product type \(\pt\). Note that without this
extension the problem is computationally infeasible due to \(3^{6} = 729\) possible actions.
Removing the dependency between the lead times of different product types reduces the set of
possible actions, and thus the number of ANN output layer nodes, to \(3 \cdot 6 = 18\). In
preliminary experiments on small examples we only observed a very slight increase of required steps
during training. Therefore, this is a vital adaption to ensure scalability.

\paragraph*{Reward.}
In each period the agents choose actions, which when executed generate a reward while traversing to
the next period by simulating the production system. The rewards are the accumulated costs at the
end of the period consisting of the backorders, the WIP orders and the number of orders in the
FGI\@. The objective is to minimise the returned reward. % Therefore, we adapt the algorithm by
% swapping all maximisation functions with minimisations.

\paragraph*{Optimised Algorithm.}
Our implementation includes several optimisations, that help the algorithm in better exploring the
solution space and stabilise the learning process. These include the above discussed independent
agents that operate on the same state, \(n\)-step learning \citep{mnih2016asynchronous},
% overestimating the average reward \(\avgrew^{\pol}\)
and ensuring to constantly reduce the average reward as searching for policies with higher average
costs is unnecessary. Algorithm~\ref{alg:full}
depicts a detailed view on the algorithm including these extensions and with the objective to
minimise the returned rewards. The idea is the same as in Algorithm~\ref{alg:near}, except that the
action selection procedure is a loop over all independent agents and the implementation of the
n-step logic.
%
The algorithm is based around The agent first choose an action (steps \(4-5\)), executes the
selected action and stores the experience in the replay memory (step \(6\)) and then samples a
mini-batch of experiences each of length \(nstep\) to improve the policy (steps \(7-21\)). Step
\(17\) uses an exponentially smoothed overestimation (by \(2.5\%\)) of the average reward and rate
\(2^{-5}\). All average reward updates above this overestimation are rejected.


% N-Step: 5
As in \citep{mnih2016asynchronous, mnih2015human} the algorithm uses a \textit{Replay Memory} to
save experiences and then randomly selects experiences from there for learning. This ensures that
the independent and identically distributed (i.i.d.) assumption of the samples for the ANN
backpropagation algorithms are satisfied. We extend this idea by splitting the available size of the
replay memory into subsets, where each subset represents one action and sample uniformly over these
subsets first. This prevents overfitting when one action is overrepresented in the policy, e.g.\@ no
lead time change occurs much more often than the other actions.

\textit{N-step learning} collects a set of consecutive experiences, in our case retrieved from the
replay memory, and connects the state values starting from the latest observed experience, such that
the newly computed state value of the corresponding experience propagates to the experience of the
previous period. This allows a faster convergence, as values bootstrap over multiple periods in
contrast to just one step. The experiences are stored according to the first taken action in the
n-step series. We uniformly sample over the actions to ensure each action is constantly trained.
This adaption reduces catastrophic forgetting of already accumulated knowledge.
%
% Overestimating
% The idea behind overestimating the average reward is to constantly seek for better policies than the
% currently known. We overestimate the reward by subtracting \(10\%\) of the computed average reward
% \(\avgrew^{\pol}\) and an exponentially smoothed average reward of the actually returned reward
% values of greedy actions. Thus, if the computed and actually observed average reward coincide, the
% overestimation equals \(0\).
% Rho Minimum
% Preventing an increase of the average reward with \(\avgrew_{\max}^{\pol}\) reduces the search space
% and provides a methodology for constantly improving policy.
%
% Workers Min Exploration [0.01, 0.02, 0.03, 0.04, 0.05, 0.10, 0.15, 0.20]
Furthermore, we use \(8\) additional \textsf{worker threads} (parallel RL) that operate on the same
environment to collect a broader range of experiences~\citep[as e.g. in][]{mnih2016asynchronous}.
%
% Shared Rho (Yes)
The average reward value is shared and computed as average over the main agent and all workers.
% \MS[t]{shared rho}
%
%
% \MS[t]{worker min exploration, epsilon greedy exploration}
%
%
% Learning rate decay: ExponentialDecay (Just 5e-6) 0.25 100000
% Learning rate: OptAdam 0.005 0.9 0.999 1e-8 1e-3,


% Algorithms tested:
 %            AlgDQNAvgRewAdjusted 0.8 1.0 ByStateValues
 %          , AlgDQNAvgRewAdjusted 0.8 0.99 ByStateValues
 %          , AlgDQN 0.99 Exact


% Alpha: 0.01, Decay: ExponentialDecay (Just 5e-5) 0.25 50000
% AlphaRhoMin: 2e-5, Decay: ExponentialDecay (Just 2e-5) 0.25 50000
% Gamma: 0.01, Decay: ExponentialDecay (Just 1e-3) 0.25 100000
% Epsilon: 0.25, 0.25, Decay: No decay
% Exploration: 1.0, Decay: ExponentialDecay (Just 0.005) 0.25 100000
% LearnRandomAbove: 0.5
%
%
% UNUSED: Delta: 0.005, Decay: ExponentialDecay (Just 5e-4) 0.25 100000


The algorithm initiation strategy is to first fill the replay memory buffer by skipping any learning
(steps \(7-21\) in Algorithm~\ref{alg:full}) until the period, that corresponds to the replay
memory size, is reached. Then for \(500\) steps the average reward is set to the last computed
exponentially smoothed value to prevent divergence due to the untrained network weight
initialisation values.


% \MS[t]{Extensions: overestimate rho, rho minimum, Multiple Agents (shared state,  shared rho?),
%   init-phase (0.5 for expSmthRewRate, always adapt rho, same rho for all agents): exp smooth reward
%   as rho, replay memory actions, n-step (describe replay memory), gradient clipping }
% \MS[t]{Side info: Replay memory also stores (disallowed) filtered action indices for optimisation purposes and on
% episodic tasks needs to store a bool indicating an episode end. Uses Int8 for state features values.  }


\begin{algorithm}[t!]
  \begin{algorithmic}[1]
    \State{}Initialise state \(s_{0}\) and network parameters \(\netTarget, \netWorker\) randomly,
    set an exploration rate \(0 \leqslant \plearn \leqslant \pexp \leqslant 1\), exponential
    smoothing learning rates \(0 < \alpha_{\max}, \alpha, \gamma < 1\), a sufficiently large default
    value for \(\avgrew_{\max}^{\pol}\) and discount factors \(0 < \gamma_{0} < \gamma_{1} \leqslant
    1\)
    \While{the stopping criterion is not fulfilled}
    \State{\(\isRandAct{t} \gets \mathsf{False}\)}
    \ForEach{independent agent \(i \in \agent_{1},\agent_{2},\ldots,\agent_{n}\)}
    \State{}%
    \begin{minipage}[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth-\leftmargin+2pt}
      With probability \(\frac{\pexp}{n}\) choose action \(a_{t,i}\) randomly or otherwise one of
      the set defined by
      \(\min_{a \in \mathcal{A}(i)}\lexeq(\X^{\pol}_{\gamma_{1}}(s_{t},a; \netWorker{}),\X^\pol_
      {\gamma_{0}}(s_{t},a; \netWorker{}))\). Set \(\isRandAct{t} \gets \mathsf{True}\) if the
      action was chosen randomly.
    \end{minipage}
    \EndFor{}
    \State{}%
    \begin{minipage}[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}
      Carry out action \(a_{t}=(a_{t,\agent_{1}},\ldots,a_{t,\agent_{n}})\), observe reward
      \(r_{t}\) and resulting state \(s_{t+1}\). Store experience
      \((s_{t}, a_{t}, \isRandAct{t}, r_{t}, s_{t+1})\) in the experience replay memory \(M\).
    \end{minipage}
    \If{\(t \mod{\nstep} \equiv 0\)}
    \State{Reset gradient: \(d\netWorker{} \gets 0\)}
    \State{}%
    \begin{minipage}[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth-\leftmargin+2pt}
      Sample \(m\) random mini-batches of \(\nstep\) consecutive experiences from \(M\) numbered by
      \(1 \leqslant i \leqslant \nstep\), each with
      \((s_{i}, \isRandAct{i}, a_{i}, r_{i}, s_{i+1})\).
    \end{minipage}
    \ForEach{mini-batch}
    \ForEach{experience, starting with the latest seen experience \(\nstep\)}
    \If{experience \(\nstep\) or \(\neg \isRandAct{i}\) or  \(\pexp > \plearn\)}
      \State{\(\target_{\gamma0} \gets \min_{a} \X^\pol_{\gamma_0}(s_{i+1},a; \netTarget{})\)}
      \State{\(\target_{\gamma1} \gets \min_{a} \X^\pol_{\gamma_1}(s_{i+1},a; \netTarget{})\)}
    \EndIf
    \If{\(\neg \isRandAct{i}\) or \(\pexp > \plearn\)}
      \State{\(\avgrew^{\pol} \gets (1- \alpha) \avgrew^{\pol} + \alpha [r_{i} +
        \min_{a}\X^\pol_{\gamma_1}(s_{i+1},a; \netWorker{}) - \X^\pol_{\gamma_1}(s_{i},a_{i};
        \netWorker{})]\)}
      \State{Ensure improvement of \(\avgrew^{\pol}\), otherwise reset to old value}

    % \State{\(\avgrew^{\pol} \gets \min(\avgrew^{\pol}, \avgrew_{\max}^{\pol})\)}
    % \State{\(\avgrew_{\max}^{\pol} \gets (1- \alpha_{\max}) \avgrew_{\max}^{\pol} +
    % \alpha_{\max} 1.025 \avgrew^{\pol}\)}
    \EndIf
    % \State{Compute overestimate of \(\avgrew^{\pol}\) and store result in \(\avgrew_{\Overestim}^{\pol}\)}
    \State{\(\target_{\gamma_0}  \gets r_{i} + \gamma_{0} \target_{\gamma_{0}} - \avgrew^{\pol}  \)}
    \State{\(\target_{\gamma_{1}}  \gets r_{i} + \gamma_{1} \target_{\gamma_{1}} - \avgrew^{\pol}\)}
    \State{\(d\netWorker{} \gets d\netWorker{} + \partial{(\target_{\gamma_{0}} - \X^\pol_{\gamma_{0}}(s_{i+1},a; \netWorker{}))}^{2} / \partial\netWorker{}\)}
    \State{\(d\netWorker{} \gets d\netWorker{} +
      \partial{(\target_{\gamma_{1}} - \X^\pol_{\gamma_{1}}(s_{i+1},a; \netWorker{}))}^{2} / \partial\netWorker{}\)}
    \EndFor{}
    \EndFor{}
    \State{Perform update of \(\netWorker{}\) using \(d\netWorker{} / (m \cdot \nstep)\)}
    \EndIf{}
    \State{Every \(C\) steps exponential smoothly set target network:
    \(\netTarget \gets (1 - \gamma) \netTarget{} + \gamma \netWorker{}\)}
    \State{Set \(s \gets s'\), \(t \gets t+1\) and decay parameters}
    \EndWhile{}
  \end{algorithmic}
  \caption{\label{alg:full}Optimised near-Blackwell-optimal deep RL for unichain MDPs}
\end{algorithm}


\paragraph*{Neural Network Setup.}
%
Due to the vast possibilities the neural network architecture was manually optimised, starting with
less stochastic systems and aiming for a coffin shape that can be used in all setups. During this
process we constantly adapted and reevaluated the best network on various production system
configurations. The resulting architecture depends on the number of features \(\anninp\) and the
number of output nodes \(\annout\), which itself depends on the number of actions
\(\acts = \size{\States}\), the independent agent configuration, and the number of functions that
are approximated. For the later, in case of \ARA{} this is \(2\), as we represent both state-action
value functions \(\X^\pol_{\gamma_{0}}\) and \(\X^\pol_{\gamma_{1}}\) with the same neural network,
while deep Q-Learning \citep[cf.][]{mnih2015human} only approximates the function
\(Q_{\gamma}^\pol\).
%
The resulting feedforward deep ANN architecture is given as %
\(\anninp %
\rightarrow 1.75 \anninp % \rightarrow \relu
\rightarrow 1.5 \anninp % \rightarrow \relu
% \rightarrow 2 \annout % \rightarrow \relu
\rightarrow \annout %
% \rightarrow   \leakyTanh%
\), where each arrow symbolises a fully-connected layer with a leaky rectified linear unit
(\(\relu\)) as activation function, which leaks values smaller than \(0\) with rate \(0.02\).
%
Note
that we explicitly do not use an output activation function. The idea is that we do not want to
restrict the approximations to values inside this range. This is especially important for
Q-Learning, as the range of the state values are difficult to forecast.
%
The actual values \(\anninp\) and \(\annout\) depend on the used algorithm. In our case using 6
independent agents we gain \(\anninp=324\) and \(\annout=36\).
%
% SpecFullyConnected 324 567 :=> SpecLeakyRelu (567,1,1) :=> SpecFullyConnected 567 486 :=>
% SpecLeakyRelu (486,1,1) :=> SpecFullyConnected 486 36 :=> SpecReshape (36,1,1) (18,2,1) :=>
% SpecNNil2D18x2
%


% Min-Max Scaling of input/output. Output: ScalingNetOutParameters (-800) 800 (-300) 300 (-400) 800 (-400) 800
The features representing order counts are scaled using min-max scaling with minimum \(0\) and
maximum \(5\), the lead times with minimum \(0\) and maximum \(7\) respectively.
% Output layer scaling
To further ensure smaller ANN parameter values the output is transformed using min-max scaling to
the interval \((-400, 800)\), s.t.\@ an ANN output of \(1\) represents a value of \(800\).
%
% Training Mini-Batch Size:  4
On each training step, i.e.\@ every n-step steps, randomly \(m\) sets of experiences the training
mini-batch size each consisting of n-step experiences are selected, there gradients computed and the
sum of gradients backpropagated though the worker network using Adam \citep{kingma2014adam}, where
the weight decay with parameter \(\lambda\) is implemented as presented by
\citet{loshchilov2017decoupled}.
%
% Smooth target network period: 100 with update 0.01
Every \(C\) steps the target network is updated by exponentially smoothing the parameters with rate
\(\gamma\).
%
% ANN Init
As neural network
initialisation method we use the commonly known Xavier initialisation
\citep{glorot2010understanding}, i.e.\@ the initial value for each weight \(i\) is sampled from a
uniform distribution \(W_{l,i} \sim \Unif(-1/\sqrt{n_l},1/\sqrt{n_l})\), where \(n_{l}\) is the
number of nodes in layer \(l\).
%


\subsection{Deep Q-Learning Algorithm}

Furthermore, we adapt the deep Q-Learning algorithm of \citet{mnih2015human} with integrated
extensions as described above, i.e.\@ the adaptions to n-step learning, multiple workers,
independent agents, scaling of the output, smoothly adapting the target network. The resulting
Algorithm~\ref{alg:ql} is provided in the Appendix.

To ensure comparability we use the same neural network setup as for \ARA{}, except that we halve the
number of output nodes, as the Q-Learning algorithm only approximates one state-action value
function. Therefore the output nodes \(\annout\) are \(18\), while the input nodes \(\anninp=324\)
stay the same.


\section{Computational Experiments}
\label{sec:Computational}

This section presents the experimental results and gives an overview of the performance of the
established algorithm compared to the conventional static lead time setting algorithms.

% \MS[t]{todo} We evaluate two different kinds of reward reporting as they have been proposed by
% \cite{Schneckenreither2019}. They find that it is beneficial to overcome the time offset imposed
% by the production system. That is, although orders are immediately released to the production
% system and thus generate a response in terms of a reward to the agent, the full impact of the
% chosen action takes several periods to materialise. For instance, by deciding not to release an
% order, which later becomes a backorder, it is beneficial to reward the actions actually
% responsible for the backorder instead of the last action (only). Thus they propose to keep track
% of the orders currently in the order pool until all fully traversed through the production system
% and reward the actions according to all orders in the order pool.

\begin{table}[t]
  \centering
  \footnotesize
  \begin{tabular}{lP{1cm}P{1cm}P{1cm}P{1cm}P{1cm}P{1cm}P{1cm}P{1cm}}
    \toprule
    Parameter                      & $\alpha$   & $\gamma_{0}$ & $\gamma_{1}$ & $\gamma$ & $\epsilon$ & $p_{exp}$ & $\plearn$ \\
    \midrule
    Start value                    & 0.01       & 0.8          & 0.99/1.0     & 0.01     & 0.25       & 1.0       & 0.5       \\
    Exp. Smth. Rate                & 0.25       &              &              & 0.25     &            & 0.25      &           \\
    Exp. Smth. Steps in \(10^{3}\) & 50         &              &              & 100      &            & 100       &           \\
    Minimum value                  & \(5^{-5}\) &              &              & 0.001    &            & 0.005     &           \\
    \bottomrule
    \toprule
    Configuration                  & \multicolumn{7}{c}{Value}                                                                                  \\
    \midrule
    Workers Min. Exploration       & \multicolumn{7}{c}{\(0.01, 0.02, 0.03, 0.04, 0.05, 0.10, 0.15, 0.20\)}                                         \\
    Target Network Update          & \multicolumn{7}{c}{Exp. smooth. with rate \(\gamma\) every \(C=100\) steps}                                \\
    N-Step                         & \multicolumn{7}{c}{\(5\)}                                                                                      \\
    Training Mini-Batch Size       & \multicolumn{7}{c}{\(4\)}                                                                              \\
    Adam ANN Backpropagation       & \multicolumn{7}{c}{\(\alpha = 0.005, \beta_{1}=0.9, \beta_{2}=0.999, \epsilon=10^{-8}, \lambda=0.001\)}      \\ % OptAdam 0.005 0.9 0.999
% 1e-8 1e-3,
        Replay Memory Size         & \multicolumn{7}{c}{\(10800\)}                                                                              \\
    \bottomrule
  \end{tabular}
  \caption{\label{tbl:params}Overview of Parameter Setup.}
\end{table}
%
Table~\ref{tbl:params} presents an overview of the parameter setup. The parameters \(\alpha\),
\(\gamma\) and \(p_{exp}\) are exponentially decayed as indicated. Due to the vast possibilities we
manually optimised the values. The methodology was to investigate the values on very small
production system with just one machine and product due to the computational complexity of bigger
sized problems. The resulting values were copied to the problem at hand and further manually
optimised in pre-experimental runs. The actual used discount factors \(\gamma_{1}\) differ for each
method and thus are further specified in the result tables.
% \footnote{The decay is defined as \(0.05 ^{(t/350k)}\), where \(t\) is the current period}.
In a similar approach we reached the configuration values, where a further increase of the values
was hindered by the available computational resources.


For the experimental runs we used personal computers with Intel CPUs i7-7700 (16GB RAM) and i9-7900X
(32GB RAM). For each method we performed \(500.000\) learning steps, which took about \(2\)-\(3\)
days. For the same experimental setup the inputs/random variables are the same for each period.
After learning each order release method was evaluated on \(20\) demand streams, each with \(1000\)
periods of warm-up and \(7000\) periods of evaluation phase. Welch’s procedure was applied to
approximate the length of the warm-up period (see~\cite{law:simulationc}).

  % a discount factor of \(0.99\).
  %    demandUnif65_145            -- 90%   -- 6.
  % -- demandUnif78_158 -- 80%   -- 3.
  % -- demandUnif95_175  -- 70%   -- 1.
  % -- demandExp105  -- 90%           -- 5.
  % -- demandExp118  -- 80%          -- 4.
  % -- demandExp135  -- 70%          -- 2.
 \begin{table}[tb]
  \begin{center}
    \begin{tabular}{crrrrrlrrrrr}
      % \toprule
      % For 10 replications
      % 70-\(\Unif\) & WIPC              & FGIC              & BOC                & \fgibocName                     & SUM                        &                          & SFTT          & FGIT                & SL(\%)       & Cost.p.P                     & \(\avgrew^{\pol}\) \\
      % \midrule     &
      % \ql{0.99}    & \cost{38776.300}  & \cost{133872.800} & \cost{82673.600 }  &                                 & \cost{255322.700}          &                          & \timem{1.340} & \timem{2.558}       & \tard{0.095} &                              &                    \\
      % \rl{0.99}    & \cost{37111.000}  & \cost{113425.600} & \cost{98654.400 }  &                                 & \cost{249191.000}          &                          & \timem{1.305} & \timem{2.412}       & \tard{0.110} &                              &                    \\
      % \rl{1.00}    & \cost{38755.000}  & \cost{156444.000} & \cost{59718.400 }  &                                 & \cost{254917.400}          &                          & \timem{1.339} & \timem{2.679}       & \tard{0.068} &                              &                    \\
      % \BILOne{}    & \cost{32159.900}  & \cost{     0.000} & \cost{514558.400}  &                                 & \cost{546718.300}          &                          & \timem{1.202} & \timem{1.693}       & \tard{0.552} &                              &                    \\
      % \BILTwo{}    & \cost{32169.900}  & \cost{ 83391.200} & \cost{106219.200}  &                                 & \cost{221780.300}          &                          & \timem{1.202} & \timem{2.143}       & \tard{0.114} &                              &                    \\
      % \BILThree{}  & \cost{32239.800}  & \cost{247416.800} & \cost{21382.400 }  &                                 & \cost{301039.000}          &                          & \timem{1.204} & \timem{3.029}       & \tard{0.022} &                              &                    \\
      % \BILFour{}   & \cost{32262.600}  & \cost{428865.200} & \cost{5483.200  }  &                                 & \cost{466611.000}          &                          & \timem{1.204} & \timem{4.007}       & \tard{0.006} &                              &                    \\
      % \bottomrule
      \toprule
      70-\(\Unif\)   & WIPC              & FGIC              & BOC                & \fgibocName                     & SUM                        & \multicolumn{2}{r}{SFTT} & FGIT          & SL(\%)              & Cost.p.P     & \(\avgrew^{\pol}\)                                \\
      \midrule
      \ql{0.99}      & \cost{38563.000}  & \cost{134703.600} & \cost{81704.800}   & \fgiboc{134703.600}{81704.800}  & \cost{254971.400}          & $^{1}$                   & \timem{1.335} & \fgit{1.335}{2.558} & \tard{0.094} & \costpp{254971.400}          &                    \\
      \rl{0.99}      & \cost{37197.750}  & \cost{113100.800} & \cost{100398.400}  & \fgiboc{113100.800}{100398.400} & \cost{250696.950}          &                          & \timem{1.307} & \fgit{1.307}{2.412} & \tard{0.111} & \costpp{250696.950}          & \rnd{35.040}       \\
      \rl{1.00}      & \cost{38659.100}  & \cost{156659.200} & \cost{59743.200}   & \fgiboc{156659.200}{59743.200}  & \cost{255061.500}          & $^{1}$                   & \timem{1.338} & \fgit{1.338}{2.678} & \tard{0.068} & \costpp{255061.500}          & \rnd{36.501}       \\
      \BILOne{}      & \cost{32056.500}  & \cost{0.000}      & \cost{512904.000}  & \fgiboc{0.000}{512904.000}      & \cost{544960.500}          &                          & \timem{1.200} & \fgit{1.200}{1.691} & \tard{0.551} & \costpp{544960.500}          &                    \\
      \BILTwo{}      & \cost{32063.850}  & \cost{83461.000}  & \cost{105009.600}  & \fgiboc{83461.000}{105009.600}  & \textbf{\cost{220534.450}} &                          & \timem{1.200} & \fgit{1.200}{2.142} & \tard{0.114} & \textbf{\costpp{220534.450}} &                    \\
      \BILThree{}    & \cost{32155.150}  & \cost{247727.000} & \cost{21680.800 }  & \fgiboc{247727.000}{21680.800 } & \cost{301562.950}          &                          & \timem{1.202} & \fgit{1.202}{3.029} & \tard{0.022} & \costpp{301562.950}          &                    \\
      \BILFour{}     & \cost{32140.550}  & \cost{429097.800} & \cost{5076.800 }   & \fgiboc{429097.800}{5076.800 }  & \cost{466315.150}          &                          & \timem{1.202} & \fgit{1.202}{4.007} & \tard{0.005} & \costpp{466315.150}          &                    \\
      \bottomrule
      \toprule
      80-\(\Unif\)   & WIPC              & FGIC              & BOC                & \fgibocName                     & SUM                        & \multicolumn{2}{r}{SFTT} & FGIT          & SL(\%)              & Cost.p.P     & \(\avgrew^{\pol}\)                                \\
      \midrule
      \ql{0.99}      & \cost{63095.100}  & \cost{180403.800} & \cost{121528.000}  & \fgiboc{180403.800}{121528.000} & \textbf{\cost{365026.900}} &                          & \timem{1.680} & \fgit{1.680}{3.019} & \tard{0.103} & \textbf{\costpp{365026.900}} &                    \\
      \rl{0.99}      & \cost{65310.650}  & \cost{168489.200} & \cost{148593.600}  & \fgiboc{168489.200}{148593.600} & \cost{382393.450}          &                          & \timem{1.721} & \fgit{1.721}{3.005} & \tard{0.136} & \costpp{382393.450}          & \rnd{52.918}       \\
      \rl{1.00}      & \cost{60778.500}  & \cost{234045.000} & \cost{76168.000}   & \fgiboc{234045.000}{76168.000}  & \cost{370991.500}          & $^{1}$                   & \timem{1.637} & \fgit{1.637}{3.226} & \tard{0.066} & \costpp{370991.500}          & \rnd{52.291}       \\
      \BILOne{}      & \cost{51888.650}  & \cost{0.000}      & \cost{830218.400}  & \fgiboc{0.000}{830218.400}      & \cost{882107.050}          &                          & \timem{1.474} & \fgit{1.474}{1.968} & \tard{0.672} & \costpp{882107.050}          &                    \\
      \BILTwo{}      & \cost{51998.950}  & \cost{70108.800}  & \cost{255166.400}  & \fgiboc{70108.800}{255166.400}  & \cost{377274.150}          &                          & \timem{1.476} & \fgit{1.476}{2.298} & \tard{0.206} & \costpp{377274.150}          &                    \\
      \BILThree{}    & \cost{51976.550}  & \cost{240175.000} & \cost{77822.400}   & \fgiboc{240175.000}{77822.400}  & \cost{369973.950}          & $^{1}$                   & \timem{1.475} & \fgit{1.475}{3.091} & \tard{0.060} & \costpp{369973.950}          &                    \\
      \BILFour{}     & \cost{51827.050}  & \cost{442307.600} & \cost{26711.200}   & \fgiboc{442307.600}{26711.200}  & \cost{520845.850}          &                          & \timem{1.472} & \fgit{1.472}{4.031} & \tard{0.019} & \costpp{520845.850}          &                    \\
      \bottomrule
      \toprule
      90-\(\Unif\)   & WIPC              & FGIC              & BOC                & \fgibocName                     & SUM                        & \multicolumn{2}{r}{SFTT} & FGIT          & SL(\%)              & Cost.p.P     & \(\avgrew^{\pol}\)                                \\
      \midrule
      \ql{0.99}      & \cost{98909.150}  & \cost{190681.000} & \cost{297333.600}  & \fgiboc{190681.000}{297333.600} & \cost{586923.750}          & $^{1}$                   & \timem{2.133} & \fgit{2.133}{3.417} & \tard{0.173} & \costpp{586923.750}          &                    \\
      \rl{0.99}      & \cost{105348.200} & \cost{263950.600} & \cost{207183.200}  & \fgiboc{263950.600}{207183.200} & \textbf{\cost{576482.000}} & $^{1}$                   & \timem{2.238} & \fgit{2.238}{3.825} & \tard{0.121} & \textbf{\costpp{576482.000}} & \rnd{79.826}       \\
      \rl{1.00}      & \cost{107873.800} & \cost{240798.800} & \cost{253865.600}  & \fgiboc{240798.800}{253865.600} & \cost{602538.200}          & $^{2}$                   & \timem{2.280} & \fgit{2.280}{3.771} & \tard{0.153} & \costpp{602538.200}          & \rnd{80.425}       \\
      \BILOne{}      & \cost{88658.100}  & \cost{0.000}      & \cost{1418529.600} & \fgiboc{0.000}{1418529.600}     & \cost{1507187.700}         &                          & \timem{1.964} & \fgit{1.964}{2.462} & \tard{0.790} & \costpp{1507187.700}         &                    \\
      \BILTwo{}      & \cost{88317.700}  & \cost{50854.400}  & \cost{646000.800}  & \fgiboc{50854.400}{646000.800}  & \cost{785172.900}          &                          & \timem{1.958} & \fgit{1.958}{2.666} & \tard{0.355} & \costpp{785172.900}          &                    \\
      \BILThree{}    & \cost{88252.450}  & \cost{207210.600} & \cost{299885.600}  & \fgiboc{207210.600}{299885.600} & \cost{595348.650}          & $^{2}$                   & \timem{1.957} & \fgit{1.957}{3.309} & \tard{0.148} & \costpp{595348.650}          &                    \\
      \BILFour{}     & \cost{87774.600}  & \cost{415178.200} & \cost{153612.800}  & \fgiboc{415178.200}{153612.800} & \cost{656565.600}          &                          & \timem{1.950} & \fgit{1.950}{4.158} & \tard{0.071} & \costpp{656565.600}          &                    \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{\label{tbl:unif} Results with moderate variability in the system due to uniformly
    distributed demand interarrival times. \(^{1,2}\) Costs marked with the same number are
    not significantly different from each other.}
\end{table}

\begin{comment}
* Unif-70%

           QL	AvgQL0.99	AvgQL1.0	BIL1	    BIL2	  BIL3	  BIL4
P_1    251303	   243743	  259512	540549	213541	295577	467766
P_2    264665	   247607	  253436	533630	217064	296323	466820
P_3    260043	   257121	  256038	564332	232896	303723	465187
P_4    250869	   256036	  254776	541620	221602	301724	465467
P_5    254079	   250814	  252465	546975	225815	300616	468210
P_6    247780	   248298	  254246	549508	220674	300177	467039
P_7    265459	   248659	  257828	546295	220635	305236	464263
P_8    251896	   245786	  254077	544850	219209	301524	466451
P_9    254502	   245544	  254622	545734	221235	302580	466944
P_10   252631	   248302	  252174	553690	225132	302910	467963
P_11   253068	   258304	  253873	539393	216099	302757	465670
P_12   260997	   254848	  250599	547757	227971	303835	463119
P_13   253329	   248691	  256341	549066	219881	306322	469024
P_14   254715	   267491	  259271	559487	216614	304241	465459
P_15   251007	   248119	  259367	528088	213454	301483	464478
P_16   254686	   250503	  257132	552262	219856	304220	466426
P_17   250830	   249925	  255187	529669	214652	298888	468042
P_18   259411	   249452	  252322	545394	217661	298885	464701
P_19   261476	   240635	  256826	535296	224470	300230	465799
P_20   246682	   254061	  251138	545615	222228	300008	467475

- Friedman chi-squared statistic: 112.778571
- p-value: 5.335902e-22
- This p-value is for rejection of the omnibus null hypothesis, that all samples (groups) are from
  the same distribution, i.e. there is no effect of groups. The alternate hypothesis that one or
  more of the correlated samples (groups) is different, i.e. there is a group effect.
- The omnibus p-value is at or below the respectable critical threshold of 0.05, so post-hoc
  pairwise multiple comparison tests are conducted to discern which of the pairs have significantly
  differences. Post-hoc tests conducted: Conover, where the p-value is adjusted by the false
  discovery rate (FDR) procedure of Benjaminyi-Hochberg.

  Post-hoc p-values of all possible pairs (of samples/ groups) are compactly represented as a lower
  triangular matrix. Each numerical entry is the p-value of row/column pair, i.e. the null
  hypotheses that the group represented by a particular column name is different from the group
  represented by a particular row name. Conover p-values, further adjusted by the
  Benjaminyi-Hochberg FDR method

  * Unif-80%

      QL	AvgQL0.99	AvgQL1.0	BIL1	    BIL2	  BIL3	  BIL4
      \ql{0.99} \rl{0.99} \rl{1.00} \BILOne{} \BILTwo{} \BILThree{} \BILFour{}
P_1  387809	388915	383835	885904	393992	398202	514142
P_2  359157	379690	363164	886975	384114	376660	515336
P_3  383065	376724	360581	894013	379834	360186	520716
P_4  376629	379476	381806	882742	360301	358114	528800
P_5  347754	383895	370965	864484	375807	366606	523167
P_6  357321	384470	369683	871624	366181	364609	530412
P_7  358944	383526	358428	911472	366868	363209	514686
P_8  347414	376654	385126	895509	392616	383019	532197
P_9  360972	383002	369881	899334	378537	355148	513209
P_10 357457	374808	368923	866065	355299	359736	513984
P_11 366141	373215	369628	860455	396973	377169	520757
P_12 368787	388541	361227	894268	378112	376437	513099
P_13 370668	397366	371919	900575	368307	357662	514924
P_14 388434	384845	375453	861305	399392	369540	522818
P_15 367029	381957	369656	884187	390026	362042	520379
P_16 367760	383548	371404	860149	363596	370414	530716
P_17 349081	372732	369545	855763	344912	353143	515141
P_18 361824	382886	372029	904230	376210	400139	531858
P_19 366121	398561	379479	889253	403883	378528	515531
P_20 358171	373058	367098	873834	370523	368916	525045

- Friedman chi-squared statistic: 90.578571
- p-value: 2.297453e-17
- rejection of the omnibus null hypothesis

* Unif-90%

      \ql{0.99} \rl{0.99} \rl{1.00} \BILOne{} \BILTwo{} \BILThree{} \BILFour{}
P_1   653676	521714	583292	1411442	701676	548748	673075
P_2   678801	606952	610287	1599513	862870	608274	655640
P_3   600219	602535	594998	1496442	864121	605376	630665
P_4   662569	518558	612290	1417290	835203	580285	657643
P_5   557821	607801	610968	1522469	859054	570829	680786
P_6   561714	553561	569823	1493178	776052	612004	621577
P_7   577479	538003	539180	1473951	790533	564949	704889
P_8   538715	603415	575945	1598306	798817	602557	610314
P_9   569077	528191	571533	1479493	713889	602314	631613
P_10   615113	527510	650835	1522554	744289	551199	598620
P_11   558858	561689	580324	1526107	755665	596688	643571
P_12   564356	564868	594560	1517267	730110	587742	616851
P_13   567569	619918	619763	1433219	755946	647755	730470
P_14   576338	619782	636887	1512337	800148	543174	681310
P_15   585967	581851	658242	1548853	907391	615699	652752
P_16   594172	609886	596591	1561824	836903	616500	718513
P_17   535899	570047	600196	1531139	707526	537206	621174
P_18   560293	612670	645596	1488537	782649	584652	673597
P_19   633629	571462	627070	1523982	745810	659741	683916
P_20   546210	609227	572384	1485851	734806	671281	644336


- Friedman chi-squared statistic: 96.150000
- p-value: 1.592809e-18
- rejection of the omnibus null hypothesis


\end{comment}


\paragraph{Medium Demand Variability.} The results for medium variability in the system are
presented in Table~\ref{tbl:unif}, where a uniform demand interarrival time is used. We use the
tuples 70-\(\Unif\), 80-\(\Unif\), and 90-\(\Unif\) to indicate the experimental setup of using
uniform demand interarrival times with a aimed bottleneck utilisation of \(70\%\), \(80\%\), or
\(90\%\) respectively. The results state the accumulated costs, averaged over all replications, for
the work in process (WIPC), the finished goods inventory (FGIC), the backorder costs (BOC), as well
as the sum of the finished good inventory and backorder costs (\fgibocName{}) and the sum over all
three cost centers (SUM). To ease readability all costs are given in \(1000\)~Dollars. For a better
insight in the timing performance we also provide the average shopfloor throughput time (SFTT),
i.e.\@ the average time an order spends in the system after being released and until reaching the
finished goods inventory, the finished goods inventory time (FGIT), as well as the service level
(SL). The later is given in percentage and states the percentage of orders that were produced on
time. Finally, the actual measured costs per period (Cost.p.P) and the by the algorithm learned
average reward \(\avgrew\) are stated. The tested methods are the deep Q-Learning algorithm with
discount factor \(0.99\) (\qlt{0.99}), the \ARA{} algorithm with discount factors \(0.99\)
(\rlt{0.99}) and \(1.00\) (\rlt{1.00}), as well as the backward infinite loading technique \BIL{}
with fixed lead times between \(1\) and \(4\) as indicated by the number ($\BILOne$-$\BILFour$).


For the case of medium variability with a utilisation of \(70\%\) 70-(\(\Unif\)) we can see that
\(\BILTwo\) performs best in the sense of accumulated costs (\(220.53\)). The reinforcement learning
algorithm follow, where \rlt{0.99} is slightly but significantly better then the other two variants
(\(250.70\), \(254.97\), \(255.06\)). The other \BIL{} variants accumulate much more costs. For the
timing one can see that \rlt{1.00} performs very good with a service level of \(93.2\%\).
Furthermore, the learned average reward \(\avgrew\) and actual average reward are very similar for
both \ARA{} variants. However, with \(70\%\) of utilisation the overhead of the reinforcement
learning variants are too big to be able to compete with the simple rule based release mechanism of
\BILTwo{} as for changing workload the utilisation changes rather linearly due to the low
variability in the system.

In the case of 80-\(\Unif\) the deep Q-Learning variant \qlt{0.99} performs best in the cost related
measures (\(365.03\) for SUM), followed by \BILThree{} (\(369.97\)) and \rlt{1.00} (\(370.99\)),
which are not significantly different from each other. One can see the \BILTwo{} (\(377.27\))
performs slightly worse than \rlt{1.00} and is followed by \rlt{0.99} (\(382.39\)). Again \rlt{1.00}
achieves a very high service level of \(93.4\%\) with reasonable sum of cost, which is also
reflected in the low Backorder costs. Furthermore, the measured and learned average reward values
are quite close. Thus, the results already provide the insight that \ARA{} performs quite well in
terms of timing for the completion of orders. This pays off when confronted with higher
probabilistic systems.

For instance, with \(90\%\) utilisation and medium variable demand stream, i.e.\@ 90-(\(\Unif\)), we
can see that \rlt{0.99} performs best (\(576.48\)), followed by \qlt{0.99} (\(586.92\)) and
\BILThree{} (\(595.35\)), as well as \rlt{1.00} (\(602.54\)), where the later two are not
significantly different. In this case \rlt{0.99} also achieves the best service level of \(87.9\%\)
within this group of best performing algorithms. The WIPC costs, as well as the SFTT, show that the
reinforcement learning agents have a higher work in process, which is clear as they try to smooth
the workload of demand stream. When looking at the timing cost it becomes even more apparent that
the agents perform well in timing as \rlt{0.99} performs much better (\(471.13\)) than the other two
agents (\(488.01\), \(494.66\)) and all the \BIL{} techniques accumulated more \fgibocName{} costs
(\(507.10\) for the best performing \BIL{}, \BILThree{}).


\begin{table}[tb]
  \footnotesize
  \begin{center}
    \begin{tabular}{crrrrrlrrrrr}
      \toprule
      70-\(\Exp\) & WIPC              & FGIC              & BOC               & \hide{\fgibocName{}}                        & SUM                            &  \multicolumn{2}{r}{SFTT}          & FGIT                & SL(\%)       & Cost.p.P                     & \(\avgrew^{\pol}\) \\
      \midrule
      \ql{0.99}   & \cost{53873.500}  & \cost{142398.800} & \cost{164427.200} & \fgiboc{142398.800}{164427.200} & \cost{360699.500}&$^{1}$          & \timem{1.585} & \fgit{1.585}{2.796} & \tard{0.162} & \costpp{360699.500}          &                    \\
      \rl{0.99}   & \cost{61419.850}  & \cost{181702.200} & \cost{115287.200} & \fgiboc{181702.200}{115287.200} & \cost{358409.250}&                & \timem{1.734} & \fgit{1.734}{3.145} & \tard{0.107} & \costpp{358409.250}          & \rnd{50.787}       \\
      \rl{1.00}   & \cost{56578.800}  & \cost{199310.000} & \cost{115714.400} & \fgiboc{199310.000}{115714.400} & \cost{371603.200}&                & \timem{1.639} & \fgit{1.639}{3.136} & \tard{0.113} & \costpp{371603.200}          & \rnd{50.575}       \\
      \BILOne{}   & \cost{48547.100}  & \cost{     0.000} & \cost{776753.600} & \fgiboc{     0.000}{776753.600} & \cost{825300.700}&                & \timem{1.479} & \fgit{1.479}{1.975} & \tard{0.663} & \costpp{825300.700}          &                    \\
      \BILTwo{}   & \cost{48618.850}  & \cost{ 66927.200} & \cost{248638.400} & \fgiboc{ 66927.200}{248638.400} & \cost{364184.450}&$^{1}$          & \timem{1.480} & \fgit{1.480}{2.312} & \tard{0.220} & \costpp{364184.450}          &                    \\
      \BILThree{} & \cost{48632.050}  & \cost{222603.200} & \cost{ 74568.800} & \fgiboc{222603.200}{ 74568.800} & \textbf{\cost{345804.050}}&       & \timem{1.481} & \fgit{1.481}{3.094} & \tard{0.063} & \textbf{\costpp{345804.050}} &                    \\
      \BILFour{}  & \cost{48514.200}  & \cost{409531.200} & \cost{ 23406.400} & \fgiboc{409531.200}{ 23406.400} & \cost{481451.800}       &         & \timem{1.478} & \fgit{1.478}{4.029} & \tard{0.019} & \costpp{481451.800}          &                    \\
      \bottomrule
      \toprule
      80-\(\Exp\) & WIPC              & FGIC              & BOC               & \hide{\fgibocName}                        & SUM                             &\multicolumn{2}{r}{SFTT}          & FGIT                & SL(\%)       & Cost.p.P                     & \(\avgrew^{\pol}\) \\
      \midrule
      \ql{0.99}   & \cost{87706.700}  & \cost{227485.400} & \cost{225150.400} & \fgiboc{227485.400}{225150.400} & \cost{540342.500}&$^{1}$          & \timem{2.043} & \fgit{2.043}{3.541} & \tard{0.166} & \costpp{540342.500}          &                    \\
      \rl{0.99}   & \cost{89838.950}  & \cost{253476.000} & \cost{185687.200} & \fgiboc{253476.000}{185687.200} & \cost{529002.150}&$^{1,2}$        & \timem{2.078} & \fgit{2.078}{3.689} & \tard{0.124} & \costpp{529002.150}          & \rnd{70.527}       \\
      \rl{1.00}   & \cost{87171.500}  & \cost{242915.600} & \cost{198456.000} & \fgiboc{242915.600}{198456.000} & \cost{528543.100}&$^{2,3}$        & \timem{2.034} & \fgit{2.034}{3.599} & \tard{0.134} & \costpp{528543.100}          & \rnd{72.944}       \\
      \BILOne{}   & \cost{78731.850}  & \cost{     0.000} & \cost{1259709.60} & \fgiboc{     0.000}{1259709.60} & \cost{1338441.45}&                & \timem{1.884} & \fgit{1.884}{2.382} & \tard{0.771} & \costpp{1338441.450}         &                    \\
      \BILTwo{}   & \cost{78203.150}  & \cost{ 52303.200} & \cost{548736.000} & \fgiboc{ 52303.200}{548736.000} & \cost{679242.350}&                & \timem{1.875} & \fgit{1.875}{2.602} & \tard{0.350} & \costpp{679242.350}          &                    \\
      \BILThree{} & \cost{79029.850}  & \cost{200273.200} & \cost{242100.800} & \fgiboc{200273.200}{242100.800} & \textbf{\cost{521403.850}}&$^{3}$ & \timem{1.890} & \fgit{1.890}{3.265} & \tard{0.142} & \textbf{\costpp{521403.850}} &                    \\
      \BILFour{}  & \cost{79384.750}  & \cost{395493.000} & \cost{116943.200} & \fgiboc{395493.000}{116943.200} & \cost{591820.950} &               & \timem{1.896} & \fgit{1.896}{4.128} & \tard{0.062} & \costpp{591820.950}          &                    \\
      \bottomrule
      \toprule
      90-\(\Exp\) & WIPC              & FGIC              & BOC               & \hide{\fgibocName}                        & SUM                              & \multicolumn{2}{r}{SFTT}          & FGIT                & SL(\%)       & Cost.p.P                     & \(\avgrew^{\pol}\) \\
      \midrule
      \ql{0.99}   & \cost{168655.950} & \cost{222145.800} & \cost{788662.400} & \fgiboc{222145.800}{788662.400} & \cost{1179464.150}&$^{1}$               & \timem{3.134} & \fgit{3.134}{4.501} & \tard{0.285} & \costpp{1179464.150}         &                    \\
      \rl{0.99}   & \cost{158034.800} & \cost{212726.200} & \cost{701300.800} & \fgiboc{212726.200}{701300.800} & \cost{1072061.800}&$^{2}$               & \timem{2.970} & \fgit{2.970}{4.300} & \tard{0.281} & \costpp{1072061.800}         & \rnd{115.801}      \\
      \rl{1.00}   & \cost{160453.950} & \cost{286065.800} & \cost{539803.200} & \fgiboc{286065.800}{539803.200} & \textbf{\cost{986322.950}}&       & \timem{3.007} & \fgit{3.007}{4.623} & \tard{0.204} & \textbf{\costpp{986322.950}} & \rnd{109.907}      \\
      \BILOne{}   & \cost{146835.900} & \cost{     0.000} & \cost{2349374.40} & \fgiboc{     0.000}{2349374.40} & \cost{2496210.300}&               & \timem{2.796} & \fgit{2.796}{3.295} & \tard{0.867} & \costpp{2496210.300}         &                    \\
      \BILTwo{}   & \cost{143756.000} & \cost{ 34112.800} & \cost{1412752.00} & \fgiboc{ 34112.800}{1412752.00} & \cost{1590620.800}&               & \timem{2.748} & \fgit{2.748}{3.380} & \tard{0.529} & \costpp{1590620.800}         &                    \\
      \BILThree{} & \cost{144278.250} & \cost{154146.800} & \cost{877450.400} & \fgiboc{154146.800}{877450.400} & \cost{1175875.450}&$^{1}$               & \timem{2.756} & \fgit{2.756}{3.857} & \tard{0.294} & \costpp{1175875.450}         &                    \\
      \BILFour{}  & \cost{147077.800} & \cost{335070.400} & \cost{622141.600} & \fgiboc{335070.400}{622141.600} & \cost{1104289.800}&$^{2}$               & \timem{2.800} & \fgit{2.800}{4.608} & \tard{0.175} & \costpp{1104289.800}         &                    \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{\label{tbl:exp} Results with high variability in the system due to exponentially
    distributed interarrival times of the demand. \(^{1,2,3}\) Costs with the same number are
    not significant different from each other. }
\end{table}

\begin{comment}

* 70-Exp

      \ql{0.99} \rl{0.99} \rl{1.00} \BILOne{} \BILTwo{} \BILThree{} \BILFour{}
P_1   351947	374561	379567	834003	386432	362168	492681
P_2   347668	352692	378234	822817	364552	333854	479469
P_3   367401	390552	366704	864960	381170	361897	482144
P_4   368674	359179	387919	835193	381497	349521	479063
P_5   348052	359409	367586	818363	368443	346388	481219
P_6   372165	357376	385714	820607	381450	351540	477010
P_7   358072	353692	375693	799663	354775	342411	478984
P_8   359080	367498	371778	863328	387183	361328	484675
P_9   362323	351285	364367	839001	371559	335595	489409
P_10   363594	363521	361902	819961	367956	355976	482128
P_11   351111	351607	359002	806276	345417	336380	478077
P_12   360053	358579	372742	823191	343266	337228	481849
P_13   362083	355953	375080	800258	347386	334618	484658
P_14   362155	380252	375057	843183	354535	358173	479977
P_15   362034	352998	365647	817717	351729	338243	477084
P_16   368261	356893	379373	839426	367362	359818	471416
P_17   368115	346666	367777	806327	357377	342397	483288
P_18   356904	343756	367760	816272	361651	341250	489722
P_19   370278	342598	365683	818686	357727	328406	477375
P_20   354020	349118	364479	816782	352222	338890	478808


- Friedman chi-squared statistic: 100.242857
- p-value: 2.232967e-19
- rejection of the omnibus null hypothesis


* 80-Exp

      \ql{0.99} \rl{0.99} \rl{1.00} \BILOne{} \BILTwo{} \BILThree{} \BILFour{}
P_1  524150	491399	492299	1253359	627883	492535	576044
P_2  551981	537349	534693	1378972	702719	546737	629820
P_3  568969	541461	548726	1323977	685412	522401	579507
P_4  547969	596543	592121	1364182	705144	536206	590773
P_5  595489	518330	531775	1334075	658382	532177	639777
P_6  535908	566964	536670	1429615	731598	520384	572011
P_7  557047	518934	569058	1336778	648986	497168	563390
P_8  538454	540953	512740	1395377	724787	530955	586574
P_9  522057	547783	539467	1387591	709643	522574	604104
P_10  545500	524853	523222	1307334	710386	511576	591671
P_11  550026	519641	518909	1312791	661352	529574	602428
P_12  520436	513871	520666	1302676	627306	495800	579365
P_13  534303	530579	517976	1377731	693986	553154	570858
P_14  527004	530556	538978	1407617	740972	528730	663494
P_15  546023	548754	539483	1324827	759339	560228	594110
P_16  551856	525486	542790	1326459	673565	553607	582936
P_17  509943	494150	491463	1302608	648931	482086	581970
P_18  538290	482416	487062	1275391	608823	502715	558249
P_19  515500	518251	507451	1331100	627861	495598	567573
P_20  525945	531770	525313	1296369	637772	513872	601765

- Friedman chi-squared statistic: 97.628571
- p-value: 7.835701e-19
- rejection of the omnibus null hypothesis

* 90-Exp

      \ql{0.99} \rl{0.99} \rl{1.00} \BILOne{} \BILTwo{} \BILThree{} \BILFour{}
P_1   1252433	1173395	1024728	2399890	1655026	1159432	1012319
P_2   1074569	870660	941377	2341852	1466107	1214910	971734
P_3   1188157	965337	871242	2510900	1409012	1050054	1260441
P_4   911949	1044494	758677	2345507	1566310	1118957	1130965
P_5   1033474	897575	836501	2197182	1219749	998558	946565
P_6   1216651	1228266	933766	2692086	1685058	1391199	1272896
P_7   1015338	983438	1040244	2399295	1429570	958909	951637
P_8   1512067	878268	1430194	2565419	1444940	1067515	906021
P_9   1108092	1232311	1309534	2641681	1850877	1153341	1079572
P_10   1344449	1264977	968913	2375393	1761920	1118482	1042407
P_11   1188536	1274797	1168751	3040365	1669772	1673853	1853585
P_12   1271137	985585	915254	2327385	1599041	1175113	1205382
P_13   1337521	1080134	1114726	2410549	1531861	1078113	1003334
P_14   1347283	1098738	861594	2728058	1566173	1387567	916554
P_15   1060109	1000957	947592	2422075	1704239	1016923	1070665
P_16   1095527	922726	957775	2363731	1637310	1232300	1017197
P_17   1157196	1095892	1011316	2669102	1959411	1245607	1212991
P_18   1171769	1047445	804936	2506905	1765175	1140291	1122519
P_19   1162809	1312587	949697	2481932	1361007	1166369	1052711
P_20   1140217	1083654	879642	2504899	1529858	1170016	1056301

- Friedman chi-squared statistic: 85.800000
- p-value: 2.253674e-16
- rejection of the omnibus null hypothesis


\end{comment}


\paragraph{High Demand Variability.} For results of the experiments using exponential demand
interarrival times, presented in Table~\ref{tbl:exp} we can see a similar pattern as before. In this
case we use the tuples 70-\(\Exp\), 80-\(\Exp\), and 90-\(\Exp\) to indicate the corresponding
experimental setups.

For \(70\%\) utilization of the bottleneck, i.e. 80-\(\Exp\), \BILThree{} performs best with costs
of \(345.80\), followed by \rlt{0.99} with \(358.41\), and then by \qlt{0.99} and \BILTwo{}
(\(360.70\), \(364.18\)). In terms of timing performance, again \ARA{} outperforms the opponents
with \fgibocName{} costs of \(296.99\), and \BILThree{} with the second highest costs (\(297.17\)).
However, the service level of \BILThree{} is higher with \(93.7\%\) due to the lower shop floor
throughput time (SFTT, \(1.48\) periods for \BILThree{} versus \(1.73\) periods for \rlt{0.99}).

In case of 80-\(\Exp\) \BILThree{} performs best with costs of \(521.40\), but is not significantly
better than \rlt{1.00} with \(528.54\). Also \rlt{0.99} performs quite well (\(529.00\)). The
service level of all three algorithm are good, but the order is reversed: \rlt{0.99} with
\(87.6\%\), \rlt{1.00} with \(86.6\%\) and \BILThree{} with \(85.8\%\). Again the SFTT of the RL
agents are slightly higher, i.e. \(0.13\)-\(0.20\) periods, than the ones of the \BIL{} release
procedures. The other three \BIL{} methods, \BILOne{}, \BILTwo{} and \BILFour{}, are performing
rather bad (\(1,338.44\), \(679.24\) and \(591.82\) respectively). Interesting to see is that in
this scenario \rlt{0.99} and \rlt{1.00} handle the due date performance better than \qlt{0.99}, as
can be seen in the backorder costs (BOC). Here the former two accumulate costs of \(185.69\) and
\(198.46\), while \qlt{0.99} faces costs of \(225.15\). This results in timing costs (\fgibocName{})
of \(439.16\) for \rlt{0.99} and \(441.37\) for \rlt{1.00}, which also means a better timing cost
than \BILThree{} with \(442.37\). For this scenario the actual measured average reward (average
costs) of about \(75.5\) for both \ARA{} variants and learned average rewards of \(70.53\) for
\rlt{0.99} and \(72.94\) for \rlt{1.00} are a little bit offset. This probably results from the
higher variability and utilisation in the system.

Finally, for the case of 90-\(\Exp\) \rlt{1.00} performs much better than the other variants with
cots of \(986.32\). It is followed by \rlt{0.99} with costs of \(1,072.06\) and rather far, but not
significantly, afterwards follows \BILFour{} (\(1,104.29\)). As before the shopfloor throughput time
(SFTT) is higher for the RL variants, and with that of course also the work in process costs (WIPC).
The backorder costs of \rlt{1.00} (\(539.80\)) are the lowest of all tested methods. Like for the
overall costs also in terms of timing costs (\fgibocName{}) \rlt{1.00} performs best (\(825.87\)),
followed by \rlt{0.99} (\(914.03\)) and \BILFour{} (\(957.21\)).
%
\BILThree{} and \ql{0.99} perform quite similar in the sense of accumulated costs, \(1,175.88\) and
\(1,179.46\) respectively. But the other measures show that \ql{0.99} has a better timing
performance, thus also a slightly high service level with \(71.5\%\) as compared to \BILThree{} with
\(70.6\%\). The best service level is achieved by \BILFour{} with \(82.5\%\), before \rlt{1.00}
\(79.6\%\), but with a high cost as the difference of SUM is \(117.97\).
%
Interesting to see is that the measured average reward and the learned average reward are very
different for both \ARA{} variants (about \(30\)-\(40\)). This is probably due to the high 
variability and utilisation in the system, but might also be an indicator that the agents need more time for
learning. However, due to the high computational complexity we encountered we leave this to future
work.

Overall we can see that the RL agents, and especially the established \ARA{} algorithm performs very
well in systems with high utilisation. In case of the less utilised systems the results reveal that
there is still potential for the improvement of reinforcement learning variants.


\section{Conclusion}
\label{sec:conclusion}

% Therefore, in a current working paper I apply an enhanced state/action-value iteration version with
% ANN as function approximation on bigger sized problems, where preliminary promising results could be
% achieved. In the future I want to adapt and investigate the use of an actor-critic reinforcement
% learning version to problems from research and industry, where I plan to focus on production
% planning. We hypothesize that due to more correct state value estimations from the critic, the
% resulting policies from the actor are will outperform the currently known actor-critic algorithms.
% This would be in line with the observation of overestimated state values of standard discounted
% reinforcement learning algorithms, which lead to the development of
% extensions~\cite[]{hessel2018rainbow}. We believe that instead of developing extensions to resolve
% structurally fundamental problems a better approach is to aim for correct state value estimations,
% which will consequently lead to better policies.\\

This paper is one of the first papers that use reinforcement learning for setting lead times to
release orders into the production system. We present a novel average reward adjusted reinforcement
learning algorithm that uses an artificial neural network to approximate the state-action value
functions. We add several extensions to this algorithm, including n-step learning, and a special
experience replay memory that stores its experiences according to the actions takes to prevent
catastrophic forgetting. Additionally we invent \textit{independent agents} to reduce the action
space by independently choosing a subset of possible actions, while operating on the same state.
Additionally we adapt the deep Q-Learning algorithm of \citet{mnih2015human} to be able to present a
fair comparison between the algorithms, where we also use several backward infinite loading
techniques.

The results suggest that for high utilisation the RL learning agents, and here especially the novel
\ARA{} variant, performs very well. In case of medium utilisation the achieved results are as good
as with the \BIL{} variants. And for low utilisation one can see that the RL agents still have room
for improvement, as \BIL{} outperforms them. When comparing deep Q-Learning to the established
\ARA{} algorithm, we can see that for all cases, except one (80-\(\Unif\)), \ARA{} performs better.

Therefore, in the future we plan to develop \ARA{} further to a actor-critic state-of-the-art
reinforcement learning algorithm. We hypothesise that due to the more correct state-action value
estimations of \ARA{} as compared to deep Q-Learning, cf. \citet{schneckenreither2020average}, the
critic will provide a better feedback of the value function update to the actor. This should lead to
better policies than with standard actor-critic algorithms, e.g. A3C~\citep{mnih2016asynchronous}.
%
Furthermore, one limitation of this proof-of-concept study is that due to the computational
complexity we restricted our scope to flow-shop production systems only. In subsequent studies we
plan to test the algorithms on job-shop queuing systems as well.
%
Additionally, we want to exploit hierarchical production
planning~\citep{schneeweibeta1995hierarchical} by implementing agents on different hierarchical
levels which can bargain with each other. For instance, one agent might be responsible for medium
term planning order release decision at the top level, while another agent concentrates on
short-term scheduling at the base level. At the start of the process a proposed order release plan
is generated by the first agent. Accordingly the agent from the base level performs a preliminary
scheduling of these orders and may report very expensive orders as feedback. This information is
then taken into consideration on the top level which adapts the order release plan accordingly. Here
the top level agent minimises the overall costs by load leveling on an aggregated model, while at
the bottom level the agent (mainly) minimises backorder costs.
%
Finally, although the introduced actions space reduction method, named \textit{independent agents},
works very well, it remains to be shown that the achievable policies are different to the ones for
agents without this adaption.


% \MS[t]{It remains to be shown that independent agents find the same (or better) policies.}

% \MS[t]{Multiple agents on different tasks (hierarchical PP): e.g. Order acceptance + release,
%   scheduling + release, etc.}

% \MS[t]{Job shop}

% This paper introduces deep theoretical insights in reinfocement learning and provides first evidence
% that average reward reinfocement learning is better applicable to operations research problems than
% the discounted framework. The paper describes an application of an order release model based on
% reinforcement learning. The performance is tested on a multi-product, two-stage hypothetical flow
% shop and is measured by cost, delivery and lead time related measures. We show that in the current
% version the machine learning approach is not able to outperform all other tested order release
% approaches. In the future we expect to be able to yield better results by investigating the
% parameterisation and resolving current issues when scaling average reinforcement learning to complex
% scenarios.

% \MS[t]{todo}

\bibliographystyle{plainnat}
\bibliography{references}
\vfill
\pagebreak[4!]
\appendix

\section{Deep Q-Learning Algorithm}

Algorithm~\ref{alg:ql} presents the detailed deep Q-Learning algorithm. It is very similar to the
established \ARA{} algorithm that is given in Algorithm~\ref{alg:full}. However, it does not compute
the average reward. Furthermore, it also operates on a single estimation of the overall future
discounted reward.

\begin{algorithm}[h!]
  \begin{algorithmic}[1]
    \State{}Initialise state \(s_{0}\) and network parameters \(\netTarget, \netWorker\) randomly,
    set exploration rate \(0 < \pexp \leqslant 1\), target network adaption rate
    \(0 < \gamma \ll 1\) and discount factor \(0 < \gamma_{1} < 1\)
    \While{the stopping criterion is not fulfilled} \ForEach{independent agent
      \(i \in \agent_{1},\agent_{2},\ldots,\agent_{n}\)} \State{}%
    \begin{minipage}[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth-\leftmargin+2pt}
      With probability \(\frac{\pexp}{n}\) choose action \(a_{t,i}\) randomly or otherwise one of
      the set defined by
      \(\min_{a \in \mathcal{A}(i)}\X^{\pol}_{\gamma_{1}}(s_{t},a; \netWorker{})\).
    \end{minipage}
    \EndFor{}
    \State{}%
    \begin{minipage}[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}
      Carry out action \(a_{t}=(a_{t,\agent_{1}},\ldots,a_{t,\agent_{n}})\), observe reward
      \(r_{t}\) and resulting state \(s_{t+1}\). Store experience
      \((s_{t}, a_{t}, \isRandAct{t}, r_{t}, s_{t+1})\) in the experience replay memory \(M\).
    \end{minipage}
    \If{\(t \mod{\nstep} \equiv 0\)}
    \State{Reset gradient: \(d\netWorker{} \gets 0\)}
    \State{}%
    \begin{minipage}[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth-\leftmargin+2pt}
      Sample \(m\) random mini-batches of \(\nstep\) consecutive experiences from \(M\) numbered by
      \(1 \leqslant i \leqslant \nstep\), each with
      \((s_{i}, \isRandAct{i}, a_{i}, r_{i}, s_{i+1})\).
    \end{minipage}
    \ForEach{mini-batch}
    \ForEach{experience, starting with the latest seen experience \(\nstep\)}
    \If{experience \(\nstep\) or \(\neg \isRandAct{i}\) or  \(\pexp > \plearn\)}
      \State{\(\target_{\gamma1} \gets \min_{a} \X^\pol_{\gamma_1}(s_{i+1},a; \netTarget{})\)}
    \EndIf
    \State{\(\target_{\gamma_{1}}  \gets r_{i} + \gamma_{1} \target_{\gamma_{1}} \)}
    \State{\(d\netWorker{} \gets d\netWorker{} +
      \partial{(\target_{\gamma_{1}} - \X^\pol_{\gamma_{1}}(s_{i+1},a; \netWorker{}))}^{2} / \partial\netWorker{}\)}
    \EndFor{}
    \EndFor{}
    \State{Perform update of \(\netWorker{}\) using \(d\netWorker{} / (m \cdot \nstep)\)}
    \EndIf{}
    \State{Every \(C\) steps exponential smoothly set target network:
    \(\netTarget \gets (1 - \gamma) \netTarget{} + \gamma \netWorker{}\)}
    \State{Set \(s \gets s'\), \(t \gets t+1\) and decay parameters}
    \EndWhile{}
  \end{algorithmic}
  \caption{\label{alg:ql}Deep Q-Learning}
\end{algorithm}


\section{Statistical Analysis of the Results}

Table~\ref{tbl:stats-unif} and Table~\ref{tbl:stats-exp} provide the statistical results of the
computation results. We used the Friedman test with a significance level of \(p=0.05\) for a
statistical analysis of the mean sum of costs. As expected the omnibus null hypotheses (all samples
are from the same distribution) are rejected for all cases. Therefore, we conducted pairwise Conover
post-hoc tests adjusted by the Benjaminyi-Hochberg FDR method~\citep{benjamini1995controlling} to
reduce liberality. These results are shown in Table~\ref{tbl:stats-unif} and
Table~\ref{tbl:stats-exp} for the uniformly and exponentially distributed interarrival times of the
demand respectively. The values given in bold are below the significance level of \(5\%\) and
therefore indicate which row-column tuples are not statistically significant.


\begin{table}
  \begin{center}
    \begin{tabular}{lP{1.75cm}P{1.75cm}P{1.75cm}P{1.75cm}P{1.75cm}P{1.75cm}}
      \toprule
      70-\(\Unif\) & \ql{0.99}         & \rl{0.99} & \rl{1.00}         & \BILOne{} & \BILTwo{} & \BILThree{} \\
      \midrule
      \rl{0.99}    & 3.37e-04          &           &                   &           &           &             \\
      \rl{1.0}     & \textbf{3.55e-01} & 1.04e-05  &                   &           &           &             \\
      \BILOne{}    & 1.94e-45          & 2.48e-51  & 6.90e-44          &           &           &             \\
      \BILTwo{}    & 1.09e-24          & 3.35e-16  & 9.57e-27          & 3.66e-64  &           &             \\
      \BILThree{}  & 1.78e-20          & 9.27e-29  & 2.44e-18          & 1.35e-22  & 6.26e-47  &             \\
      \BILFour{}   & 6.30e-34          & 3.85e-41  & 4.83e-32          & 1.16e-08  & 3.00e-56  & 1.16e-08    \\
      \bottomrule
      \toprule
      80-\(\Unif\) & \ql{0.99}         & \rl{0.99} & \rl{1.00}         & \BILOne{} & \BILTwo{} & \BILThree{} \\
      \midrule
      \rl{0.99}    & 4.78e-16          &           &                   &           &           &             \\
      \rl{1.00}    & 5.26e-04          & 3.96e-08  &                   &           &           &             \\
      \BILOne{}    & 1.02e-40          & 1.98e-21  & 3.92e-34          &           &           &             \\
      \BILTwo{}    & 6.98e-10          & 7.12e-03  & 2.06e-03          & 1.31e-27  &           &             \\
      \BILThree{}  & 2.16e-02          & 8.93e-11  & \textbf{2.03e-01} & 1.57e-36  & 2.46e-05  &             \\
      \BILFour{}   & 6.16e-33          & 1.10e-11  & 2.74e-25          & 5.33e-05  & 5.38e-18  & 5.30e-28    \\
      \bottomrule
      \toprule
      90-\(\Unif\) & \ql{0.99}         & \rl{0.99} & \rl{1.00}         & \BILOne{} & \BILTwo{} & \BILThree{} \\
      \midrule
      \rl{0.99}    & \textbf{5.18e-01} &           &                   &           &           &             \\
      \rl{1.00}    & 1.94e-04          & 1.56e-05  &                   &           &           &             \\
      \BILOne{}    & 8.40e-42          & 9.61e-43  & 1.10e-34          &           &           &             \\
      \BILTwo{}    & 2.36e-33          & 1.10e-34  & 5.21e-25          & 1.56e-05  &           &             \\
      \BILThree{}  & 4.16e-04          & 3.64e-05  & \textbf{8.19e-01} & 6.27e-35  & 1.81e-25  &             \\
      \BILFour{}   & 2.33e-20          & 6.57e-22  & 1.79e-11          & 2.93e-18  & 4.39e-08  & 5.97e-12    \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{\label{tbl:stats-unif}P-Values from the statistical analysis for the results of
    moderate variability in the system. Tuples marked bold are not significantly different.}
\end{table}


\begin{table}
  \begin{center}
    \begin{tabular}{lP{1.75cm}P{1.75cm}P{1.75cm}P{1.75cm}P{1.75cm}P{1.75cm}}
      \toprule
      70-\(\Exp\) & \ql{0.99}         & \rl{0.99}         & \rl{1.00}         & \BILOne{} & \BILTwo{} & \BILThree{} \\
      \midrule
      \rl{0.99}   & 1.75e-02          &                   &                   &           &           &             \\
      \rl{1.00}   & 6.15e-07          & 5.50e-12          &                   &           &           &             \\
      \BILOne{}   & 7.62e-35          & 1.74e-39          & 1.32e-23          &           &           &             \\
      \BILTwo{}   & \textbf{3.33e-01} & 1.02e-03          & 3.17e-05          & 6.06e-33  &           &             \\
      \BILThree{} & 5.96e-17          & 1.79e-11          & 6.09e-29          & 7.00e-52  & 3.47e-19  &             \\
      \BILFour{}  & 1.16e-24          & 6.25e-30          & 1.68e-12          & 4.68e-06  & 1.56e-22  & 3.43e-44    \\
      \bottomrule
      \toprule
      80-\(\Exp\) & \ql{0.99}         & \rl{0.99}         & \rl{1.00}         & \BILOne{} & \BILTwo{} & \BILThree{} \\
      \midrule
      \rl{0.99}   & \textbf{7.07e-02} &                   &                   &           &           &             \\
      \rl{1.00}   & 1.74e-03          & \textbf{1.72e-01} &                   &           &           &             \\
      \BILOne{}   & 2.39e-35          & 6.77e-39          & 2.38e-41          &           &           &             \\
      \BILTwo{}   & 1.22e-25          & 1.00e-29          & 1.21e-32          & 1.13e-05  &           &             \\
      \BILThree{} & 2.72e-05          & 1.33e-02          & \textbf{2.44e-01} & 3.65e-43  & 5.68e-35  &             \\
      \BILFour{}  & 2.07e-13          & 1.12e-17          & 6.55e-21          & 3.62e-17  & 6.09e-07  & 1.45e-23    \\
      \bottomrule
      \toprule
      90-\(\Exp\) & \ql{0.99}         & \rl{0.99}         & \rl{1.00}         & \BILOne{} & \BILTwo{} & \BILThree{} \\
      \midrule
      \rl{0.99}   & 2.61e-06          &                   &                   &           &           &             \\
      \rl{1.00}   & 5.03e-13          & 1.74e-03          &                   &           &           &             \\
      \BILOne{}   & 9.64e-23          & 2.73e-33          & 2.22e-39          &           &           &             \\
      \BILTwo{}   & 3.46e-12          & 1.32e-23          & 1.30e-30          & 1.23e-05  &           &             \\
      \BILThree{} & \textbf{6.86e-01} & 1.23e-05          & 3.46e-12          & 1.32e-23  & 5.03e-13  &             \\
      \BILFour{}  & 2.45e-04          & \textbf{2.38e-01} & 2.60e-05          & 6.39e-31  & 6.30e-21  & 9.46e-4     \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{\label{tbl:stats-exp}P-Values from the statistical analysis for the results of
    high variability in the system. Tuples marked bold are not significantly different.}
\end{table}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
